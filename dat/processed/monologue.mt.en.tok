Today , I would like to talk about voice interaction systems .
We study voice interaction interfaces . We do research on the things we hope to improve on , such as ease of use and safety while driving .
Above all , we do research on the recognition and understanding of natural speech . Let me explain what I mean by " natural speech . "
For example , there are a lot of ways to ask whether there are Chinese restaurants in the area .
There are a lot of ways to express one thing .
We think that being able to permit any kind of way to say something , without restrictions , is natural speech . Many voice interaction interfaces only allow certain ways of speaking .
They might recognize expression one but not expressions two to four . This happens often .
In order for natural speech to be recognized , we collected many natural speech patterns . We collected many examples of ways to say things in different circumstances .
We thought about how information may be communicated between two people , the user and the operator , when information is sought . The user may ask the operator , " I would like to eat Chinese food . "
Then , the operator uses a search machine to find the information and produce results . This is how the operator listens to the request of the user and makes a judgment call in their mind .
A fundamental way of looking at this is that this can be directly replaced by a machine , using past knowledge . So we had pairs of people have this kind of information-seeking dialogue in a car , and gathered data .
Using that data , we managed the dialogue .
Now I will talk specifically about how we processed the dialogue . I said that we would collect many examples , and this is how we stored it .
The content of the dialogue in the car conversation and the search format is left like this . For example , in response to the user 's expression , " I want some spaghetti , " the operator makes a search formula like this , and is recorded together .
The stored data is processed like this . The input speech comes in first .
When we get a piece of dialogue such as " I want to go to a spaghetti restaurant , " we extract the important keywords like this . Based on those keywords , we find the most similar speech pattern out of the many examples .
If " I want to go to a curry restaurant " turns out to be the closest , the search formula like the one used by the operator is brought out . We then replace with what is close to the dialogue that came in , and the search formula is created .
Using this structure , we created the interactive system and evaluated it .
We then used the content of what 18 people said in the car for evaluation data . And then we add and utilize the examples of what 28 people said in the same way in the car .
The results of the evaluation are on the following graph . This graph shows how well the search formula was able to be written correctly as the number of speakers stored was increased .
You can see that the more data was stored , the better the search formula got .
If you look closer , you can see that the graph is rising on the right , so we can expect to have even better performance with more data , and when we looked into why the mistakes were made , we saw that in 70 percent of cases , it was due to lack of data .
We believe that by increasing data , performance will increase even more . In summary , we have devised a way for voice interaction systems to handle natural speech . We proposed a speech recognition process that makes use of examples of speech .
We managed the dialogue by using examples of stored dialogue . We believe that this is a way to deal with a variety of different expressions .
When we conducted an evaluation , we were able to create search formulas for approximately 70 percent of speech . We also learned that it would be possible to improve performance by increasing the number of examples .
Thank you very much .
Today , I would like to talk about voice interaction systems . We study voice interaction interfaces .
We do research on the things we hope to improve on , such as ease of use and safety while driving .
Above all , we do research on the recognition and understanding of natural speech . Let me explain what I mean by " natural speech . "
For example , there are a lot of ways to ask whether there are Chinese restaurants in the area .
There are a lot of ways to express one thing .
We think that being able to permit any kind of way to say something , without restrictions , is natural speech .
Many voice interaction interfaces only allow certain ways of speaking .
They might recognize expression one but not expressions two to four . This happens often .
In order for natural speech to be recognized , we collected many natural speech patterns .
We collected many examples of ways to say things in different circumstances .
We thought about how information may be communicated between two people , the user and the operator , when information is sought . The user may ask the operator , " I would like to eat Chinese food . "
Then , the operator uses a search machine to find the information and produce results .
This is how the operator listens to the request of the user and makes a judgment call in their mind . A fundamental way of looking at this is that this can be directly replaced by a machine , using past knowledge .
So we had pairs of people have this kind of information-seeking dialogue in a car , and gathered data . Using that data , we managed the dialogue .
Now I will talk specifically about how we processed the dialogue .
I said that we would collect many examples , and this is how we stored it . The content of the dialogue in the car conversation and the search format is left like this .
For example , in response to the user 's expression , " I want some spaghetti , " the operator makes a search formula like this , and is recorded together .
The stored data is processed like this . The input speech comes in first .
When we get a piece of dialogue such as " I want to go to a spaghetti restaurant , " we extract the important keywords like this . Based on those keywords , we find the most similar speech pattern out of the many examples .
If " I want to go to a curry restaurant " turns out to be the closest , the search formula like the one used by the operator is brought out . We then replace with what is close to the dialogue that came in , and the search formula is created .
Using this structure , we created the interactive system and evaluated it . We then used the content of what 18 people said in the car for evaluation data .
And then we add and utilize the examples of what 28 people said in the same way in the car .
The results of the evaluation are on the following graph .
This graph shows how well the search formula was able to be written correctly as the number of speakers stored was increased . You can see that the more data was stored , the better the search formula got .
If you look closer , you can see that the graph is rising on the right , so we can expect to have even better performance with more data , and when we looked into why the mistakes were made , we saw that in 70 percent of cases , it was due to lack of data .
We believe that by increasing data , performance will increase even more .
In summary , we have devised a way for voice interaction systems to handle natural speech . We proposed a speech recognition process that makes use of examples of speech .
We managed the dialogue by using examples of stored dialogue .
We believe that this is a way to deal with a variety of different expressions . When we conducted an evaluation , we were able to create search formulas for approximately 70 percent of speech .
We also learned that it would be possible to improve performance by increasing the number of examples . Thank you very much .
Today , I would like to talk about voice interaction systems .
We study voice interaction interfaces . We do research on the things we hope to improve on , such as ease of use and safety while driving . Above all , we do research on the recognition and understanding of natural speech .
Let me explain what I mean by " natural speech . "
For example , there are a lot of ways to ask whether there are Chinese restaurants in the area . There are a lot of ways to express one thing .
We think that being able to permit any kind of way to say something , without restrictions , is natural speech .
Many voice interaction interfaces only allow certain ways of speaking .
They might recognize expression one but not expressions two to four . This happens often .
In order for natural speech to be recognized , we collected many natural speech patterns . We collected many examples of ways to say things in different circumstances .
We thought about how information may be communicated between two people , the user and the operator , when information is sought . The user may ask the operator , " I would like to eat Chinese food . "
Then , the operator uses a search machine to find the information and produce results .
This is how the operator listens to the request of the user and makes a judgment call in their mind . A fundamental way of looking at this is that this can be directly replaced by a machine , using past knowledge .
So we had pairs of people have this kind of information-seeking dialogue in a car , and gathered data . Using that data , we managed the dialogue .
Now I will talk specifically about how we processed the dialogue .
I said that we would collect many examples , and this is how we stored it . The content of the dialogue in the car conversation and the search format is left like this .
For example , in response to the user 's expression , " I want some spaghetti , " the operator makes a search formula like this , and is recorded together .
The stored data is processed like this .
The input speech comes in first . When we get a piece of dialogue such as " I want to go to a spaghetti restaurant , " we extract the important keywords like this .
Based on those keywords , we find the most similar speech pattern out of the many examples . If " I want to go to a curry restaurant " turns out to be the closest , the search formula like the one used by the operator is brought out .
We then replace with what is close to the dialogue that came in , and the search formula is created . Using this structure , we created the interactive system and evaluated it .
We then used the content of what 18 people said in the car for evaluation data .
And then we add and utilize the examples of what 28 people said in the same way in the car .
The results of the evaluation are on the following graph . This graph shows how well the search formula was able to be written correctly as the number of speakers stored was increased .
You can see that the more data was stored , the better the search formula got .
If you look closer , you can see that the graph is rising on the right , so we can expect to have even better performance with more data , and when we looked into why the mistakes were made , we saw that in 70 percent of cases , it was due to lack of data .
We believe that by increasing data , performance will increase even more . In summary , we have devised a way for voice interaction systems to handle natural speech .
We proposed a speech recognition process that makes use of examples of speech . We managed the dialogue by using examples of stored dialogue .
We believe that this is a way to deal with a variety of different expressions . When we conducted an evaluation , we were able to create search formulas for approximately 70 percent of speech .
We also learned that it would be possible to improve performance by increasing the number of examples . Thank you very much .
Today , I would like to talk about voice interaction systems .
We study voice interaction interfaces . We do research on the things we hope to improve on , such as ease of use and safety while driving . Above all , we do research on the recognition and understanding of natural speech .
Let me explain what I mean by " natural speech . "
For example , there are a lot of ways to ask whether there are Chinese restaurants in the area . There are a lot of ways to express one thing .
We think that being able to permit any kind of way to say something , without restrictions , is natural speech .
Many voice interaction interfaces only allow certain ways of speaking .
They might recognize expression one but not expressions two to four . This happens often .
In order for natural speech to be recognized , we collected many natural speech patterns .
We collected many examples of ways to say things in different circumstances .
We thought about how information may be communicated between two people , the user and the operator , when information is sought .
The user may ask the operator , " I would like to eat Chinese food . " Then , the operator uses a search machine to find the information and produce results .
This is how the operator listens to the request of the user and makes a judgment call in their mind . A fundamental way of looking at this is that this can be directly replaced by a machine , using past knowledge .
So we had pairs of people have this kind of information-seeking dialogue in a car , and gathered data .
Using that data , we managed the dialogue . Now I will talk specifically about how we processed the dialogue .
I said that we would collect many examples , and this is how we stored it .
The content of the dialogue in the car conversation and the search format is left like this .
For example , in response to the user 's expression , " I want some spaghetti , " the operator makes a search formula like this , and is recorded together .
The stored data is processed like this .
The input speech comes in first . When we get a piece of dialogue such as " I want to go to a spaghetti restaurant , " we extract the important keywords like this .
Based on those keywords , we find the most similar speech pattern out of the many examples . If " I want to go to a curry restaurant " turns out to be the closest , the search formula like the one used by the operator is brought out .
We then replace with what is close to the dialogue that came in , and the search formula is created . Using this structure , we created the interactive system and evaluated it .
We then used the content of what 18 people said in the car for evaluation data .
And then we add and utilize the examples of what 28 people said in the same way in the car . The results of the evaluation are on the following graph .
This graph shows how well the search formula was able to be written correctly as the number of speakers stored was increased . You can see that the more data was stored , the better the search formula got .
If you look closer , you can see that the graph is rising on the right , so we can expect to have even better performance with more data , and when we looked into why the mistakes were made , we saw that in 70 percent of cases , it was due to lack of data .
We believe that by increasing data , performance will increase even more .
In summary , we have devised a way for voice interaction systems to handle natural speech .
We proposed a speech recognition process that makes use of examples of speech . We managed the dialogue by using examples of stored dialogue . We believe that this is a way to deal with a variety of different expressions .
When we conducted an evaluation , we were able to create search formulas for approximately 70 percent of speech . We also learned that it would be possible to improve performance by increasing the number of examples .
Thank you very much .
This has been published under the title , " Syntactic Analysis of Natural Language using Finite-State Automata " .
Firstly , to speak of the background , currently , the need for the analysis of spoken language is increasing . Because what is being analyzed is spoken language , the analysis must happen in real time .
Furthermore , it is believed that this kind of analysis will be utilized well in natural language processing systems , such as that of speech dialogue . Next , I 'll explain in detail about natural language analysis processing .
Natural language analysis is made up of three stages : morphemic analysis , syntactical analysis , and semantic analysis . I 'll now explain about these various analyzes .
First , in morphemic analysis , the sentence input is divided into individual words , and each word is assigned a part of speech .
Next , in syntactical analysis , the structure of the sentence input is analyzed , and its structure is output as a syntax tree . Various grammar rules for the purpose of analyzing natural language can be given , such as context-free grammar and regular grammar .
Finally , in semantic analysis , the meaning of the words , the context between them , and so on are analyzed . Based on this , we can come to know what sort of meaning a sentence indicates .
From here , I 'd like to consider syntactical analysis .
We 'll consider an example of syntactical analysis . We 'll think about these six grammar rules .
In the first , what is shown is how a noun phrase and verb phrase arranged together are recognized as a sentence .
In the second , when an article determiner and a noun are arranged together is indicated to be a noun phrase .
In the third , a verb is indicated to be a verbal phrase . In light of the grammar rules up to this point , the following three show what parts of speech correspond to each word .
For example , in this , " the " is an article . " Boy " is a noun . And , lastly , " runs " in a verb .
From that , each is represented . So , as an actual case , we will consider the analysis of the sentence input " The boy runs " using these grammar rules .
First of all , from the following three grammar rules , we understand that " the " is an article , " boy " is a noun , and " run " is a verb ; this sort of structure is made firstly in this way , with " the " an article , " boy " a noun , and " runs " a verb .
Then , next , because we understand that the article and the noun together are an arrangement , we understand that those two , " The boy " , are a noun phrase ; at the same time , because this " runs " is a verb , from these three grammar rules we can also understand that " runs " is a verbal phrase .
Based on this , it becomes like so .
Finally , from the arrangement of the noun phrase and verbal phrase , this " The boy runs " becomes a sentence , and these together form a complete syntax tree .
Next , I 'd like to make some comparisons for analyzes using context-free grammar and finite-state automata .
Firstly , analysis using context-free grammar has the ability to accurately analyze natural language sentences . However , it has a weakness in that processing in this manner takes time .
Conversely , analysis using finite-state automata is good in that it processes quickly , but it also has a drawback in that it does n't necessarily always accurately analyze the full of a natural language sentence .
With this , I 'd like to consider an analysis that combines the strengths of the two : context-free grammar 's ability to analyze accurately and the speed of analysis that comes with using finite-state automata . This figure shows the flow of analysis using finite-state automata .
First of all , context-free grammar is converted to a finite-state automaton . Next , using the converted finite-state automaton , the sentence input is analyzed and a syntax tree is created .
Based on this conversion conversion of context-free grammar to a finite-state automaton , this finite-state automaton has the same capabilities as this context-free grammar , and , also , through the use of this finite-state automaton in carrying out analyzes , we can analyze quickly , making for an analysis that combines both strengths .
As an actual example for analysis using finite-state automata , we will first look at the previously-considered context-free grammar .
Converting it to a finite-state automaton , it will become like so . Thusly , here , too , we consider the analysis of the sentence input " The boy runs " , using the converted finite-state automaton .
When the sentence input is analyzed morphemically and converted into parts of speech , we can see the " article noun verb " arrangement . With that , we can generate a syntax tree from the transition that comes from the initial state of each to the final state , that is , article noun verb .
Now for a summary . This time , I explained about a method of analysis using finite-state automata .
This analysis is made up of two stages , in which the first stage is the conversation from context-free grammar to a finite-state automaton .
The other stage is a processing procedure in which the sentence input is analyzed using the finite-state automaton . Using this method , it is possible to have a high-speed analysis that also has a capacity near that of context-free grammar .
With that , we conclude .
This has been published under the title , " Syntactic Analysis of Natural Language using Finite-State Automata " .
Firstly , to speak of the background , currently , the need for the analysis of spoken language is increasing .
Because what is being analyzed is spoken language , the analysis must happen in real time . Furthermore , it is believed that this kind of analysis will be utilized well in natural language processing systems , such as that of speech dialogue .
Next , I 'll explain in detail about natural language analysis processing . Natural language analysis is made up of three stages : morphemic analysis , syntactical analysis , and semantic analysis .
I 'll now explain about these various analyzes . First , in morphemic analysis , the sentence input is divided into individual words , and each word is assigned a part of speech .
Next , in syntactical analysis , the structure of the sentence input is analyzed , and its structure is output as a syntax tree .
Various grammar rules for the purpose of analyzing natural language can be given , such as context-free grammar and regular grammar .
Finally , in semantic analysis , the meaning of the words , the context between them , and so on are analyzed . Based on this , we can come to know what sort of meaning a sentence indicates .
From here , I 'd like to consider syntactical analysis . We 'll consider an example of syntactical analysis .
We 'll think about these six grammar rules . In the first , what is shown is how a noun phrase and verb phrase arranged together are recognized as a sentence .
In the second , when an article determiner and a noun are arranged together is indicated to be a noun phrase .
In the third , a verb is indicated to be a verbal phrase .
In light of the grammar rules up to this point , the following three show what parts of speech correspond to each word . For example , in this , " the " is an article .
" Boy " is a noun . And , lastly , " runs " in a verb . From that , each is represented .
So , as an actual case , we will consider the analysis of the sentence input " The boy runs " using these grammar rules .
First of all , from the following three grammar rules , we understand that " the " is an article , " boy " is a noun , and " run " is a verb ; this sort of structure is made firstly in this way , with " the " an article , " boy " a noun , and " runs " a verb .
Then , next , because we understand that the article and the noun together are an arrangement , we understand that those two , " The boy " , are a noun phrase ; at the same time , because this " runs " is a verb , from these three grammar rules we can also understand that " runs " is a verbal phrase .
Based on this , it becomes like so .
Finally , from the arrangement of the noun phrase and verbal phrase , this " The boy runs " becomes a sentence , and these together form a complete syntax tree .
Next , I 'd like to make some comparisons for analyzes using context-free grammar and finite-state automata .
Firstly , analysis using context-free grammar has the ability to accurately analyze natural language sentences . However , it has a weakness in that processing in this manner takes time .
Conversely , analysis using finite-state automata is good in that it processes quickly , but it also has a drawback in that it does n't necessarily always accurately analyze the full of a natural language sentence .
With this , I 'd like to consider an analysis that combines the strengths of the two : context-free grammar 's ability to analyze accurately and the speed of analysis that comes with using finite-state automata .
This figure shows the flow of analysis using finite-state automata . First of all , context-free grammar is converted to a finite-state automaton .
Next , using the converted finite-state automaton , the sentence input is analyzed and a syntax tree is created .
Based on this conversion conversion of context-free grammar to a finite-state automaton , this finite-state automaton has the same capabilities as this context-free grammar , and , also , through the use of this finite-state automaton in carrying out analyzes , we can analyze quickly , making for an analysis that combines both strengths .
As an actual example for analysis using finite-state automata , we will first look at the previously-considered context-free grammar . Converting it to a finite-state automaton , it will become like so .
Thusly , here , too , we consider the analysis of the sentence input " The boy runs " , using the converted finite-state automaton . When the sentence input is analyzed morphemically and converted into parts of speech , we can see the " article noun verb " arrangement .
With that , we can generate a syntax tree from the transition that comes from the initial state of each to the final state , that is , article noun verb .
Now for a summary . This time , I explained about a method of analysis using finite-state automata .
This analysis is made up of two stages , in which the first stage is the conversation from context-free grammar to a finite-state automaton . The other stage is a processing procedure in which the sentence input is analyzed using the finite-state automaton .
Using this method , it is possible to have a high-speed analysis that also has a capacity near that of context-free grammar . With that , we conclude .
This has been published under the title , " Syntactic Analysis of Natural Language using Finite-State Automata " .
Firstly , to speak of the background , currently , the need for the analysis of spoken language is increasing .
Because what is being analyzed is spoken language , the analysis must happen in real time . Furthermore , it is believed that this kind of analysis will be utilized well in natural language processing systems , such as that of speech dialogue .
Next , I 'll explain in detail about natural language analysis processing . Natural language analysis is made up of three stages : morphemic analysis , syntactical analysis , and semantic analysis .
I 'll now explain about these various analyzes . First , in morphemic analysis , the sentence input is divided into individual words , and each word is assigned a part of speech .
Next , in syntactical analysis , the structure of the sentence input is analyzed , and its structure is output as a syntax tree . Various grammar rules for the purpose of analyzing natural language can be given , such as context-free grammar and regular grammar .
Finally , in semantic analysis , the meaning of the words , the context between them , and so on are analyzed . Based on this , we can come to know what sort of meaning a sentence indicates .
From here , I 'd like to consider syntactical analysis . We 'll consider an example of syntactical analysis .
We 'll think about these six grammar rules . In the first , what is shown is how a noun phrase and verb phrase arranged together are recognized as a sentence .
In the second , when an article determiner and a noun are arranged together is indicated to be a noun phrase . In the third , a verb is indicated to be a verbal phrase .
In light of the grammar rules up to this point , the following three show what parts of speech correspond to each word .
For example , in this , " the " is an article . " Boy " is a noun . And , lastly , " runs " in a verb .
From that , each is represented . So , as an actual case , we will consider the analysis of the sentence input " The boy runs " using these grammar rules .
First of all , from the following three grammar rules , we understand that " the " is an article , " boy " is a noun , and " run " is a verb ; this sort of structure is made firstly in this way , with " the " an article , " boy " a noun , and " runs " a verb .
Then , next , because we understand that the article and the noun together are an arrangement , we understand that those two , " The boy " , are a noun phrase ; at the same time , because this " runs " is a verb , from these three grammar rules we can also understand that " runs " is a verbal phrase .
Based on this , it becomes like so . Finally , from the arrangement of the noun phrase and verbal phrase , this " The boy runs " becomes a sentence , and these together form a complete syntax tree .
Next , I 'd like to make some comparisons for analyzes using context-free grammar and finite-state automata . Firstly , analysis using context-free grammar has the ability to accurately analyze natural language sentences .
However , it has a weakness in that processing in this manner takes time .
Conversely , analysis using finite-state automata is good in that it processes quickly , but it also has a drawback in that it does n't necessarily always accurately analyze the full of a natural language sentence .
With this , I 'd like to consider an analysis that combines the strengths of the two : context-free grammar 's ability to analyze accurately and the speed of analysis that comes with using finite-state automata . This figure shows the flow of analysis using finite-state automata .
First of all , context-free grammar is converted to a finite-state automaton . Next , using the converted finite-state automaton , the sentence input is analyzed and a syntax tree is created .
Based on this conversion conversion of context-free grammar to a finite-state automaton , this finite-state automaton has the same capabilities as this context-free grammar , and , also , through the use of this finite-state automaton in carrying out analyzes , we can analyze quickly , making for an analysis that combines both strengths .
As an actual example for analysis using finite-state automata , we will first look at the previously-considered context-free grammar .
Converting it to a finite-state automaton , it will become like so .
Thusly , here , too , we consider the analysis of the sentence input " The boy runs " , using the converted finite-state automaton . When the sentence input is analyzed morphemically and converted into parts of speech , we can see the " article noun verb " arrangement .
With that , we can generate a syntax tree from the transition that comes from the initial state of each to the final state , that is , article noun verb . Now for a summary . This time , I explained about a method of analysis using finite-state automata .
This analysis is made up of two stages , in which the first stage is the conversation from context-free grammar to a finite-state automaton .
The other stage is a processing procedure in which the sentence input is analyzed using the finite-state automaton . Using this method , it is possible to have a high-speed analysis that also has a capacity near that of context-free grammar . With that , we conclude .
This has been published under the title , " Syntactic Analysis of Natural Language using Finite-State Automata " .
Firstly , to speak of the background , currently , the need for the analysis of spoken language is increasing .
Because what is being analyzed is spoken language , the analysis must happen in real time .
Furthermore , it is believed that this kind of analysis will be utilized well in natural language processing systems , such as that of speech dialogue . Next , I 'll explain in detail about natural language analysis processing .
Natural language analysis is made up of three stages : morphemic analysis , syntactical analysis , and semantic analysis .
I 'll now explain about these various analyzes .
First , in morphemic analysis , the sentence input is divided into individual words , and each word is assigned a part of speech .
Next , in syntactical analysis , the structure of the sentence input is analyzed , and its structure is output as a syntax tree .
Various grammar rules for the purpose of analyzing natural language can be given , such as context-free grammar and regular grammar .
Finally , in semantic analysis , the meaning of the words , the context between them , and so on are analyzed . Based on this , we can come to know what sort of meaning a sentence indicates .
From here , I 'd like to consider syntactical analysis .
We 'll consider an example of syntactical analysis . We 'll think about these six grammar rules .
In the first , what is shown is how a noun phrase and verb phrase arranged together are recognized as a sentence . In the second , when an article determiner and a noun are arranged together is indicated to be a noun phrase .
In the third , a verb is indicated to be a verbal phrase .
In light of the grammar rules up to this point , the following three show what parts of speech correspond to each word . For example , in this , " the " is an article . " Boy " is a noun .
And , lastly , " runs " in a verb . From that , each is represented . So , as an actual case , we will consider the analysis of the sentence input " The boy runs " using these grammar rules .
First of all , from the following three grammar rules , we understand that " the " is an article , " boy " is a noun , and " run " is a verb ; this sort of structure is made firstly in this way , with " the " an article , " boy " a noun , and " runs " a verb .
Then , next , because we understand that the article and the noun together are an arrangement , we understand that those two , " The boy " , are a noun phrase ; at the same time , because this " runs " is a verb , from these three grammar rules we can also understand that " runs " is a verbal phrase .
Based on this , it becomes like so . Finally , from the arrangement of the noun phrase and verbal phrase , this " The boy runs " becomes a sentence , and these together form a complete syntax tree .
Next , I 'd like to make some comparisons for analyzes using context-free grammar and finite-state automata .
Firstly , analysis using context-free grammar has the ability to accurately analyze natural language sentences . However , it has a weakness in that processing in this manner takes time .
Conversely , analysis using finite-state automata is good in that it processes quickly , but it also has a drawback in that it does n't necessarily always accurately analyze the full of a natural language sentence .
With this , I 'd like to consider an analysis that combines the strengths of the two : context-free grammar 's ability to analyze accurately and the speed of analysis that comes with using finite-state automata . This figure shows the flow of analysis using finite-state automata .
First of all , context-free grammar is converted to a finite-state automaton . Next , using the converted finite-state automaton , the sentence input is analyzed and a syntax tree is created .
Based on this conversion conversion of context-free grammar to a finite-state automaton , this finite-state automaton has the same capabilities as this context-free grammar , and , also , through the use of this finite-state automaton in carrying out analyzes , we can analyze quickly , making for an analysis that combines both strengths .
As an actual example for analysis using finite-state automata , we will first look at the previously-considered context-free grammar . Converting it to a finite-state automaton , it will become like so .
Thusly , here , too , we consider the analysis of the sentence input " The boy runs " , using the converted finite-state automaton .
When the sentence input is analyzed morphemically and converted into parts of speech , we can see the " article noun verb " arrangement . With that , we can generate a syntax tree from the transition that comes from the initial state of each to the final state , that is , article noun verb .
Now for a summary . This time , I explained about a method of analysis using finite-state automata .
This analysis is made up of two stages , in which the first stage is the conversation from context-free grammar to a finite-state automaton . The other stage is a processing procedure in which the sentence input is analyzed using the finite-state automaton .
Using this method , it is possible to have a high-speed analysis that also has a capacity near that of context-free grammar . With that , we conclude .
Today I am making a presentation on a support system for multilingual text chats . As globalization continues to progress , there will be increasing opportunities to communicate multilingually .
In these situations , the differences between languages will be a problem . Therefore , a variety of support systems are being developed to eliminate the differences between languages .
For example , there are systems for translating websites and emails , as well as systems for translating text chats and actual spoken dialog .
In this presentation I would like to concentrate on translation systems for text dialogs .
Text chat programs are generally for any online text chats , but I would like to focus in particular on text chat programs of the same type as the Unix talk program . The talk program differs from conventional online chats in that what the user types is immediately displayed on their chat partner 's screen .
With conventional chat programs , what the user types is not displayed on their chat partner 's screen until the user presses " enter " .
Another feature of text chats is that casual , grammatically incorrect text is often entered , in contrast to formal texts .
Accordingly , text chats allow fluent conversation that resembles spoken speech in its features . A number of support systems for multilingual text chats have already been developed to deal with online text chats. However , these single sentence translation programs do not display the translation to the user 's chat partner until the user has finished entering the entire sentence , thereby impeding fluid conversation .
In that context , I would like to introduce the multilingual text chat program . This program introduces real-time translation in place of the single sentence translation system previously mentioned .
This real-time translation program immediately displays the translation result on the chat partner 's screen , even as the user is still typing their sentence . For example , if the user is in the middle of typing " I love you so much " , and has typed " I love " , the user 's chat partner will immediately see the translation up to that point , " watashi wa aishiteimasu " .
The user 's chat partner can therefore predict what the chat partner wants to say , and can interrupt if they choose . This facilitates a more fluid progression of the conversation .
As far as actual implementation goes , we have created a product with an implemented text chat program that performs English-Japanese real-time translation . This is the system 's configuration diagram , which consists of two terminals ; one for English users and one for Japanese users .
They are connected through the Internet . After an English text has been entered by the English user , the text goes to the English-Japanese real-time translation program through the Internet .
The text is translated here , and the result is the translation output that is displayed to the Japanese user .
The original English text that the English user entered is simultaneously displayed to the Japanese user . The Japanese user is thereby also able to make use of the information from their partner 's English text .
This time we are only using the English-Japanese translation program , so the Japanese user will also input an English text . We used here the gradual translation method of the English-Japanese real-time translation system .
I installed the system on a Unix workstation . These are the three languages that were used .
Also , this is the actual operation screen of the Japanese user .
There are three screens , and the original English text of the English user is displayed on the top screen . While still typing " language " from the sentence " I like soccer and language " , output is produced .
Below , the translation result of the input text is displayed . As you can see , a result is displayed even as the text is still incomplete .
At the very bottom is the user 's input screen . It is the line editor which is actually doing the inputting , so the user is able to control the response to this text .
Just as in the previous example , the English text which the Japanese user inputted is being displayed in the upper frame of the English user 's screen .
Below this is the screen showing what the English user is actually inputting . It has been verified that the implementation of this program allows for more fluid conversations . Let me sum it up .
Today I introduced a support system for multilingual online dialogs . This system facilitates more fluid conversation by introducing a real-time translation program to text chats .
And the prototype is ready . We have produced a text chat program that features English-Japanese real-time translation . I hope you found this presentation informative .
Today I am making a presentation on a support system for multilingual text chats .
As globalization continues to progress , there will be increasing opportunities to communicate multilingually .
In these situations , the differences between languages will be a problem . Therefore , a variety of support systems are being developed to eliminate the differences between languages .
For example , there are systems for translating websites and emails , as well as systems for translating text chats and actual spoken dialog .
In this presentation I would like to concentrate on translation systems for text dialogs . Text chat programs are generally for any online text chats , but I would like to focus in particular on text chat programs of the same type as the Unix talk program .
The talk program differs from conventional online chats in that what the user types is immediately displayed on their chat partner 's screen .
With conventional chat programs , what the user types is not displayed on their chat partner 's screen until the user presses " enter " .
Another feature of text chats is that casual , grammatically incorrect text is often entered , in contrast to formal texts .
Accordingly , text chats allow fluent conversation that resembles spoken speech in its features .
A number of support systems for multilingual text chats have already been developed to deal with online text chats. However , these single sentence translation programs do not display the translation to the user 's chat partner until the user has finished entering the entire sentence , thereby impeding fluid conversation .
In that context , I would like to introduce the multilingual text chat program . This program introduces real-time translation in place of the single sentence translation system previously mentioned .
This real-time translation program immediately displays the translation result on the chat partner 's screen , even as the user is still typing their sentence .
For example , if the user is in the middle of typing " I love you so much " , and has typed " I love " , the user 's chat partner will immediately see the translation up to that point , " watashi wa aishiteimasu " .
The user 's chat partner can therefore predict what the chat partner wants to say , and can interrupt if they choose .
This facilitates a more fluid progression of the conversation . As far as actual implementation goes , we have created a product with an implemented text chat program that performs English-Japanese real-time translation .
This is the system 's configuration diagram , which consists of two terminals ; one for English users and one for Japanese users .
They are connected through the Internet .
After an English text has been entered by the English user , the text goes to the English-Japanese real-time translation program through the Internet . The text is translated here , and the result is the translation output that is displayed to the Japanese user .
The original English text that the English user entered is simultaneously displayed to the Japanese user . The Japanese user is thereby also able to make use of the information from their partner 's English text .
This time we are only using the English-Japanese translation program , so the Japanese user will also input an English text .
We used here the gradual translation method of the English-Japanese real-time translation system . I installed the system on a Unix workstation .
These are the three languages that were used . Also , this is the actual operation screen of the Japanese user .
There are three screens , and the original English text of the English user is displayed on the top screen . While still typing " language " from the sentence " I like soccer and language " , output is produced .
Below , the translation result of the input text is displayed .
As you can see , a result is displayed even as the text is still incomplete . At the very bottom is the user 's input screen .
It is the line editor which is actually doing the inputting , so the user is able to control the response to this text .
Just as in the previous example , the English text which the Japanese user inputted is being displayed in the upper frame of the English user 's screen .
Below this is the screen showing what the English user is actually inputting . It has been verified that the implementation of this program allows for more fluid conversations .
Let me sum it up . Today I introduced a support system for multilingual online dialogs .
This system facilitates more fluid conversation by introducing a real-time translation program to text chats .
And the prototype is ready . We have produced a text chat program that features English-Japanese real-time translation . I hope you found this presentation informative .
Today I am making a presentation on a support system for multilingual text chats . As globalization continues to progress , there will be increasing opportunities to communicate multilingually .
In these situations , the differences between languages will be a problem .
Therefore , a variety of support systems are being developed to eliminate the differences between languages . For example , there are systems for translating websites and emails , as well as systems for translating text chats and actual spoken dialog .
In this presentation I would like to concentrate on translation systems for text dialogs .
Text chat programs are generally for any online text chats , but I would like to focus in particular on text chat programs of the same type as the Unix talk program .
The talk program differs from conventional online chats in that what the user types is immediately displayed on their chat partner 's screen .
With conventional chat programs , what the user types is not displayed on their chat partner 's screen until the user presses " enter " . Another feature of text chats is that casual , grammatically incorrect text is often entered , in contrast to formal texts .
Accordingly , text chats allow fluent conversation that resembles spoken speech in its features .
A number of support systems for multilingual text chats have already been developed to deal with online text chats. However , these single sentence translation programs do not display the translation to the user 's chat partner until the user has finished entering the entire sentence , thereby impeding fluid conversation .
In that context , I would like to introduce the multilingual text chat program .
This program introduces real-time translation in place of the single sentence translation system previously mentioned .
This real-time translation program immediately displays the translation result on the chat partner 's screen , even as the user is still typing their sentence . For example , if the user is in the middle of typing " I love you so much " , and has typed " I love " , the user 's chat partner will immediately see the translation up to that point , " watashi wa aishiteimasu " .
The user 's chat partner can therefore predict what the chat partner wants to say , and can interrupt if they choose . This facilitates a more fluid progression of the conversation .
As far as actual implementation goes , we have created a product with an implemented text chat program that performs English-Japanese real-time translation . This is the system 's configuration diagram , which consists of two terminals ; one for English users and one for Japanese users .
They are connected through the Internet .
After an English text has been entered by the English user , the text goes to the English-Japanese real-time translation program through the Internet . The text is translated here , and the result is the translation output that is displayed to the Japanese user . The original English text that the English user entered is simultaneously displayed to the Japanese user .
The Japanese user is thereby also able to make use of the information from their partner 's English text . This time we are only using the English-Japanese translation program , so the Japanese user will also input an English text .
We used here the gradual translation method of the English-Japanese real-time translation system . I installed the system on a Unix workstation .
These are the three languages that were used . Also , this is the actual operation screen of the Japanese user .
There are three screens , and the original English text of the English user is displayed on the top screen .
While still typing " language " from the sentence " I like soccer and language " , output is produced . Below , the translation result of the input text is displayed .
As you can see , a result is displayed even as the text is still incomplete .
At the very bottom is the user 's input screen . It is the line editor which is actually doing the inputting , so the user is able to control the response to this text .
Just as in the previous example , the English text which the Japanese user inputted is being displayed in the upper frame of the English user 's screen .
Below this is the screen showing what the English user is actually inputting . It has been verified that the implementation of this program allows for more fluid conversations .
Let me sum it up . Today I introduced a support system for multilingual online dialogs .
This system facilitates more fluid conversation by introducing a real-time translation program to text chats . And the prototype is ready .
We have produced a text chat program that features English-Japanese real-time translation . I hope you found this presentation informative .
Today I am making a presentation on a support system for multilingual text chats . As globalization continues to progress , there will be increasing opportunities to communicate multilingually .
In these situations , the differences between languages will be a problem . Therefore , a variety of support systems are being developed to eliminate the differences between languages .
For example , there are systems for translating websites and emails , as well as systems for translating text chats and actual spoken dialog .
In this presentation I would like to concentrate on translation systems for text dialogs . Text chat programs are generally for any online text chats , but I would like to focus in particular on text chat programs of the same type as the Unix talk program .
The talk program differs from conventional online chats in that what the user types is immediately displayed on their chat partner 's screen .
With conventional chat programs , what the user types is not displayed on their chat partner 's screen until the user presses " enter " .
Another feature of text chats is that casual , grammatically incorrect text is often entered , in contrast to formal texts . Accordingly , text chats allow fluent conversation that resembles spoken speech in its features .
A number of support systems for multilingual text chats have already been developed to deal with online text chats. However , these single sentence translation programs do not display the translation to the user 's chat partner until the user has finished entering the entire sentence , thereby impeding fluid conversation .
In that context , I would like to introduce the multilingual text chat program . This program introduces real-time translation in place of the single sentence translation system previously mentioned .
This real-time translation program immediately displays the translation result on the chat partner 's screen , even as the user is still typing their sentence . For example , if the user is in the middle of typing " I love you so much " , and has typed " I love " , the user 's chat partner will immediately see the translation up to that point , " watashi wa aishiteimasu " .
The user 's chat partner can therefore predict what the chat partner wants to say , and can interrupt if they choose . This facilitates a more fluid progression of the conversation .
As far as actual implementation goes , we have created a product with an implemented text chat program that performs English-Japanese real-time translation . This is the system 's configuration diagram , which consists of two terminals ; one for English users and one for Japanese users .
They are connected through the Internet .
After an English text has been entered by the English user , the text goes to the English-Japanese real-time translation program through the Internet . The text is translated here , and the result is the translation output that is displayed to the Japanese user .
The original English text that the English user entered is simultaneously displayed to the Japanese user . The Japanese user is thereby also able to make use of the information from their partner 's English text .
This time we are only using the English-Japanese translation program , so the Japanese user will also input an English text . We used here the gradual translation method of the English-Japanese real-time translation system . I installed the system on a Unix workstation .
These are the three languages that were used . Also , this is the actual operation screen of the Japanese user .
There are three screens , and the original English text of the English user is displayed on the top screen .
While still typing " language " from the sentence " I like soccer and language " , output is produced . Below , the translation result of the input text is displayed .
As you can see , a result is displayed even as the text is still incomplete . At the very bottom is the user 's input screen .
It is the line editor which is actually doing the inputting , so the user is able to control the response to this text .
Just as in the previous example , the English text which the Japanese user inputted is being displayed in the upper frame of the English user 's screen .
Below this is the screen showing what the English user is actually inputting .
It has been verified that the implementation of this program allows for more fluid conversations . Let me sum it up .
Today I introduced a support system for multilingual online dialogs .
This system facilitates more fluid conversation by introducing a real-time translation program to text chats . And the prototype is ready .
We have produced a text chat program that features English-Japanese real-time translation . I hope you found this presentation informative .
I 'll explain about simultaneous interpretation systems for English lectures .
There 's a difference , you see , when translating conversational language , which is useful when people both or all with different languages are conversing .
For example , let 's say a woman and a man are having a conversation , and they 're able to have it by means of this system between them that 's translating their dialogue .
That is , it 's a construct where what he says is translated by the system and heard by her .
Before , this is what it 's been like : in a phrase , the systems developed up until now are these dialogue translation systems . The fundamental unit for translation has been the sentence .
That is , the basic framework is , after he speaks , the system takes the results of the translation and vocalizes it as sound data : resultantly , the system , with him and her as users , produces alternating voice data .
On the other hand , having talked about dialogues , I come back to lectures , and we can think of their interpretation as being that of monologues .
In this case , it 's one-sided , a sort of supporting of communication , and in actuality it has come to be used quite a lot at places like the UN , so it 's quite thinkable that this sort of interpretation has a huge demand .
In the case of this kind of interpretation , like we have here , for the interpreter 's spoken words , the system takes them and outputs the results of the interpretation as voice data to the audience , while the lecturer continues to speak . Accordingly , with this sort of speaker , gestures are being used , and there 's a need to synchronize with those , as well .
So , to actualize that sort of thing , a simultaneous interpretation system is desired .
Focusing on the lectures that I 've talking about , what I 'd like us to aim for is the actualization of a simultaneous interpretation system for lectures .
Concretely , what I will consider is simultaneous interpretation that is the conversion of English lectures into Japanese voice data . For this purpose , in establishing a fundamental technique , I 've made the base the gradual advancement of spoken language processing , and I want to take a design approach .
Actually , we 've developed an experimental system called Linus .
Through the operation of this kind of system , I 'd like to ascertain the viability of automatic simultaneous interpretation . I 'll explain simply about this system , Linus .
The first notable characteristic of this system is that it can handle the concurrent input and output of two different languages , let us say English input and Japanese output .
Regarding its second characteristic , first of all , in connection to lectures , linguistic knowledge is employed , yes , but in addition to that there 's also specialist , or technical , knowledge included as well , and this is to be acquired beforehand , in this way for example : a published presentation sheet , a resume , or similar .
Its third characteristic is that its output is , as much as possible , in the natural language of spoken Japanese . The actual design used is this , Synchtrance , a simultaneous interpretation system for dialogue , which is what we 've been making up until now .
Based on that , we 've employed a simple design in the creation of a lecture-use system . I 'll explain about the construction of Linus , which is what we aim to realize .
Fundamentally , Linus is formed of two processing modules : one that analyses English and one that produces Japanese . Firstly , it receives the English voice data , going through recognition while listening to it , with the recognition results being in English , but at the same time it is also going through an analysis of the English data and creating Japanese in terms of the analysis results .
With that , in terms of the creation results , it will output Japanese voice data by means of compound voice data . Various processes like these , through simultaneous progression , bring about a simultaneous interpretation mechanism .
Actually , Linus , which is a system , as I 've been saying , for interpreting English into Japan , has been made a reality . It 's executed on a computer called a Unix Workstation .
As a notable characteristic , concretely about the knowledge information , at current the framework has humans as proxies . In actuality , for the next time , it wo n't be information ; for the speaking , the composition will have the form of Japanese voice data output from a Windows computer .
Furthermore , it 'll use a dictionary or a grammar employing the necessary linguistic data , which will be drawn by people from things like lecture drafts beforehand .
Actually , for English lectures , we went through with validation operations by means of the system 's voice data input . I feel that perhaps there 's hope here , a forecast for the possibility of realizing the simultaneous interpretation of lectures .
Here 's a display of the actual interpretation results . Actually , the results of the interpretation will be presented to the audience as sound ; for the audience 's understanding , this window has been constructed .
At a given point in time , the English input sentences will increase rapidly as the English input comes in . Along with that , the Japanese translation , too , will rapidly come out .
These are the output results of a lecture about information technology . As a last point , I 've talked about the design of our system for the simultaneous interpretation of English lectures , Linus , and its further realization .
Using a Workstation , in reality we 've validated the actual operations of the system , which , constructed of an analytical part and a creative part , executes interpretation with its data being based on this , the actual lecture draft .
It 's currently at an experimental level , but in the future , I 'd like to aim for the realization of a system that can do simultaneous interpretation for relatively short or fifteen minute lectures , such as at an academic conference .
I 'll end here .
I 'll explain about simultaneous interpretation systems for English lectures .
There 's a difference , you see , when translating conversational language , which is useful when people both or all with different languages are conversing . For example , let 's say a woman and a man are having a conversation , and they 're able to have it by means of this system between them that 's translating their dialogue .
That is , it 's a construct where what he says is translated by the system and heard by her .
Before , this is what it 's been like : in a phrase , the systems developed up until now are these dialogue translation systems . The fundamental unit for translation has been the sentence .
That is , the basic framework is , after he speaks , the system takes the results of the translation and vocalizes it as sound data : resultantly , the system , with him and her as users , produces alternating voice data .
On the other hand , having talked about dialogues , I come back to lectures , and we can think of their interpretation as being that of monologues .
In this case , it 's one-sided , a sort of supporting of communication , and in actuality it has come to be used quite a lot at places like the UN , so it 's quite thinkable that this sort of interpretation has a huge demand .
In the case of this kind of interpretation , like we have here , for the interpreter 's spoken words , the system takes them and outputs the results of the interpretation as voice data to the audience , while the lecturer continues to speak . Accordingly , with this sort of speaker , gestures are being used , and there 's a need to synchronize with those , as well .
So , to actualize that sort of thing , a simultaneous interpretation system is desired . Focusing on the lectures that I 've talking about , what I 'd like us to aim for is the actualization of a simultaneous interpretation system for lectures .
Concretely , what I will consider is simultaneous interpretation that is the conversion of English lectures into Japanese voice data . For this purpose , in establishing a fundamental technique , I 've made the base the gradual advancement of spoken language processing , and I want to take a design approach .
Actually , we 've developed an experimental system called Linus . Through the operation of this kind of system , I 'd like to ascertain the viability of automatic simultaneous interpretation .
I 'll explain simply about this system , Linus .
The first notable characteristic of this system is that it can handle the concurrent input and output of two different languages , let us say English input and Japanese output .
Regarding its second characteristic , first of all , in connection to lectures , linguistic knowledge is employed , yes , but in addition to that there 's also specialist , or technical , knowledge included as well , and this is to be acquired beforehand , in this way for example : a published presentation sheet , a resume , or similar .
Its third characteristic is that its output is , as much as possible , in the natural language of spoken Japanese .
The actual design used is this , Synchtrance , a simultaneous interpretation system for dialogue , which is what we 've been making up until now .
Based on that , we 've employed a simple design in the creation of a lecture-use system . I 'll explain about the construction of Linus , which is what we aim to realize .
Fundamentally , Linus is formed of two processing modules : one that analyses English and one that produces Japanese .
Firstly , it receives the English voice data , going through recognition while listening to it , with the recognition results being in English , but at the same time it is also going through an analysis of the English data and creating Japanese in terms of the analysis results .
With that , in terms of the creation results , it will output Japanese voice data by means of compound voice data . Various processes like these , through simultaneous progression , bring about a simultaneous interpretation mechanism .
Actually , Linus , which is a system , as I 've been saying , for interpreting English into Japan , has been made a reality . It 's executed on a computer called a Unix Workstation .
As a notable characteristic , concretely about the knowledge information , at current the framework has humans as proxies . In actuality , for the next time , it wo n't be information ; for the speaking , the composition will have the form of Japanese voice data output from a Windows computer .
Furthermore , it 'll use a dictionary or a grammar employing the necessary linguistic data , which will be drawn by people from things like lecture drafts beforehand .
Actually , for English lectures , we went through with validation operations by means of the system 's voice data input . I feel that perhaps there 's hope here , a forecast for the possibility of realizing the simultaneous interpretation of lectures .
Here 's a display of the actual interpretation results .
Actually , the results of the interpretation will be presented to the audience as sound ; for the audience 's understanding , this window has been constructed .
At a given point in time , the English input sentences will increase rapidly as the English input comes in . Along with that , the Japanese translation , too , will rapidly come out .
These are the output results of a lecture about information technology .
As a last point , I 've talked about the design of our system for the simultaneous interpretation of English lectures , Linus , and its further realization .
Using a Workstation , in reality we 've validated the actual operations of the system , which , constructed of an analytical part and a creative part , executes interpretation with its data being based on this , the actual lecture draft .
It 's currently at an experimental level , but in the future , I 'd like to aim for the realization of a system that can do simultaneous interpretation for relatively short or fifteen minute lectures , such as at an academic conference .
I 'll end here .
I 'll explain about simultaneous interpretation systems for English lectures .
There 's a difference , you see , when translating conversational language , which is useful when people both or all with different languages are conversing . For example , let 's say a woman and a man are having a conversation , and they 're able to have it by means of this system between them that 's translating their dialogue .
That is , it 's a construct where what he says is translated by the system and heard by her .
Before , this is what it 's been like : in a phrase , the systems developed up until now are these dialogue translation systems . The fundamental unit for translation has been the sentence .
That is , the basic framework is , after he speaks , the system takes the results of the translation and vocalizes it as sound data : resultantly , the system , with him and her as users , produces alternating voice data .
On the other hand , having talked about dialogues , I come back to lectures , and we can think of their interpretation as being that of monologues . In this case , it 's one-sided , a sort of supporting of communication , and in actuality it has come to be used quite a lot at places like the UN , so it 's quite thinkable that this sort of interpretation has a huge demand .
In the case of this kind of interpretation , like we have here , for the interpreter 's spoken words , the system takes them and outputs the results of the interpretation as voice data to the audience , while the lecturer continues to speak . Accordingly , with this sort of speaker , gestures are being used , and there 's a need to synchronize with those , as well .
So , to actualize that sort of thing , a simultaneous interpretation system is desired . Focusing on the lectures that I 've talking about , what I 'd like us to aim for is the actualization of a simultaneous interpretation system for lectures .
Concretely , what I will consider is simultaneous interpretation that is the conversion of English lectures into Japanese voice data .
For this purpose , in establishing a fundamental technique , I 've made the base the gradual advancement of spoken language processing , and I want to take a design approach .
Actually , we 've developed an experimental system called Linus .
Through the operation of this kind of system , I 'd like to ascertain the viability of automatic simultaneous interpretation . I 'll explain simply about this system , Linus .
The first notable characteristic of this system is that it can handle the concurrent input and output of two different languages , let us say English input and Japanese output .
Regarding its second characteristic , first of all , in connection to lectures , linguistic knowledge is employed , yes , but in addition to that there 's also specialist , or technical , knowledge included as well , and this is to be acquired beforehand , in this way for example : a published presentation sheet , a resume , or similar .
Its third characteristic is that its output is , as much as possible , in the natural language of spoken Japanese . The actual design used is this , Synchtrance , a simultaneous interpretation system for dialogue , which is what we 've been making up until now .
Based on that , we 've employed a simple design in the creation of a lecture-use system . I 'll explain about the construction of Linus , which is what we aim to realize .
Fundamentally , Linus is formed of two processing modules : one that analyses English and one that produces Japanese .
Firstly , it receives the English voice data , going through recognition while listening to it , with the recognition results being in English , but at the same time it is also going through an analysis of the English data and creating Japanese in terms of the analysis results .
With that , in terms of the creation results , it will output Japanese voice data by means of compound voice data . Various processes like these , through simultaneous progression , bring about a simultaneous interpretation mechanism .
Actually , Linus , which is a system , as I 've been saying , for interpreting English into Japan , has been made a reality .
It 's executed on a computer called a Unix Workstation . As a notable characteristic , concretely about the knowledge information , at current the framework has humans as proxies .
In actuality , for the next time , it wo n't be information ; for the speaking , the composition will have the form of Japanese voice data output from a Windows computer .
Furthermore , it 'll use a dictionary or a grammar employing the necessary linguistic data , which will be drawn by people from things like lecture drafts beforehand . Actually , for English lectures , we went through with validation operations by means of the system 's voice data input .
I feel that perhaps there 's hope here , a forecast for the possibility of realizing the simultaneous interpretation of lectures .
Here 's a display of the actual interpretation results .
Actually , the results of the interpretation will be presented to the audience as sound ; for the audience 's understanding , this window has been constructed .
At a given point in time , the English input sentences will increase rapidly as the English input comes in . Along with that , the Japanese translation , too , will rapidly come out .
These are the output results of a lecture about information technology . As a last point , I 've talked about the design of our system for the simultaneous interpretation of English lectures , Linus , and its further realization .
Using a Workstation , in reality we 've validated the actual operations of the system , which , constructed of an analytical part and a creative part , executes interpretation with its data being based on this , the actual lecture draft .
It 's currently at an experimental level , but in the future , I 'd like to aim for the realization of a system that can do simultaneous interpretation for relatively short or fifteen minute lectures , such as at an academic conference . I 'll end here .
I 'll explain about simultaneous interpretation systems for English lectures .
There 's a difference , you see , when translating conversational language , which is useful when people both or all with different languages are conversing .
For example , let 's say a woman and a man are having a conversation , and they 're able to have it by means of this system between them that 's translating their dialogue .
That is , it 's a construct where what he says is translated by the system and heard by her .
Before , this is what it 's been like : in a phrase , the systems developed up until now are these dialogue translation systems . The fundamental unit for translation has been the sentence .
That is , the basic framework is , after he speaks , the system takes the results of the translation and vocalizes it as sound data : resultantly , the system , with him and her as users , produces alternating voice data .
On the other hand , having talked about dialogues , I come back to lectures , and we can think of their interpretation as being that of monologues .
In this case , it 's one-sided , a sort of supporting of communication , and in actuality it has come to be used quite a lot at places like the UN , so it 's quite thinkable that this sort of interpretation has a huge demand .
In the case of this kind of interpretation , like we have here , for the interpreter 's spoken words , the system takes them and outputs the results of the interpretation as voice data to the audience , while the lecturer continues to speak . Accordingly , with this sort of speaker , gestures are being used , and there 's a need to synchronize with those , as well .
So , to actualize that sort of thing , a simultaneous interpretation system is desired .
Focusing on the lectures that I 've talking about , what I 'd like us to aim for is the actualization of a simultaneous interpretation system for lectures .
Concretely , what I will consider is simultaneous interpretation that is the conversion of English lectures into Japanese voice data . For this purpose , in establishing a fundamental technique , I 've made the base the gradual advancement of spoken language processing , and I want to take a design approach .
Actually , we 've developed an experimental system called Linus . Through the operation of this kind of system , I 'd like to ascertain the viability of automatic simultaneous interpretation .
I 'll explain simply about this system , Linus .
The first notable characteristic of this system is that it can handle the concurrent input and output of two different languages , let us say English input and Japanese output .
Regarding its second characteristic , first of all , in connection to lectures , linguistic knowledge is employed , yes , but in addition to that there 's also specialist , or technical , knowledge included as well , and this is to be acquired beforehand , in this way for example : a published presentation sheet , a resume , or similar .
Its third characteristic is that its output is , as much as possible , in the natural language of spoken Japanese .
The actual design used is this , Synchtrance , a simultaneous interpretation system for dialogue , which is what we 've been making up until now . Based on that , we 've employed a simple design in the creation of a lecture-use system .
I 'll explain about the construction of Linus , which is what we aim to realize .
Fundamentally , Linus is formed of two processing modules : one that analyses English and one that produces Japanese . Firstly , it receives the English voice data , going through recognition while listening to it , with the recognition results being in English , but at the same time it is also going through an analysis of the English data and creating Japanese in terms of the analysis results .
With that , in terms of the creation results , it will output Japanese voice data by means of compound voice data . Various processes like these , through simultaneous progression , bring about a simultaneous interpretation mechanism .
Actually , Linus , which is a system , as I 've been saying , for interpreting English into Japan , has been made a reality . It 's executed on a computer called a Unix Workstation .
As a notable characteristic , concretely about the knowledge information , at current the framework has humans as proxies . In actuality , for the next time , it wo n't be information ; for the speaking , the composition will have the form of Japanese voice data output from a Windows computer .
Furthermore , it 'll use a dictionary or a grammar employing the necessary linguistic data , which will be drawn by people from things like lecture drafts beforehand . Actually , for English lectures , we went through with validation operations by means of the system 's voice data input .
I feel that perhaps there 's hope here , a forecast for the possibility of realizing the simultaneous interpretation of lectures .
Here 's a display of the actual interpretation results .
Actually , the results of the interpretation will be presented to the audience as sound ; for the audience 's understanding , this window has been constructed .
At a given point in time , the English input sentences will increase rapidly as the English input comes in . Along with that , the Japanese translation , too , will rapidly come out .
These are the output results of a lecture about information technology . As a last point , I 've talked about the design of our system for the simultaneous interpretation of English lectures , Linus , and its further realization .
Using a Workstation , in reality we 've validated the actual operations of the system , which , constructed of an analytical part and a creative part , executes interpretation with its data being based on this , the actual lecture draft .
It 's currently at an experimental level , but in the future , I 'd like to aim for the realization of a system that can do simultaneous interpretation for relatively short or fifteen minute lectures , such as at an academic conference . I 'll end here .
Today , we 'd like to present our research on processing Japanese based on derivational grammar , and the applications of that grammar .
First , this is about processing Japanese from a computer , but when you attempt to process vernacular human language through a computer , you divide the input sentences through what 's known as morphological analysis . Furthermore , you conduct the processes of syntactic analysis , as well as semantic analysis , as well as contextual analysis , to process human language .
The first order of business is morphological analysis . In terms of language , because morphological analysis comes first , if you make a mistake here , everything else will be in error .
With that said , this process is extremely important . Additionally , Japanese generally does n't use spaces to separate words . When it 's English , you can immediately grasp a single word because there are almost always spaces in the text .
As for Japanese , however , a computer ca n't understand it as it is , therefore , first , you have to separate each and every word .
So , what are all these words ? The word hanseibun of apology is a noun . And wo is a particle .
Added to that are verbs , and after that are attached forms called auxiliary verbs , and you must have parts of speech affixed to each word . In this manner , regarding Japanese , separating words in sentences and tagging parts of speech means morphological analysis .
Now , I wrote about simply using morphological analysis on verbs , but this aspect is , in truth , not so simple. It actually gets a little complicated . This is the thing you all learned as school grammar in elementary or middle school :
For example , in grammar school , you learned conjugations like kakanai , kakimasu , kaku , kaku toki , kakeba , kake , for the verb kaku .
Then there were also names like mizenkei , renyokei , shushikei , rentaikei , kateikei , and meireikei . I think all of you had to learn conjugations like the godan katsuyo or the kamiichidan katsuyo .
And it 's not just these kinds of conjugations. For instance , there 's the seru auxiliary verb that represents the causative form. But , the truth is , you do n't just apply it as it is - there 's a rule in which this seru can only be affixed to the mizenkei of a verb .
Then there is rareru , the auxiliary verb that represents the passive. But this too , of course , can only be affixed to the mizenkei. Then there 's ta , for the perfect tense , but , for some reason , while there 's irregularity in verbal inflections , there 's none for ren'yokei or meireikei . And , there 's a rule where this can only be affixed to the ren'yokei I mentioned before .
So , because there are these kind of complicated conjugation transformations and these kind of complicated affixation rules , in order to input the word kakaserareta , you have to have a process that can deal with these kind of complicated rules .
Therefore , the biggest issue in dealing with Japanese morphological analysis is the need for processing this kind of complex inflection . With that said , I think it seems reasonable to say that , because Japanese employs conjugations , conjugation processing is necessary .
However , there is a grammar proposed saying the Japanese language does not conjugate . This is not just the school grammar all of you had thought of .
In Japanese , there is another grammar . It is derivational grammar .
Derivational grammar is the agglutinative nature of Japanese . You may be asking what this means ; the truth is , languages like Japanese , Korean , and even Turkish are quite similar , and agglutination is the quality these languages share .
Derivational grammar is what focuses on this aspect . And though verbal inflection was called conjugation before , it is believed that that is really the affixation of suffixes to the root word .
To give an example : if you had the verb kaku in conjugation transformation or grade school grammar , it would look something like kakanai , kakimasu .
Because of the transformation , it 's been called conjugation , but we transcribe using phonological notation . Thus , you transcribe with roman letters in place of hiragana Japanese syllabary , and in so doing , all the kaku 's from before end in the K consonant. It becomes clear that this part of the word 's indeclinable , that is , it 's the root of the word .
These suffixes that follow are affixed , showing the changes we saw from before . This derivational grammar , therefore , compared to the conventional school grammar , is a simple and systematic grammar .
It was thought that when we 'd process Japanese through a computer , the results of the derivational grammar would be more suitable than those of the school grammar . So , we built what 's called the System Majo morphological analyzer , based on derivational grammar .
It contains a simpler grammar and analysis than the conventional grammar .
Moreover , in terms of analysis accuracy , it was able to parse 98 percent of the Japanese sentences , and it even was able to add parts of speech . When you make use of Majo , the Japanese word kakaserareta from earlier can be parsed this way .
And , to say nothing of System Majo 's use in applied research , we are also conducting translation research toward the Uighur language . None of you may be familiar with the Uighur language , but it 's a language spoken by the peoples in the autonomous Xinjiang , Uighur region in China .
It 's a language very similar to Japanese , whose word order is practically the same , which means that you 'd be able to translate , to some extent , the respective words just by replacing them after you 'd parsed each Japanese word and applied morphological analysis .
Thus , you 'd parse the Japanese kakaserareta from earlier , and be able to translate just by replacing the various words and linking the replacements together . In sum , we 've conducted an analysis on the Japanese language , using a grammar different from school grammar , called derivational grammar .
As a result , we were able to produce a simpler analysis than the conventional method . Plus , in application , we 're conducting automated translations from Japanese to Uighur .
Making use of the similarities of these two languages , we currently can translate verbs , and we 're also working towards translating parts besides verbs - and from Uighur to Japanese . That is all .
Today , we 'd like to present our research on processing Japanese based on derivational grammar , and the applications of that grammar .
First , this is about processing Japanese from a computer , but when you attempt to process vernacular human language through a computer , you divide the input sentences through what 's known as morphological analysis . Furthermore , you conduct the processes of syntactic analysis , as well as semantic analysis , as well as contextual analysis , to process human language .
The first order of business is morphological analysis . In terms of language , because morphological analysis comes first , if you make a mistake here , everything else will be in error .
With that said , this process is extremely important . Additionally , Japanese generally does n't use spaces to separate words .
When it 's English , you can immediately grasp a single word because there are almost always spaces in the text .
As for Japanese , however , a computer ca n't understand it as it is , therefore , first , you have to separate each and every word . So , what are all these words ? The word hanseibun of apology is a noun .
And wo is a particle . Added to that are verbs , and after that are attached forms called auxiliary verbs , and you must have parts of speech affixed to each word .
In this manner , regarding Japanese , separating words in sentences and tagging parts of speech means morphological analysis .
Now , I wrote about simply using morphological analysis on verbs , but this aspect is , in truth , not so simple. It actually gets a little complicated . This is the thing you all learned as school grammar in elementary or middle school :
For example , in grammar school , you learned conjugations like kakanai , kakimasu , kaku , kaku toki , kakeba , kake , for the verb kaku .
Then there were also names like mizenkei , renyokei , shushikei , rentaikei , kateikei , and meireikei . I think all of you had to learn conjugations like the godan katsuyo or the kamiichidan katsuyo .
And it 's not just these kinds of conjugations. For instance , there 's the seru auxiliary verb that represents the causative form. But , the truth is , you do n't just apply it as it is - there 's a rule in which this seru can only be affixed to the mizenkei of a verb .
Then there is rareru , the auxiliary verb that represents the passive. But this too , of course , can only be affixed to the mizenkei. Then there 's ta , for the perfect tense , but , for some reason , while there 's irregularity in verbal inflections , there 's none for ren'yokei or meireikei .
And , there 's a rule where this can only be affixed to the ren'yokei I mentioned before .
So , because there are these kind of complicated conjugation transformations and these kind of complicated affixation rules , in order to input the word kakaserareta , you have to have a process that can deal with these kind of complicated rules .
Therefore , the biggest issue in dealing with Japanese morphological analysis is the need for processing this kind of complex inflection . With that said , I think it seems reasonable to say that , because Japanese employs conjugations , conjugation processing is necessary .
However , there is a grammar proposed saying the Japanese language does not conjugate . This is not just the school grammar all of you had thought of .
In Japanese , there is another grammar . It is derivational grammar .
Derivational grammar is the agglutinative nature of Japanese . You may be asking what this means ; the truth is , languages like Japanese , Korean , and even Turkish are quite similar , and agglutination is the quality these languages share .
Derivational grammar is what focuses on this aspect . And though verbal inflection was called conjugation before , it is believed that that is really the affixation of suffixes to the root word .
To give an example : if you had the verb kaku in conjugation transformation or grade school grammar , it would look something like kakanai , kakimasu .
Because of the transformation , it 's been called conjugation , but we transcribe using phonological notation .
Thus , you transcribe with roman letters in place of hiragana Japanese syllabary , and in so doing , all the kaku 's from before end in the K consonant. It becomes clear that this part of the word 's indeclinable , that is , it 's the root of the word .
These suffixes that follow are affixed , showing the changes we saw from before .
This derivational grammar , therefore , compared to the conventional school grammar , is a simple and systematic grammar . It was thought that when we 'd process Japanese through a computer , the results of the derivational grammar would be more suitable than those of the school grammar .
So , we built what 's called the System Majo morphological analyzer , based on derivational grammar . It contains a simpler grammar and analysis than the conventional grammar .
Moreover , in terms of analysis accuracy , it was able to parse 98 percent of the Japanese sentences , and it even was able to add parts of speech . When you make use of Majo , the Japanese word kakaserareta from earlier can be parsed this way .
And , to say nothing of System Majo 's use in applied research , we are also conducting translation research toward the Uighur language . None of you may be familiar with the Uighur language , but it 's a language spoken by the peoples in the autonomous Xinjiang , Uighur region in China .
It 's a language very similar to Japanese , whose word order is practically the same , which means that you 'd be able to translate , to some extent , the respective words just by replacing them after you 'd parsed each Japanese word and applied morphological analysis .
Thus , you 'd parse the Japanese kakaserareta from earlier , and be able to translate just by replacing the various words and linking the replacements together .
In sum , we 've conducted an analysis on the Japanese language , using a grammar different from school grammar , called derivational grammar . As a result , we were able to produce a simpler analysis than the conventional method .
Plus , in application , we 're conducting automated translations from Japanese to Uighur . Making use of the similarities of these two languages , we currently can translate verbs , and we 're also working towards translating parts besides verbs - and from Uighur to Japanese .
That is all .
Today , we 'd like to present our research on processing Japanese based on derivational grammar , and the applications of that grammar .
First , this is about processing Japanese from a computer , but when you attempt to process vernacular human language through a computer , you divide the input sentences through what 's known as morphological analysis . Furthermore , you conduct the processes of syntactic analysis , as well as semantic analysis , as well as contextual analysis , to process human language .
The first order of business is morphological analysis . In terms of language , because morphological analysis comes first , if you make a mistake here , everything else will be in error .
With that said , this process is extremely important . Additionally , Japanese generally does n't use spaces to separate words .
When it 's English , you can immediately grasp a single word because there are almost always spaces in the text . As for Japanese , however , a computer ca n't understand it as it is , therefore , first , you have to separate each and every word .
So , what are all these words ? The word hanseibun of apology is a noun .
And wo is a particle . Added to that are verbs , and after that are attached forms called auxiliary verbs , and you must have parts of speech affixed to each word .
In this manner , regarding Japanese , separating words in sentences and tagging parts of speech means morphological analysis .
Now , I wrote about simply using morphological analysis on verbs , but this aspect is , in truth , not so simple. It actually gets a little complicated . This is the thing you all learned as school grammar in elementary or middle school :
For example , in grammar school , you learned conjugations like kakanai , kakimasu , kaku , kaku toki , kakeba , kake , for the verb kaku . Then there were also names like mizenkei , renyokei , shushikei , rentaikei , kateikei , and meireikei .
I think all of you had to learn conjugations like the godan katsuyo or the kamiichidan katsuyo .
And it 's not just these kinds of conjugations. For instance , there 's the seru auxiliary verb that represents the causative form. But , the truth is , you do n't just apply it as it is - there 's a rule in which this seru can only be affixed to the mizenkei of a verb .
Then there is rareru , the auxiliary verb that represents the passive. But this too , of course , can only be affixed to the mizenkei. Then there 's ta , for the perfect tense , but , for some reason , while there 's irregularity in verbal inflections , there 's none for ren'yokei or meireikei . And , there 's a rule where this can only be affixed to the ren'yokei I mentioned before .
So , because there are these kind of complicated conjugation transformations and these kind of complicated affixation rules , in order to input the word kakaserareta , you have to have a process that can deal with these kind of complicated rules .
Therefore , the biggest issue in dealing with Japanese morphological analysis is the need for processing this kind of complex inflection .
With that said , I think it seems reasonable to say that , because Japanese employs conjugations , conjugation processing is necessary . However , there is a grammar proposed saying the Japanese language does not conjugate . This is not just the school grammar all of you had thought of .
In Japanese , there is another grammar . It is derivational grammar . Derivational grammar is the agglutinative nature of Japanese .
You may be asking what this means ; the truth is , languages like Japanese , Korean , and even Turkish are quite similar , and agglutination is the quality these languages share .
Derivational grammar is what focuses on this aspect . And though verbal inflection was called conjugation before , it is believed that that is really the affixation of suffixes to the root word .
To give an example : if you had the verb kaku in conjugation transformation or grade school grammar , it would look something like kakanai , kakimasu . Because of the transformation , it 's been called conjugation , but we transcribe using phonological notation .
Thus , you transcribe with roman letters in place of hiragana Japanese syllabary , and in so doing , all the kaku 's from before end in the K consonant. It becomes clear that this part of the word 's indeclinable , that is , it 's the root of the word . These suffixes that follow are affixed , showing the changes we saw from before .
This derivational grammar , therefore , compared to the conventional school grammar , is a simple and systematic grammar . It was thought that when we 'd process Japanese through a computer , the results of the derivational grammar would be more suitable than those of the school grammar .
So , we built what 's called the System Majo morphological analyzer , based on derivational grammar . It contains a simpler grammar and analysis than the conventional grammar . Moreover , in terms of analysis accuracy , it was able to parse 98 percent of the Japanese sentences , and it even was able to add parts of speech .
When you make use of Majo , the Japanese word kakaserareta from earlier can be parsed this way .
And , to say nothing of System Majo 's use in applied research , we are also conducting translation research toward the Uighur language . None of you may be familiar with the Uighur language , but it 's a language spoken by the peoples in the autonomous Xinjiang , Uighur region in China .
It 's a language very similar to Japanese , whose word order is practically the same , which means that you 'd be able to translate , to some extent , the respective words just by replacing them after you 'd parsed each Japanese word and applied morphological analysis .
Thus , you 'd parse the Japanese kakaserareta from earlier , and be able to translate just by replacing the various words and linking the replacements together .
In sum , we 've conducted an analysis on the Japanese language , using a grammar different from school grammar , called derivational grammar . As a result , we were able to produce a simpler analysis than the conventional method .
Plus , in application , we 're conducting automated translations from Japanese to Uighur .
Making use of the similarities of these two languages , we currently can translate verbs , and we 're also working towards translating parts besides verbs - and from Uighur to Japanese . That is all .
Today , we 'd like to present our research on processing Japanese based on derivational grammar , and the applications of that grammar .
First , this is about processing Japanese from a computer , but when you attempt to process vernacular human language through a computer , you divide the input sentences through what 's known as morphological analysis . Furthermore , you conduct the processes of syntactic analysis , as well as semantic analysis , as well as contextual analysis , to process human language .
The first order of business is morphological analysis . In terms of language , because morphological analysis comes first , if you make a mistake here , everything else will be in error .
With that said , this process is extremely important . Additionally , Japanese generally does n't use spaces to separate words .
When it 's English , you can immediately grasp a single word because there are almost always spaces in the text . As for Japanese , however , a computer ca n't understand it as it is , therefore , first , you have to separate each and every word .
So , what are all these words ? The word hanseibun of apology is a noun . And wo is a particle .
Added to that are verbs , and after that are attached forms called auxiliary verbs , and you must have parts of speech affixed to each word . In this manner , regarding Japanese , separating words in sentences and tagging parts of speech means morphological analysis .
Now , I wrote about simply using morphological analysis on verbs , but this aspect is , in truth , not so simple. It actually gets a little complicated .
This is the thing you all learned as school grammar in elementary or middle school : For example , in grammar school , you learned conjugations like kakanai , kakimasu , kaku , kaku toki , kakeba , kake , for the verb kaku .
Then there were also names like mizenkei , renyokei , shushikei , rentaikei , kateikei , and meireikei . I think all of you had to learn conjugations like the godan katsuyo or the kamiichidan katsuyo .
And it 's not just these kinds of conjugations. For instance , there 's the seru auxiliary verb that represents the causative form. But , the truth is , you do n't just apply it as it is - there 's a rule in which this seru can only be affixed to the mizenkei of a verb .
Then there is rareru , the auxiliary verb that represents the passive. But this too , of course , can only be affixed to the mizenkei. Then there 's ta , for the perfect tense , but , for some reason , while there 's irregularity in verbal inflections , there 's none for ren'yokei or meireikei .
And , there 's a rule where this can only be affixed to the ren'yokei I mentioned before .
So , because there are these kind of complicated conjugation transformations and these kind of complicated affixation rules , in order to input the word kakaserareta , you have to have a process that can deal with these kind of complicated rules .
Therefore , the biggest issue in dealing with Japanese morphological analysis is the need for processing this kind of complex inflection .
With that said , I think it seems reasonable to say that , because Japanese employs conjugations , conjugation processing is necessary .
However , there is a grammar proposed saying the Japanese language does not conjugate . This is not just the school grammar all of you had thought of .
In Japanese , there is another grammar .
It is derivational grammar . Derivational grammar is the agglutinative nature of Japanese .
You may be asking what this means ; the truth is , languages like Japanese , Korean , and even Turkish are quite similar , and agglutination is the quality these languages share .
Derivational grammar is what focuses on this aspect . And though verbal inflection was called conjugation before , it is believed that that is really the affixation of suffixes to the root word .
To give an example : if you had the verb kaku in conjugation transformation or grade school grammar , it would look something like kakanai , kakimasu .
Because of the transformation , it 's been called conjugation , but we transcribe using phonological notation .
Thus , you transcribe with roman letters in place of hiragana Japanese syllabary , and in so doing , all the kaku 's from before end in the K consonant. It becomes clear that this part of the word 's indeclinable , that is , it 's the root of the word . These suffixes that follow are affixed , showing the changes we saw from before .
This derivational grammar , therefore , compared to the conventional school grammar , is a simple and systematic grammar . It was thought that when we 'd process Japanese through a computer , the results of the derivational grammar would be more suitable than those of the school grammar .
So , we built what 's called the System Majo morphological analyzer , based on derivational grammar . It contains a simpler grammar and analysis than the conventional grammar .
Moreover , in terms of analysis accuracy , it was able to parse 98 percent of the Japanese sentences , and it even was able to add parts of speech .
When you make use of Majo , the Japanese word kakaserareta from earlier can be parsed this way . And , to say nothing of System Majo 's use in applied research , we are also conducting translation research toward the Uighur language .
None of you may be familiar with the Uighur language , but it 's a language spoken by the peoples in the autonomous Xinjiang , Uighur region in China .
It 's a language very similar to Japanese , whose word order is practically the same , which means that you 'd be able to translate , to some extent , the respective words just by replacing them after you 'd parsed each Japanese word and applied morphological analysis .
Thus , you 'd parse the Japanese kakaserareta from earlier , and be able to translate just by replacing the various words and linking the replacements together .
In sum , we 've conducted an analysis on the Japanese language , using a grammar different from school grammar , called derivational grammar . As a result , we were able to produce a simpler analysis than the conventional method . Plus , in application , we 're conducting automated translations from Japanese to Uighur .
Making use of the similarities of these two languages , we currently can translate verbs , and we 're also working towards translating parts besides verbs - and from Uighur to Japanese . That is all .
This presentation is about simultaneous interpretation performed by computers .
Research into automatic translation using computers has been carried out for a long time . This field of research is known as " machine translation . "
The most basic form of machine translation research is called " text translation . " This research deals with the translation of text written by humans .
There is also another subfield of machine translation known as " speech translation . " This is research into the automatic translation of human speech .
In contrast to these two approaches , we are thinking of making computers perform simultaneous interpretation .
A system to perform simultaneous translation is also a system for the automatic translation of human speech ; however , the usual methodology in speech translation has been to translate individual utterances .
In other words , the system would start translation after a human finishes speaking . However , in simultaneous translation , speech is translated continuously , at the same time as the speech input .
There are a number of difficulties involved in performing simultaneous translation .
When we think about what kind of difficulties we may encounter , the first thing that becomes apparent is that when we begin translating we do not know the meaning of the whole sentence . That is , a difficulty of beginning translation at the stage of not knowing the meaning of the entire sentence .
Also , for a language pairing such as Japanese and English , the word order is vastly different from language pairings such as English and German , or English and French ; it is said that doing simultaneous translation for language pairings with large differences in word order is extremely difficult .
In addition , there is also the difficulty of having to keep up with the speed of the speaker .
However , despite the difficulties involved in performing simultaneous interpretation , professional interpreters do it as a matter of course . So , how do these interpreters do it ?
First , they use the situation and the context as they translate , to predict what the speaker might say . They also maintain flexibility in the middle of sentences by using a style of translation that can cope with a wide variety of subsequent language .
Furthermore , they do not translate everything that the speaker says , but rather choose what things need to be translated .
Interpreters use a variety of such techniques when performing simultaneous translation . In light of this , we thought that to make computers perform simultaneous interpretation there is a need to teach computers the same kinds of techniques .
To do this , first we need to find out , in detail , what these translation techniques are .
To satisfy this need , we created a simultaneous interpretation database . This database contains a large number of samples of actual translations made by professional interpreters .
We are learning about the kinds of techniques used by simultaneous interpreters through analysing this data . By repeating this process we hope to find how best to perform translation , and how we can improve our methods .
I will now show you how this database is constructed .
First the sound is recorded by a native speaker ; in other words , a Japanese person will speak Japanese .
Alternatively , a native English speaker will record English speech. Then , we record an interpreter translating it from English to Japanese or from Japanese to English using simultaneous interpretation . After that , we listen to the recordings and make transcriptions .
We then think of what additional information might be necessary to analyse the transcriptions , and add that information to the database . We will now show you some example transcriptions ; this is a transcription of an original speech sample , and this is a transcription of that speech sample as translated by an interpreter .
The context is a phone call to make a hotel reservation. When you make an utterance like this ... This red part shows when the utterance started and when it stopped .
You can see a number of other figures like this ; we are using a variety of such information to make the transcription data . We are using this data to analyse how interpreters are performing simultaneous interpretation .
Now , I will show you an example of the kind of technique that we found through our analysis . An interpreter heard the sentence " Japanese is an experimental field , " and translated it as follows :
However , this sentence continued with the words " or that is one way of looking at it . " I have already told you the English sentence , but it is not really a suitable translation of the whole Japanese sentence. In situations like this , how should we go about the process of translation ?
One simple answer to this question is to correct the speech that is output . This is the method of monitoring the translation as it is made to decide whether it is suitable , and if not , making a correction to produce the correct meaning .
However , by using this method we must output unnecessary words .
In contrast , the interpreter said the following after the end of the sentence :
This is a form that leads on from this sentence to this sentence ; it is a perfect fit for the Japanese sentence . In other words , by using this kind of form , we can perform simultaneous interpretation without making corrections .
To sum up , for computers to perform simultaneous interpretation , we need to program them to use the same kinds of techniques that professional interpreters use .
To investigate these techniques , we made a database .
Finally , I showed you an example of a technique used by an interpreter . I would like to continue doing this kind of analysis as part of my further research . Thank you very much for listening .
This presentation is about simultaneous interpretation performed by computers . Research into automatic translation using computers has been carried out for a long time .
This field of research is known as " machine translation . "
The most basic form of machine translation research is called " text translation . " This research deals with the translation of text written by humans .
There is also another subfield of machine translation known as " speech translation . "
This is research into the automatic translation of human speech . In contrast to these two approaches , we are thinking of making computers perform simultaneous interpretation .
A system to perform simultaneous translation is also a system for the automatic translation of human speech ; however , the usual methodology in speech translation has been to translate individual utterances .
In other words , the system would start translation after a human finishes speaking . However , in simultaneous translation , speech is translated continuously , at the same time as the speech input .
There are a number of difficulties involved in performing simultaneous translation . When we think about what kind of difficulties we may encounter , the first thing that becomes apparent is that when we begin translating we do not know the meaning of the whole sentence .
That is , a difficulty of beginning translation at the stage of not knowing the meaning of the entire sentence . Also , for a language pairing such as Japanese and English , the word order is vastly different from language pairings such as English and German , or English and French ; it is said that doing simultaneous translation for language pairings with large differences in word order is extremely difficult .
In addition , there is also the difficulty of having to keep up with the speed of the speaker .
However , despite the difficulties involved in performing simultaneous interpretation , professional interpreters do it as a matter of course .
So , how do these interpreters do it ?
First , they use the situation and the context as they translate , to predict what the speaker might say . They also maintain flexibility in the middle of sentences by using a style of translation that can cope with a wide variety of subsequent language .
Furthermore , they do not translate everything that the speaker says , but rather choose what things need to be translated .
Interpreters use a variety of such techniques when performing simultaneous translation .
In light of this , we thought that to make computers perform simultaneous interpretation there is a need to teach computers the same kinds of techniques . To do this , first we need to find out , in detail , what these translation techniques are .
To satisfy this need , we created a simultaneous interpretation database . This database contains a large number of samples of actual translations made by professional interpreters .
We are learning about the kinds of techniques used by simultaneous interpreters through analysing this data . By repeating this process we hope to find how best to perform translation , and how we can improve our methods .
I will now show you how this database is constructed .
First the sound is recorded by a native speaker ; in other words , a Japanese person will speak Japanese . Alternatively , a native English speaker will record English speech. Then , we record an interpreter translating it from English to Japanese or from Japanese to English using simultaneous interpretation .
After that , we listen to the recordings and make transcriptions . We then think of what additional information might be necessary to analyse the transcriptions , and add that information to the database .
We will now show you some example transcriptions ; this is a transcription of an original speech sample , and this is a transcription of that speech sample as translated by an interpreter . The context is a phone call to make a hotel reservation. When you make an utterance like this ...
This red part shows when the utterance started and when it stopped . You can see a number of other figures like this ; we are using a variety of such information to make the transcription data .
We are using this data to analyse how interpreters are performing simultaneous interpretation . Now , I will show you an example of the kind of technique that we found through our analysis .
An interpreter heard the sentence " Japanese is an experimental field , " and translated it as follows :
However , this sentence continued with the words " or that is one way of looking at it . "
I have already told you the English sentence , but it is not really a suitable translation of the whole Japanese sentence. In situations like this , how should we go about the process of translation ?
One simple answer to this question is to correct the speech that is output .
This is the method of monitoring the translation as it is made to decide whether it is suitable , and if not , making a correction to produce the correct meaning . However , by using this method we must output unnecessary words .
In contrast , the interpreter said the following after the end of the sentence :
This is a form that leads on from this sentence to this sentence ; it is a perfect fit for the Japanese sentence .
In other words , by using this kind of form , we can perform simultaneous interpretation without making corrections . To sum up , for computers to perform simultaneous interpretation , we need to program them to use the same kinds of techniques that professional interpreters use .
To investigate these techniques , we made a database .
Finally , I showed you an example of a technique used by an interpreter . I would like to continue doing this kind of analysis as part of my further research .
Thank you very much for listening .
This presentation is about simultaneous interpretation performed by computers .
Research into automatic translation using computers has been carried out for a long time . This field of research is known as " machine translation . "
The most basic form of machine translation research is called " text translation . " This research deals with the translation of text written by humans .
There is also another subfield of machine translation known as " speech translation . " This is research into the automatic translation of human speech .
In contrast to these two approaches , we are thinking of making computers perform simultaneous interpretation .
A system to perform simultaneous translation is also a system for the automatic translation of human speech ; however , the usual methodology in speech translation has been to translate individual utterances .
In other words , the system would start translation after a human finishes speaking . However , in simultaneous translation , speech is translated continuously , at the same time as the speech input .
There are a number of difficulties involved in performing simultaneous translation . When we think about what kind of difficulties we may encounter , the first thing that becomes apparent is that when we begin translating we do not know the meaning of the whole sentence .
That is , a difficulty of beginning translation at the stage of not knowing the meaning of the entire sentence .
Also , for a language pairing such as Japanese and English , the word order is vastly different from language pairings such as English and German , or English and French ; it is said that doing simultaneous translation for language pairings with large differences in word order is extremely difficult .
In addition , there is also the difficulty of having to keep up with the speed of the speaker .
However , despite the difficulties involved in performing simultaneous interpretation , professional interpreters do it as a matter of course . So , how do these interpreters do it ?
First , they use the situation and the context as they translate , to predict what the speaker might say .
They also maintain flexibility in the middle of sentences by using a style of translation that can cope with a wide variety of subsequent language .
Furthermore , they do not translate everything that the speaker says , but rather choose what things need to be translated .
Interpreters use a variety of such techniques when performing simultaneous translation .
In light of this , we thought that to make computers perform simultaneous interpretation there is a need to teach computers the same kinds of techniques . To do this , first we need to find out , in detail , what these translation techniques are .
To satisfy this need , we created a simultaneous interpretation database . This database contains a large number of samples of actual translations made by professional interpreters .
We are learning about the kinds of techniques used by simultaneous interpreters through analysing this data . By repeating this process we hope to find how best to perform translation , and how we can improve our methods .
I will now show you how this database is constructed .
First the sound is recorded by a native speaker ; in other words , a Japanese person will speak Japanese . Alternatively , a native English speaker will record English speech. Then , we record an interpreter translating it from English to Japanese or from Japanese to English using simultaneous interpretation .
After that , we listen to the recordings and make transcriptions . We then think of what additional information might be necessary to analyse the transcriptions , and add that information to the database .
We will now show you some example transcriptions ; this is a transcription of an original speech sample , and this is a transcription of that speech sample as translated by an interpreter . The context is a phone call to make a hotel reservation. When you make an utterance like this ...
This red part shows when the utterance started and when it stopped . You can see a number of other figures like this ; we are using a variety of such information to make the transcription data .
We are using this data to analyse how interpreters are performing simultaneous interpretation .
Now , I will show you an example of the kind of technique that we found through our analysis .
An interpreter heard the sentence " Japanese is an experimental field , " and translated it as follows : However , this sentence continued with the words " or that is one way of looking at it . "
I have already told you the English sentence , but it is not really a suitable translation of the whole Japanese sentence. In situations like this , how should we go about the process of translation ? One simple answer to this question is to correct the speech that is output .
This is the method of monitoring the translation as it is made to decide whether it is suitable , and if not , making a correction to produce the correct meaning . However , by using this method we must output unnecessary words .
In contrast , the interpreter said the following after the end of the sentence : This is a form that leads on from this sentence to this sentence ; it is a perfect fit for the Japanese sentence .
In other words , by using this kind of form , we can perform simultaneous interpretation without making corrections . To sum up , for computers to perform simultaneous interpretation , we need to program them to use the same kinds of techniques that professional interpreters use .
To investigate these techniques , we made a database .
Finally , I showed you an example of a technique used by an interpreter . I would like to continue doing this kind of analysis as part of my further research .
Thank you very much for listening .
This presentation is about simultaneous interpretation performed by computers .
Research into automatic translation using computers has been carried out for a long time . This field of research is known as " machine translation . "
The most basic form of machine translation research is called " text translation . " This research deals with the translation of text written by humans .
There is also another subfield of machine translation known as " speech translation . " This is research into the automatic translation of human speech .
In contrast to these two approaches , we are thinking of making computers perform simultaneous interpretation . A system to perform simultaneous translation is also a system for the automatic translation of human speech ; however , the usual methodology in speech translation has been to translate individual utterances .
In other words , the system would start translation after a human finishes speaking . However , in simultaneous translation , speech is translated continuously , at the same time as the speech input .
There are a number of difficulties involved in performing simultaneous translation . When we think about what kind of difficulties we may encounter , the first thing that becomes apparent is that when we begin translating we do not know the meaning of the whole sentence .
That is , a difficulty of beginning translation at the stage of not knowing the meaning of the entire sentence .
Also , for a language pairing such as Japanese and English , the word order is vastly different from language pairings such as English and German , or English and French ; it is said that doing simultaneous translation for language pairings with large differences in word order is extremely difficult .
In addition , there is also the difficulty of having to keep up with the speed of the speaker .
However , despite the difficulties involved in performing simultaneous interpretation , professional interpreters do it as a matter of course . So , how do these interpreters do it ?
First , they use the situation and the context as they translate , to predict what the speaker might say . They also maintain flexibility in the middle of sentences by using a style of translation that can cope with a wide variety of subsequent language .
Furthermore , they do not translate everything that the speaker says , but rather choose what things need to be translated . Interpreters use a variety of such techniques when performing simultaneous translation .
In light of this , we thought that to make computers perform simultaneous interpretation there is a need to teach computers the same kinds of techniques . To do this , first we need to find out , in detail , what these translation techniques are .
To satisfy this need , we created a simultaneous interpretation database . This database contains a large number of samples of actual translations made by professional interpreters .
We are learning about the kinds of techniques used by simultaneous interpreters through analysing this data . By repeating this process we hope to find how best to perform translation , and how we can improve our methods .
I will now show you how this database is constructed .
First the sound is recorded by a native speaker ; in other words , a Japanese person will speak Japanese . Alternatively , a native English speaker will record English speech. Then , we record an interpreter translating it from English to Japanese or from Japanese to English using simultaneous interpretation .
After that , we listen to the recordings and make transcriptions . We then think of what additional information might be necessary to analyse the transcriptions , and add that information to the database .
We will now show you some example transcriptions ; this is a transcription of an original speech sample , and this is a transcription of that speech sample as translated by an interpreter .
The context is a phone call to make a hotel reservation. When you make an utterance like this ...
This red part shows when the utterance started and when it stopped . You can see a number of other figures like this ; we are using a variety of such information to make the transcription data .
We are using this data to analyse how interpreters are performing simultaneous interpretation . Now , I will show you an example of the kind of technique that we found through our analysis .
An interpreter heard the sentence " Japanese is an experimental field , " and translated it as follows :
However , this sentence continued with the words " or that is one way of looking at it . " I have already told you the English sentence , but it is not really a suitable translation of the whole Japanese sentence. In situations like this , how should we go about the process of translation ?
One simple answer to this question is to correct the speech that is output . This is the method of monitoring the translation as it is made to decide whether it is suitable , and if not , making a correction to produce the correct meaning .
However , by using this method we must output unnecessary words . In contrast , the interpreter said the following after the end of the sentence :
This is a form that leads on from this sentence to this sentence ; it is a perfect fit for the Japanese sentence . In other words , by using this kind of form , we can perform simultaneous interpretation without making corrections .
To sum up , for computers to perform simultaneous interpretation , we need to program them to use the same kinds of techniques that professional interpreters use . To investigate these techniques , we made a database .
Finally , I showed you an example of a technique used by an interpreter . I would like to continue doing this kind of analysis as part of my further research .
Thank you very much for listening .
Today 's presentation is entitled , " Designing a multi-domain speech dialog system " .
In recent years , many researchers are conducting studies on speech dialog systems .
As a result , we now have practical systems that can be used for specific task domains .
For example , there is a system called Jupiter . This speech dialog system is used to provide weather information on the telephones .
Another example is the Tossberg 2. This system can take your orders at fast food restaurants .
And lastly , we have the Thinkmail system. It is being developed at our lab. This system understands Japanese conversations and organizes your e-mails accordingly .
What do all these speech dialog systems have in common ? Each system can only process one task domain . In Jupiter 's case , it can only process weather information , and Tossberg can only take your hamburger orders .
Similarly , Thinkmail can only manage emails . In these examples , a single system can not process multiple task domains .
If we want to handle multiple task domains , it is difficult to do so using these systems. For instance , think about a speech dialog system used inside a car . The system must be able to control navigation systems , audio , and air conditioning simultaneously based on the user 's voice .
If we use the current systems , we will need one microphone for navigation , one for air conditioning , and one for audio. In other words , the number of task domains equals the number of microphones , which is a problem in terms of usability .
So we started to think about how we can solve this problem and contemplated the possibility of a system that can process multiple domains with one audio input .
We call it , the multi-domain speech dialog system . First , we thought about essential features that the multi-domain system should possess .
At the end of the day , we came up with three features that such a system should have .
First , I 'll explain about expandability .
What do we mean by expandability ? We think that a multi-domain system must be able to expand itself to multiple domains . Furthermore , it should be able to add new task domains with ease .
The next feature is scalability .
Expandability allows the system to access multiple domains with ease. Great. But if it adds too much domains and the system slows down , that can be a problem .
Therefore , even when processing multiple domains , the system should be able to maintain a reasonable speed. This is what we refer to as scalability .
The last feature is usability. Even if we fulfill requirements for expandability and scalability , the system must be easy to use for the user . So , no matter which domain the user is trying to control , they should be able to do so with ease , as if they 're using a single domain system .
Next , we thought about a multi-domain system design that fulfills all three requirements and came up with a design policy .
First , we defined multi-domain speech dialog system as a collection of speech dialog systems that process independent single-domains . With this definition , we can combine multiple singe-domain systems to build our multi-domain system .
By doing so , we can add/remove speech dialogs systems easily . This fulfills our first requirement mentioned earlier , expandability .
Moreover , if we can sort the incoming audio to the appropriate system , it will fulfill another requirement , usability . And lastly , if we can establish a hierarchical system , it will fulfill the last requirement , scalability .
Next , I 'll explain about the architecture of our system . How do we establish a multi-domain speech dialog system anyways ?
First , we create the manager and work module. From there , we start building a hierarchical system .
The data that goes through the system is called a " fragment " . Two types of fragments , input and output , flow through the system during operation .
Now let me talk more about work module and manager. The manager sorts the input fragments , which I 'll explain in detail alter , to the work modules. Then , it consolidates the output fragments coming out of the work module and manages them .
The work module is found in each domains. In the car example I gave earlier , there is a work module for navigation , another for air conditioning , and so on .
This diagram shows an example of a simple system based on the architecture mentioned earlier . First , there is one manager in the middle. This manager connects the two work modules .
The brief flow goes like this. The results from the speech recognition system goes to the manager. It then sorts the results to the two work modules in input fragment format .
Each work module analyzes the results independently and sends it back to the manager .
Then the manager uses these output fragments to determine which work module is suited for the job. This is a brief outline of how the system works .
OK , I will wrap it up . Today , I presented a suggestion for a multi-domain speech dialog system architecture .
Our next task is to mount a system based on this architecture . We also plan to evaluate this mounted system .
That is all for today , thank you .
Today 's presentation is entitled , " Designing a multi-domain speech dialog system " .
In recent years , many researchers are conducting studies on speech dialog systems .
As a result , we now have practical systems that can be used for specific task domains .
For example , there is a system called Jupiter .
This speech dialog system is used to provide weather information on the telephones .
Another example is the Tossberg 2. This system can take your orders at fast food restaurants .
And lastly , we have the Thinkmail system. It is being developed at our lab. This system understands Japanese conversations and organizes your e-mails accordingly .
What do all these speech dialog systems have in common ? Each system can only process one task domain .
In Jupiter 's case , it can only process weather information , and Tossberg can only take your hamburger orders . Similarly , Thinkmail can only manage emails .
In these examples , a single system can not process multiple task domains .
If we want to handle multiple task domains , it is difficult to do so using these systems. For instance , think about a speech dialog system used inside a car .
The system must be able to control navigation systems , audio , and air conditioning simultaneously based on the user 's voice .
If we use the current systems , we will need one microphone for navigation , one for air conditioning , and one for audio. In other words , the number of task domains equals the number of microphones , which is a problem in terms of usability .
So we started to think about how we can solve this problem and contemplated the possibility of a system that can process multiple domains with one audio input .
We call it , the multi-domain speech dialog system .
First , we thought about essential features that the multi-domain system should possess .
At the end of the day , we came up with three features that such a system should have .
First , I 'll explain about expandability .
What do we mean by expandability ? We think that a multi-domain system must be able to expand itself to multiple domains .
Furthermore , it should be able to add new task domains with ease .
The next feature is scalability . Expandability allows the system to access multiple domains with ease. Great. But if it adds too much domains and the system slows down , that can be a problem .
Therefore , even when processing multiple domains , the system should be able to maintain a reasonable speed. This is what we refer to as scalability .
The last feature is usability. Even if we fulfill requirements for expandability and scalability , the system must be easy to use for the user . So , no matter which domain the user is trying to control , they should be able to do so with ease , as if they 're using a single domain system .
Next , we thought about a multi-domain system design that fulfills all three requirements and came up with a design policy .
First , we defined multi-domain speech dialog system as a collection of speech dialog systems that process independent single-domains .
With this definition , we can combine multiple singe-domain systems to build our multi-domain system .
By doing so , we can add/remove speech dialogs systems easily . This fulfills our first requirement mentioned earlier , expandability .
Moreover , if we can sort the incoming audio to the appropriate system , it will fulfill another requirement , usability .
And lastly , if we can establish a hierarchical system , it will fulfill the last requirement , scalability .
Next , I 'll explain about the architecture of our system . How do we establish a multi-domain speech dialog system anyways ?
First , we create the manager and work module. From there , we start building a hierarchical system .
The data that goes through the system is called a " fragment " . Two types of fragments , input and output , flow through the system during operation .
Now let me talk more about work module and manager. The manager sorts the input fragments , which I 'll explain in detail alter , to the work modules. Then , it consolidates the output fragments coming out of the work module and manages them .
The work module is found in each domains. In the car example I gave earlier , there is a work module for navigation , another for air conditioning , and so on .
This diagram shows an example of a simple system based on the architecture mentioned earlier .
First , there is one manager in the middle. This manager connects the two work modules .
The brief flow goes like this. The results from the speech recognition system goes to the manager. It then sorts the results to the two work modules in input fragment format . Each work module analyzes the results independently and sends it back to the manager .
Then the manager uses these output fragments to determine which work module is suited for the job. This is a brief outline of how the system works .
OK , I will wrap it up . Today , I presented a suggestion for a multi-domain speech dialog system architecture .
Our next task is to mount a system based on this architecture .
We also plan to evaluate this mounted system . That is all for today , thank you .
Today 's presentation is entitled , " Designing a multi-domain speech dialog system " .
In recent years , many researchers are conducting studies on speech dialog systems .
As a result , we now have practical systems that can be used for specific task domains . For example , there is a system called Jupiter .
This speech dialog system is used to provide weather information on the telephones .
Another example is the Tossberg 2. This system can take your orders at fast food restaurants .
And lastly , we have the Thinkmail system. It is being developed at our lab. This system understands Japanese conversations and organizes your e-mails accordingly .
What do all these speech dialog systems have in common ? Each system can only process one task domain .
In Jupiter 's case , it can only process weather information , and Tossberg can only take your hamburger orders . Similarly , Thinkmail can only manage emails . In these examples , a single system can not process multiple task domains .
If we want to handle multiple task domains , it is difficult to do so using these systems. For instance , think about a speech dialog system used inside a car .
The system must be able to control navigation systems , audio , and air conditioning simultaneously based on the user 's voice .
If we use the current systems , we will need one microphone for navigation , one for air conditioning , and one for audio. In other words , the number of task domains equals the number of microphones , which is a problem in terms of usability .
So we started to think about how we can solve this problem and contemplated the possibility of a system that can process multiple domains with one audio input .
We call it , the multi-domain speech dialog system . First , we thought about essential features that the multi-domain system should possess .
At the end of the day , we came up with three features that such a system should have . First , I 'll explain about expandability .
What do we mean by expandability ? We think that a multi-domain system must be able to expand itself to multiple domains .
Furthermore , it should be able to add new task domains with ease . The next feature is scalability .
Expandability allows the system to access multiple domains with ease. Great. But if it adds too much domains and the system slows down , that can be a problem .
Therefore , even when processing multiple domains , the system should be able to maintain a reasonable speed. This is what we refer to as scalability .
The last feature is usability. Even if we fulfill requirements for expandability and scalability , the system must be easy to use for the user . So , no matter which domain the user is trying to control , they should be able to do so with ease , as if they 're using a single domain system .
Next , we thought about a multi-domain system design that fulfills all three requirements and came up with a design policy .
First , we defined multi-domain speech dialog system as a collection of speech dialog systems that process independent single-domains . With this definition , we can combine multiple singe-domain systems to build our multi-domain system .
By doing so , we can add/remove speech dialogs systems easily . This fulfills our first requirement mentioned earlier , expandability .
Moreover , if we can sort the incoming audio to the appropriate system , it will fulfill another requirement , usability . And lastly , if we can establish a hierarchical system , it will fulfill the last requirement , scalability .
Next , I 'll explain about the architecture of our system . How do we establish a multi-domain speech dialog system anyways ?
First , we create the manager and work module. From there , we start building a hierarchical system . The data that goes through the system is called a " fragment " . Two types of fragments , input and output , flow through the system during operation .
Now let me talk more about work module and manager. The manager sorts the input fragments , which I 'll explain in detail alter , to the work modules. Then , it consolidates the output fragments coming out of the work module and manages them .
The work module is found in each domains. In the car example I gave earlier , there is a work module for navigation , another for air conditioning , and so on .
This diagram shows an example of a simple system based on the architecture mentioned earlier .
First , there is one manager in the middle. This manager connects the two work modules .
The brief flow goes like this. The results from the speech recognition system goes to the manager. It then sorts the results to the two work modules in input fragment format . Each work module analyzes the results independently and sends it back to the manager .
Then the manager uses these output fragments to determine which work module is suited for the job. This is a brief outline of how the system works . OK , I will wrap it up .
Today , I presented a suggestion for a multi-domain speech dialog system architecture . Our next task is to mount a system based on this architecture .
We also plan to evaluate this mounted system . That is all for today , thank you .
Today 's presentation is entitled , " Designing a multi-domain speech dialog system " .
In recent years , many researchers are conducting studies on speech dialog systems .
As a result , we now have practical systems that can be used for specific task domains .
For example , there is a system called Jupiter . This speech dialog system is used to provide weather information on the telephones .
Another example is the Tossberg 2. This system can take your orders at fast food restaurants .
And lastly , we have the Thinkmail system. It is being developed at our lab. This system understands Japanese conversations and organizes your e-mails accordingly .
What do all these speech dialog systems have in common ? Each system can only process one task domain . In Jupiter 's case , it can only process weather information , and Tossberg can only take your hamburger orders .
Similarly , Thinkmail can only manage emails . In these examples , a single system can not process multiple task domains .
If we want to handle multiple task domains , it is difficult to do so using these systems. For instance , think about a speech dialog system used inside a car .
The system must be able to control navigation systems , audio , and air conditioning simultaneously based on the user 's voice .
If we use the current systems , we will need one microphone for navigation , one for air conditioning , and one for audio. In other words , the number of task domains equals the number of microphones , which is a problem in terms of usability .
So we started to think about how we can solve this problem and contemplated the possibility of a system that can process multiple domains with one audio input .
We call it , the multi-domain speech dialog system . First , we thought about essential features that the multi-domain system should possess .
At the end of the day , we came up with three features that such a system should have .
First , I 'll explain about expandability . What do we mean by expandability ? We think that a multi-domain system must be able to expand itself to multiple domains .
Furthermore , it should be able to add new task domains with ease . The next feature is scalability .
Expandability allows the system to access multiple domains with ease. Great. But if it adds too much domains and the system slows down , that can be a problem .
Therefore , even when processing multiple domains , the system should be able to maintain a reasonable speed. This is what we refer to as scalability .
The last feature is usability. Even if we fulfill requirements for expandability and scalability , the system must be easy to use for the user .
So , no matter which domain the user is trying to control , they should be able to do so with ease , as if they 're using a single domain system .
Next , we thought about a multi-domain system design that fulfills all three requirements and came up with a design policy .
First , we defined multi-domain speech dialog system as a collection of speech dialog systems that process independent single-domains . With this definition , we can combine multiple singe-domain systems to build our multi-domain system .
By doing so , we can add/remove speech dialogs systems easily . This fulfills our first requirement mentioned earlier , expandability .
Moreover , if we can sort the incoming audio to the appropriate system , it will fulfill another requirement , usability .
And lastly , if we can establish a hierarchical system , it will fulfill the last requirement , scalability .
Next , I 'll explain about the architecture of our system . How do we establish a multi-domain speech dialog system anyways ?
First , we create the manager and work module. From there , we start building a hierarchical system .
The data that goes through the system is called a " fragment " . Two types of fragments , input and output , flow through the system during operation .
Now let me talk more about work module and manager. The manager sorts the input fragments , which I 'll explain in detail alter , to the work modules. Then , it consolidates the output fragments coming out of the work module and manages them .
The work module is found in each domains. In the car example I gave earlier , there is a work module for navigation , another for air conditioning , and so on .
This diagram shows an example of a simple system based on the architecture mentioned earlier .
First , there is one manager in the middle. This manager connects the two work modules .
The brief flow goes like this. The results from the speech recognition system goes to the manager. It then sorts the results to the two work modules in input fragment format .
Each work module analyzes the results independently and sends it back to the manager .
Then the manager uses these output fragments to determine which work module is suited for the job. This is a brief outline of how the system works . OK , I will wrap it up .
Today , I presented a suggestion for a multi-domain speech dialog system architecture . Our next task is to mount a system based on this architecture .
We also plan to evaluate this mounted system . That is all for today , thank you .
Today I would like to announce a gradual method for Japanese language dependency analysis .
In recent years , there has been a strong demand for a conversation handling system that uses Japanese Japanese spoken language .
And being spoken language we are dealing with here , it is desirable that it be a real-time system that immediately outputs what is input into it .
In order to implement a real-time system , people are seeking a method that gradually interprets natural language .
This would be a method where inputted text would be interpreted occurrence by occurrence. For example , for a sentence like " I met his friend at this store. " after first entering the clause " I met " the system would interpret the " his friend " then once " at this store " is inputted , the system would interpret the inputted text from " I met his friend " to " at this store . "
For Japanese analysis , the Japanese language dependency analysis method is widely utilized . As this chart shows , this analysis would decide a case relationship for each clause that says " I met his friend at the store . "
For example in a sentence like " I met his friend at this store " , the phrase " at this store " is linked to the word " met . " The word " his " is linked to the word " friend . " The word " friend " is linked to the word " met . "
The analysis uses a system in which every two clauses the words are in a head-modifier relationship with each other , and the system in turn decides the structure for that sentence .
One might ask why is there such an analysis , and why is it being used ? You can say that the reason is that the Japanese language is characteristically quite lenient when it comes to word order .
For instance if you were to substitute the position of " at this store " and " his friend " , the meaning would be the same .
Additionally , unlike English , the subject is frequently omitted in Japanese .
For example in this sentence Japanese literally reads " at this store met his friend " a subject like " I " could be inserted , but since it is already understood by the speaker and listener it has been omitted in the Japanese sentence .
As I said earlier , the goal of this system of analysis is to decide the dependency relationship between two clauses .
In this case , using dependency analysis , an input that is to be a receiving clause becomes necessary for the linked clause . I apologize for using a similar example .
If you consider a situation where a clause is linked with the very last clause in a sentence , the system would end up having to wait until the final text is input and the gradualness necessary to implement the real-time conversation handling system will not be sufficient . Therefore , I propose a gradual Japanese dependency analysis method .
In my proposed method , when an input 's receiver of a linked clause does n't yet exist , the system would predict that receiver clause and use that clause to create a dependency construction . First , I will explain the characteristics of the system I am proposing .
A dependency construction is created from dependency grammar and demonstrates the likelihood of dependency between two clauses .
Furthermore , there are restrictions in this dependency construction , of these three the first one is the non-intersection of dependencies .
In other words , dependent relations can not intersect each other .
The second restriction is the solitariness of the link , in other words there can be only one link per clause .
The last restriction is posterior modification , which in the case of Japanese would mean only considering a dependency relationship from the anterior to the posterior of the clause .
Using these three restrictions , would mean going on to create a dependency construction that satisfies these restrictions .
Now I will give a summary regarding gradual dependency analysis . With gradual dependency analysis , it is necessary to proceed with analysis in a situation where a receiver clause for an inputted clause has not yet been inputted .
The system would use dependency grammar for such a case , and go on to create a dependency construction for the input up to there using that construction or a predicted clause while predicting the link 's part of speech .
For instance , in a situation where clauses like " At this store " followed by " a book " are entered , each " at the store " is linked to a verb and each " a book " is further linked to a verb. When is obtained that they are the same verb , you can create a construction like in this chart where " at the store " and " a book " are linked to two of the same verbs .
By creating a construction like this , you can create a dependency relation for these two clauses before a receiver clause for them is entered , and because of that you can grasp the structure of the sentence to an extent before the word " this " is entered .
That sums up everything . At this time I have explained a method of explanation for a method of gradual Japanese language dependency analysis .
This method uses dependency grammar and clauses predicted from that grammar to create dependency constructions .
In this way , by using predicted clauses and creating dependency constructions , there is the advantage of being able to create its dependency construction before a receiver clause is entered for that input .
I will implement this method on a Linux workstation using the GNU common lisp .
This concludes my explanation .
Today I would like to announce a gradual method for Japanese language dependency analysis . In recent years , there has been a strong demand for a conversation handling system that uses Japanese Japanese spoken language .
And being spoken language we are dealing with here , it is desirable that it be a real-time system that immediately outputs what is input into it . In order to implement a real-time system , people are seeking a method that gradually interprets natural language .
This would be a method where inputted text would be interpreted occurrence by occurrence. For example , for a sentence like " I met his friend at this store. " after first entering the clause " I met " the system would interpret the " his friend " then once " at this store " is inputted , the system would interpret the inputted text from " I met his friend " to " at this store . "
For Japanese analysis , the Japanese language dependency analysis method is widely utilized .
As this chart shows , this analysis would decide a case relationship for each clause that says " I met his friend at the store . " For example in a sentence like " I met his friend at this store " , the phrase " at this store " is linked to the word " met . "
The word " his " is linked to the word " friend . " The word " friend " is linked to the word " met . " The analysis uses a system in which every two clauses the words are in a head-modifier relationship with each other , and the system in turn decides the structure for that sentence .
One might ask why is there such an analysis , and why is it being used ? You can say that the reason is that the Japanese language is characteristically quite lenient when it comes to word order .
For instance if you were to substitute the position of " at this store " and " his friend " , the meaning would be the same . Additionally , unlike English , the subject is frequently omitted in Japanese .
For example in this sentence Japanese literally reads " at this store met his friend " a subject like " I " could be inserted , but since it is already understood by the speaker and listener it has been omitted in the Japanese sentence .
As I said earlier , the goal of this system of analysis is to decide the dependency relationship between two clauses .
In this case , using dependency analysis , an input that is to be a receiving clause becomes necessary for the linked clause . I apologize for using a similar example .
If you consider a situation where a clause is linked with the very last clause in a sentence , the system would end up having to wait until the final text is input and the gradualness necessary to implement the real-time conversation handling system will not be sufficient .
Therefore , I propose a gradual Japanese dependency analysis method . In my proposed method , when an input 's receiver of a linked clause does n't yet exist , the system would predict that receiver clause and use that clause to create a dependency construction .
First , I will explain the characteristics of the system I am proposing . A dependency construction is created from dependency grammar and demonstrates the likelihood of dependency between two clauses .
Furthermore , there are restrictions in this dependency construction , of these three the first one is the non-intersection of dependencies .
In other words , dependent relations can not intersect each other . The second restriction is the solitariness of the link , in other words there can be only one link per clause .
The last restriction is posterior modification , which in the case of Japanese would mean only considering a dependency relationship from the anterior to the posterior of the clause .
Using these three restrictions , would mean going on to create a dependency construction that satisfies these restrictions . Now I will give a summary regarding gradual dependency analysis .
With gradual dependency analysis , it is necessary to proceed with analysis in a situation where a receiver clause for an inputted clause has not yet been inputted .
The system would use dependency grammar for such a case , and go on to create a dependency construction for the input up to there using that construction or a predicted clause while predicting the link 's part of speech .
For instance , in a situation where clauses like " At this store " followed by " a book " are entered , each " at the store " is linked to a verb and each " a book " is further linked to a verb. When is obtained that they are the same verb , you can create a construction like in this chart where " at the store " and " a book " are linked to two of the same verbs .
By creating a construction like this , you can create a dependency relation for these two clauses before a receiver clause for them is entered , and because of that you can grasp the structure of the sentence to an extent before the word " this " is entered . That sums up everything .
At this time I have explained a method of explanation for a method of gradual Japanese language dependency analysis . This method uses dependency grammar and clauses predicted from that grammar to create dependency constructions .
In this way , by using predicted clauses and creating dependency constructions , there is the advantage of being able to create its dependency construction before a receiver clause is entered for that input . I will implement this method on a Linux workstation using the GNU common lisp .
This concludes my explanation .
Today I would like to announce a gradual method for Japanese language dependency analysis . In recent years , there has been a strong demand for a conversation handling system that uses Japanese Japanese spoken language .
And being spoken language we are dealing with here , it is desirable that it be a real-time system that immediately outputs what is input into it .
In order to implement a real-time system , people are seeking a method that gradually interprets natural language .
This would be a method where inputted text would be interpreted occurrence by occurrence. For example , for a sentence like " I met his friend at this store. " after first entering the clause " I met " the system would interpret the " his friend " then once " at this store " is inputted , the system would interpret the inputted text from " I met his friend " to " at this store . "
For Japanese analysis , the Japanese language dependency analysis method is widely utilized .
As this chart shows , this analysis would decide a case relationship for each clause that says " I met his friend at the store . " For example in a sentence like " I met his friend at this store " , the phrase " at this store " is linked to the word " met . " The word " his " is linked to the word " friend . "
The word " friend " is linked to the word " met . " The analysis uses a system in which every two clauses the words are in a head-modifier relationship with each other , and the system in turn decides the structure for that sentence .
One might ask why is there such an analysis , and why is it being used ? You can say that the reason is that the Japanese language is characteristically quite lenient when it comes to word order .
For instance if you were to substitute the position of " at this store " and " his friend " , the meaning would be the same . Additionally , unlike English , the subject is frequently omitted in Japanese .
For example in this sentence Japanese literally reads " at this store met his friend " a subject like " I " could be inserted , but since it is already understood by the speaker and listener it has been omitted in the Japanese sentence .
As I said earlier , the goal of this system of analysis is to decide the dependency relationship between two clauses .
In this case , using dependency analysis , an input that is to be a receiving clause becomes necessary for the linked clause . I apologize for using a similar example .
If you consider a situation where a clause is linked with the very last clause in a sentence , the system would end up having to wait until the final text is input and the gradualness necessary to implement the real-time conversation handling system will not be sufficient .
Therefore , I propose a gradual Japanese dependency analysis method . In my proposed method , when an input 's receiver of a linked clause does n't yet exist , the system would predict that receiver clause and use that clause to create a dependency construction .
First , I will explain the characteristics of the system I am proposing .
A dependency construction is created from dependency grammar and demonstrates the likelihood of dependency between two clauses .
Furthermore , there are restrictions in this dependency construction , of these three the first one is the non-intersection of dependencies . In other words , dependent relations can not intersect each other .
The second restriction is the solitariness of the link , in other words there can be only one link per clause .
The last restriction is posterior modification , which in the case of Japanese would mean only considering a dependency relationship from the anterior to the posterior of the clause .
Using these three restrictions , would mean going on to create a dependency construction that satisfies these restrictions . Now I will give a summary regarding gradual dependency analysis .
With gradual dependency analysis , it is necessary to proceed with analysis in a situation where a receiver clause for an inputted clause has not yet been inputted .
The system would use dependency grammar for such a case , and go on to create a dependency construction for the input up to there using that construction or a predicted clause while predicting the link 's part of speech .
For instance , in a situation where clauses like " At this store " followed by " a book " are entered , each " at the store " is linked to a verb and each " a book " is further linked to a verb. When is obtained that they are the same verb , you can create a construction like in this chart where " at the store " and " a book " are linked to two of the same verbs .
By creating a construction like this , you can create a dependency relation for these two clauses before a receiver clause for them is entered , and because of that you can grasp the structure of the sentence to an extent before the word " this " is entered .
That sums up everything .
At this time I have explained a method of explanation for a method of gradual Japanese language dependency analysis . This method uses dependency grammar and clauses predicted from that grammar to create dependency constructions .
In this way , by using predicted clauses and creating dependency constructions , there is the advantage of being able to create its dependency construction before a receiver clause is entered for that input .
I will implement this method on a Linux workstation using the GNU common lisp . This concludes my explanation .
Today I would like to announce a gradual method for Japanese language dependency analysis . In recent years , there has been a strong demand for a conversation handling system that uses Japanese Japanese spoken language .
And being spoken language we are dealing with here , it is desirable that it be a real-time system that immediately outputs what is input into it . In order to implement a real-time system , people are seeking a method that gradually interprets natural language .
This would be a method where inputted text would be interpreted occurrence by occurrence. For example , for a sentence like " I met his friend at this store. " after first entering the clause " I met " the system would interpret the " his friend " then once " at this store " is inputted , the system would interpret the inputted text from " I met his friend " to " at this store . "
For Japanese analysis , the Japanese language dependency analysis method is widely utilized .
As this chart shows , this analysis would decide a case relationship for each clause that says " I met his friend at the store . "
For example in a sentence like " I met his friend at this store " , the phrase " at this store " is linked to the word " met . "
The word " his " is linked to the word " friend . " The word " friend " is linked to the word " met . "
The analysis uses a system in which every two clauses the words are in a head-modifier relationship with each other , and the system in turn decides the structure for that sentence .
One might ask why is there such an analysis , and why is it being used ? You can say that the reason is that the Japanese language is characteristically quite lenient when it comes to word order .
For instance if you were to substitute the position of " at this store " and " his friend " , the meaning would be the same . Additionally , unlike English , the subject is frequently omitted in Japanese .
For example in this sentence Japanese literally reads " at this store met his friend " a subject like " I " could be inserted , but since it is already understood by the speaker and listener it has been omitted in the Japanese sentence .
As I said earlier , the goal of this system of analysis is to decide the dependency relationship between two clauses .
In this case , using dependency analysis , an input that is to be a receiving clause becomes necessary for the linked clause . I apologize for using a similar example .
If you consider a situation where a clause is linked with the very last clause in a sentence , the system would end up having to wait until the final text is input and the gradualness necessary to implement the real-time conversation handling system will not be sufficient . Therefore , I propose a gradual Japanese dependency analysis method .
In my proposed method , when an input 's receiver of a linked clause does n't yet exist , the system would predict that receiver clause and use that clause to create a dependency construction . First , I will explain the characteristics of the system I am proposing .
A dependency construction is created from dependency grammar and demonstrates the likelihood of dependency between two clauses .
Furthermore , there are restrictions in this dependency construction , of these three the first one is the non-intersection of dependencies . In other words , dependent relations can not intersect each other .
The second restriction is the solitariness of the link , in other words there can be only one link per clause . The last restriction is posterior modification , which in the case of Japanese would mean only considering a dependency relationship from the anterior to the posterior of the clause .
Using these three restrictions , would mean going on to create a dependency construction that satisfies these restrictions . Now I will give a summary regarding gradual dependency analysis .
With gradual dependency analysis , it is necessary to proceed with analysis in a situation where a receiver clause for an inputted clause has not yet been inputted . The system would use dependency grammar for such a case , and go on to create a dependency construction for the input up to there using that construction or a predicted clause while predicting the link 's part of speech .
For instance , in a situation where clauses like " At this store " followed by " a book " are entered , each " at the store " is linked to a verb and each " a book " is further linked to a verb. When is obtained that they are the same verb , you can create a construction like in this chart where " at the store " and " a book " are linked to two of the same verbs .
By creating a construction like this , you can create a dependency relation for these two clauses before a receiver clause for them is entered , and because of that you can grasp the structure of the sentence to an extent before the word " this " is entered .
That sums up everything .
At this time I have explained a method of explanation for a method of gradual Japanese language dependency analysis . This method uses dependency grammar and clauses predicted from that grammar to create dependency constructions .
In this way , by using predicted clauses and creating dependency constructions , there is the advantage of being able to create its dependency construction before a receiver clause is entered for that input . I will implement this method on a Linux workstation using the GNU common lisp .
This concludes my explanation .
Today I would like to speak on the subject of a spoken dialog email tool that is based on incremental spoken language processing . In recent years , for many people such as the elderly or children who did not have many chances to use computers , for such people chances to use computers have increased .
In such cases , in addition to the input interfaces such as mouses or keyboards that we have had until now , more friendly interfaces have become desirable . So , this is the kind of situation that we have .
Regarding such a situation , research is progressing to use spoken language , such as I am now speaking , as an input method with regards to computers .
It is thought that spoken language , which is the method by which humans most commonly will convey their will to others , would be possible as a kind of operation to directly and easily communicate ones will in the case of using and inputting with regards to computers .
With conventional spoken dialog systems that have been researched to date , it is common that once the speech of a person is finished , at that stage a response comes back from the system , and once the system has finished responding , the user again speaks. It has been firmly divided into this kind of order , a type of dialog that is something like walkie-talkies .
However , when you think of normal dialog between human beings , people do not necessarily wait until the other person has stopped talking , and might interrupt while the other person is still speaking , or might give feedback. It is while doing such things that a dialog takes place .
Therefore , when thinking of achieving an interface between computers and humans that is more friendly and easier to use , just like I said before , some kind of mechanism is required so that the computer can give a response even while the person is speaking .
Therefore , even in the middle of a speech the system gives a response. We call this incremental spoken language processing. Something like this is needed .
By achieving such a thing , and changing content in the middle of a speech , or reducing the waiting time for a response from the system , if a such a smooth dialog could be realized , it is thinkable that such things as stress and anxiety could be decreased , or the efficiency of work itself could be improved .
Following on from such thinking , we have come to develop sync email , something whose main job is to search email .
Sync email is based on the incremental spoken language processing that I mentioned before. At the point it has gathered the necessary information from the speech input , it does a search .
For example , if searching for an email from Kimura with wedding in the title , when the user talks about this , at the point that there is an input for wedding in the title , it has searched once to narrow down the emails .
In this case it performs an operation to search for emails with wedding in the title . Next , at the point that Kimura is input as the sender , it will next also search for any email that has Kimura in the sender category .
By steadily searching step-by-step , for example , while the user is still speaking , and because it is steadily narrowing down , it can find the desired email midway through , or if midway through the email search it brings up no results , in such a case it can immediately go back and try again. By such a thing becoming possible , it is thought that work efficiency could increase .
As for how it specifically returns the result midway through , to begin with searching is successfully combining two things - which category should be searched , and what within the category should be searched. Since this is how a search is configured , if it can determine this combination while someone is still talking , it becomes possible to do something like return a result even while the person is still speaking .
For example , with the previous example , searching for wedding in the title , and with Kimura as the sender , with regards to this kind of speech , at the point that the title is input , it first decides within what category this attribute should be searched , and that the attribute is the title .
Next , at the point that wedding is input , that value is entered , and it knows that it is wedding , so it can fill in the value of the category . And when the situation is such that the attribute and the value are both met , it can perform the search .
Also , for the second part about Kimura the sender , just like one and two , a similar thing can be performed for three and four , so that a search can be done midway through .
Because a conventional search would finally be performed with what was been entered so far , compared to this it can be processed at an earlier stage .
Therefore , it is thought that such things as stress can be reduced midway through . We actually performed evaluation experiments using this system , and in an experiment to evaluate work efficiency , we found that within a set period of time it was better to get a response midway to complete more things , and to improve work efficiency. Also , in a survey about the ease of use , the results showed that when rated with a five point rating system , a proposed system that would return results midway through received a high rating .
In conclusion , I would like to summarize . In this presentation I proposed a way for a system to give a response even if the user was still speaking . Also , the effectiveness of this method was confirmed through an evaluation experiment .
As a future issue , this raises the matter of being able to flexibly handle complex speech because of the various ways that humans speak , not just more simple speech . With this , my presentation is concluded .
Today I would like to speak on the subject of a spoken dialog email tool that is based on incremental spoken language processing .
In recent years , for many people such as the elderly or children who did not have many chances to use computers , for such people chances to use computers have increased .
In such cases , in addition to the input interfaces such as mouses or keyboards that we have had until now , more friendly interfaces have become desirable . So , this is the kind of situation that we have .
Regarding such a situation , research is progressing to use spoken language , such as I am now speaking , as an input method with regards to computers .
It is thought that spoken language , which is the method by which humans most commonly will convey their will to others , would be possible as a kind of operation to directly and easily communicate ones will in the case of using and inputting with regards to computers .
With conventional spoken dialog systems that have been researched to date , it is common that once the speech of a person is finished , at that stage a response comes back from the system , and once the system has finished responding , the user again speaks. It has been firmly divided into this kind of order , a type of dialog that is something like walkie-talkies .
However , when you think of normal dialog between human beings , people do not necessarily wait until the other person has stopped talking , and might interrupt while the other person is still speaking , or might give feedback. It is while doing such things that a dialog takes place .
Therefore , when thinking of achieving an interface between computers and humans that is more friendly and easier to use , just like I said before , some kind of mechanism is required so that the computer can give a response even while the person is speaking .
Therefore , even in the middle of a speech the system gives a response. We call this incremental spoken language processing. Something like this is needed .
By achieving such a thing , and changing content in the middle of a speech , or reducing the waiting time for a response from the system , if a such a smooth dialog could be realized , it is thinkable that such things as stress and anxiety could be decreased , or the efficiency of work itself could be improved .
Following on from such thinking , we have come to develop sync email , something whose main job is to search email .
Sync email is based on the incremental spoken language processing that I mentioned before. At the point it has gathered the necessary information from the speech input , it does a search . For example , if searching for an email from Kimura with wedding in the title , when the user talks about this , at the point that there is an input for wedding in the title , it has searched once to narrow down the emails .
In this case it performs an operation to search for emails with wedding in the title .
Next , at the point that Kimura is input as the sender , it will next also search for any email that has Kimura in the sender category .
By steadily searching step-by-step , for example , while the user is still speaking , and because it is steadily narrowing down , it can find the desired email midway through , or if midway through the email search it brings up no results , in such a case it can immediately go back and try again. By such a thing becoming possible , it is thought that work efficiency could increase .
As for how it specifically returns the result midway through , to begin with searching is successfully combining two things - which category should be searched , and what within the category should be searched. Since this is how a search is configured , if it can determine this combination while someone is still talking , it becomes possible to do something like return a result even while the person is still speaking .
For example , with the previous example , searching for wedding in the title , and with Kimura as the sender , with regards to this kind of speech , at the point that the title is input , it first decides within what category this attribute should be searched , and that the attribute is the title .
Next , at the point that wedding is input , that value is entered , and it knows that it is wedding , so it can fill in the value of the category .
And when the situation is such that the attribute and the value are both met , it can perform the search .
Also , for the second part about Kimura the sender , just like one and two , a similar thing can be performed for three and four , so that a search can be done midway through .
Because a conventional search would finally be performed with what was been entered so far , compared to this it can be processed at an earlier stage . Therefore , it is thought that such things as stress can be reduced midway through .
We actually performed evaluation experiments using this system , and in an experiment to evaluate work efficiency , we found that within a set period of time it was better to get a response midway to complete more things , and to improve work efficiency. Also , in a survey about the ease of use , the results showed that when rated with a five point rating system , a proposed system that would return results midway through received a high rating .
In conclusion , I would like to summarize .
In this presentation I proposed a way for a system to give a response even if the user was still speaking . Also , the effectiveness of this method was confirmed through an evaluation experiment .
As a future issue , this raises the matter of being able to flexibly handle complex speech because of the various ways that humans speak , not just more simple speech . With this , my presentation is concluded .
Today I would like to speak on the subject of a spoken dialog email tool that is based on incremental spoken language processing .
In recent years , for many people such as the elderly or children who did not have many chances to use computers , for such people chances to use computers have increased .
In such cases , in addition to the input interfaces such as mouses or keyboards that we have had until now , more friendly interfaces have become desirable . So , this is the kind of situation that we have .
Regarding such a situation , research is progressing to use spoken language , such as I am now speaking , as an input method with regards to computers .
It is thought that spoken language , which is the method by which humans most commonly will convey their will to others , would be possible as a kind of operation to directly and easily communicate ones will in the case of using and inputting with regards to computers .
With conventional spoken dialog systems that have been researched to date , it is common that once the speech of a person is finished , at that stage a response comes back from the system , and once the system has finished responding , the user again speaks. It has been firmly divided into this kind of order , a type of dialog that is something like walkie-talkies .
However , when you think of normal dialog between human beings , people do not necessarily wait until the other person has stopped talking , and might interrupt while the other person is still speaking , or might give feedback. It is while doing such things that a dialog takes place .
Therefore , when thinking of achieving an interface between computers and humans that is more friendly and easier to use , just like I said before , some kind of mechanism is required so that the computer can give a response even while the person is speaking .
Therefore , even in the middle of a speech the system gives a response. We call this incremental spoken language processing. Something like this is needed .
By achieving such a thing , and changing content in the middle of a speech , or reducing the waiting time for a response from the system , if a such a smooth dialog could be realized , it is thinkable that such things as stress and anxiety could be decreased , or the efficiency of work itself could be improved .
Following on from such thinking , we have come to develop sync email , something whose main job is to search email .
Sync email is based on the incremental spoken language processing that I mentioned before. At the point it has gathered the necessary information from the speech input , it does a search .
For example , if searching for an email from Kimura with wedding in the title , when the user talks about this , at the point that there is an input for wedding in the title , it has searched once to narrow down the emails .
In this case it performs an operation to search for emails with wedding in the title . Next , at the point that Kimura is input as the sender , it will next also search for any email that has Kimura in the sender category .
By steadily searching step-by-step , for example , while the user is still speaking , and because it is steadily narrowing down , it can find the desired email midway through , or if midway through the email search it brings up no results , in such a case it can immediately go back and try again. By such a thing becoming possible , it is thought that work efficiency could increase .
As for how it specifically returns the result midway through , to begin with searching is successfully combining two things - which category should be searched , and what within the category should be searched. Since this is how a search is configured , if it can determine this combination while someone is still talking , it becomes possible to do something like return a result even while the person is still speaking .
For example , with the previous example , searching for wedding in the title , and with Kimura as the sender , with regards to this kind of speech , at the point that the title is input , it first decides within what category this attribute should be searched , and that the attribute is the title .
Next , at the point that wedding is input , that value is entered , and it knows that it is wedding , so it can fill in the value of the category .
And when the situation is such that the attribute and the value are both met , it can perform the search .
Also , for the second part about Kimura the sender , just like one and two , a similar thing can be performed for three and four , so that a search can be done midway through . Because a conventional search would finally be performed with what was been entered so far , compared to this it can be processed at an earlier stage .
Therefore , it is thought that such things as stress can be reduced midway through .
We actually performed evaluation experiments using this system , and in an experiment to evaluate work efficiency , we found that within a set period of time it was better to get a response midway to complete more things , and to improve work efficiency. Also , in a survey about the ease of use , the results showed that when rated with a five point rating system , a proposed system that would return results midway through received a high rating .
In conclusion , I would like to summarize .
In this presentation I proposed a way for a system to give a response even if the user was still speaking . Also , the effectiveness of this method was confirmed through an evaluation experiment .
As a future issue , this raises the matter of being able to flexibly handle complex speech because of the various ways that humans speak , not just more simple speech . With this , my presentation is concluded .
Today I would like to speak on the subject of a spoken dialog email tool that is based on incremental spoken language processing . In recent years , for many people such as the elderly or children who did not have many chances to use computers , for such people chances to use computers have increased .
In such cases , in addition to the input interfaces such as mouses or keyboards that we have had until now , more friendly interfaces have become desirable . So , this is the kind of situation that we have .
Regarding such a situation , research is progressing to use spoken language , such as I am now speaking , as an input method with regards to computers .
It is thought that spoken language , which is the method by which humans most commonly will convey their will to others , would be possible as a kind of operation to directly and easily communicate ones will in the case of using and inputting with regards to computers .
With conventional spoken dialog systems that have been researched to date , it is common that once the speech of a person is finished , at that stage a response comes back from the system , and once the system has finished responding , the user again speaks. It has been firmly divided into this kind of order , a type of dialog that is something like walkie-talkies .
However , when you think of normal dialog between human beings , people do not necessarily wait until the other person has stopped talking , and might interrupt while the other person is still speaking , or might give feedback. It is while doing such things that a dialog takes place .
Therefore , when thinking of achieving an interface between computers and humans that is more friendly and easier to use , just like I said before , some kind of mechanism is required so that the computer can give a response even while the person is speaking . Therefore , even in the middle of a speech the system gives a response. We call this incremental spoken language processing. Something like this is needed .
By achieving such a thing , and changing content in the middle of a speech , or reducing the waiting time for a response from the system , if a such a smooth dialog could be realized , it is thinkable that such things as stress and anxiety could be decreased , or the efficiency of work itself could be improved .
Following on from such thinking , we have come to develop sync email , something whose main job is to search email .
Sync email is based on the incremental spoken language processing that I mentioned before. At the point it has gathered the necessary information from the speech input , it does a search .
For example , if searching for an email from Kimura with wedding in the title , when the user talks about this , at the point that there is an input for wedding in the title , it has searched once to narrow down the emails .
In this case it performs an operation to search for emails with wedding in the title . Next , at the point that Kimura is input as the sender , it will next also search for any email that has Kimura in the sender category .
By steadily searching step-by-step , for example , while the user is still speaking , and because it is steadily narrowing down , it can find the desired email midway through , or if midway through the email search it brings up no results , in such a case it can immediately go back and try again. By such a thing becoming possible , it is thought that work efficiency could increase .
As for how it specifically returns the result midway through , to begin with searching is successfully combining two things - which category should be searched , and what within the category should be searched. Since this is how a search is configured , if it can determine this combination while someone is still talking , it becomes possible to do something like return a result even while the person is still speaking .
For example , with the previous example , searching for wedding in the title , and with Kimura as the sender , with regards to this kind of speech , at the point that the title is input , it first decides within what category this attribute should be searched , and that the attribute is the title .
Next , at the point that wedding is input , that value is entered , and it knows that it is wedding , so it can fill in the value of the category . And when the situation is such that the attribute and the value are both met , it can perform the search .
Also , for the second part about Kimura the sender , just like one and two , a similar thing can be performed for three and four , so that a search can be done midway through .
Because a conventional search would finally be performed with what was been entered so far , compared to this it can be processed at an earlier stage . Therefore , it is thought that such things as stress can be reduced midway through .
We actually performed evaluation experiments using this system , and in an experiment to evaluate work efficiency , we found that within a set period of time it was better to get a response midway to complete more things , and to improve work efficiency. Also , in a survey about the ease of use , the results showed that when rated with a five point rating system , a proposed system that would return results midway through received a high rating .
In conclusion , I would like to summarize .
In this presentation I proposed a way for a system to give a response even if the user was still speaking . Also , the effectiveness of this method was confirmed through an evaluation experiment .
As a future issue , this raises the matter of being able to flexibly handle complex speech because of the various ways that humans speak , not just more simple speech . With this , my presentation is concluded .
Today I will talk about how the system decides when to output results during a gradual syntax analysis . First , I will quickly explain what I mean by gradual syntax analysis .
Gradual syntax analysis captures the sentence structure before the input is complete . This is the framework of the system .
For example , look at this sentence here. " I saw her aunt in the park " . At " saw " , the gradual syntax analysis system recognizes that the " I " is the noun and " saw " is the verb in this sentence . So that 's the framework .
So , what is this used for ? Good examples are , real-time dialogue pressing system or a simultaneous interpretation system .
When thinning about gradual syntax analysis , the output timing is extremely important . This is because it is difficult to determine when the sentence structure is established .
For example , look at this input. Here , where it says " her " , this can be an object for the verb , " saw " , or it can be a possessive pronoun if a noun follows it. As you can see here , there are many possibilities , and at this point , it 's impossible to determine which way the sentence will go .
Next , let 's say that the word " aunt " was entered after " her " . This means that the " her " can not be an object of the verb , " saw " anymore. Instead , it is a possessive noun on the word , " aunt " . Now , we can determine the role " her " plays in the sentence .
So , we now know that deciding the appropriate output timing is important for a gradual syntax analysis. Now , let 's look at a practical method to make this possible . Our method will decide the appropriate output timing for the machine .
So how does it work ? First , it delays the output timing until enough has been said to determine the sentence structure .
Then , within that time-frame , it outputs information like sentence structure as early as possible . Now I will explain about the overview of this spec .
The system consists of two modules , the gradual syntax analysis part , and the part that regulates its output .
The vocabularies are input into the gradual syntax analysis. Then , possible sentence structure relationships of these vocabularies are created here .
Then , the analysis results are passed onto the module that regulates the output . This module uses the sentence structure information and determines which parts have been confirmed and which parts are still undeterminable. Then , it outputs the confirmed parts .
Let 's look at an example .
This is the same example I gave earlier. Let 's assume the user has input everything up to " her " . First , the word " her " is input into the gradual syntax analysis system .
Then , the gradual syntax analysis system outputs potential sentence structure relationships . In this example , this includes " her " as an object to the verb , " saw " , or " her " as a possessive noun. It outputs these two possibilities .
The output regulator module receives these information. At this point , they ca n't determine which one is correct , so they hold onto it .
Then , the word " aunt " enters the system . Once " aunt " is input into the gradual syntax analysis system , the system will recognize " aunt " as a noun , and the information will be passed onto the output regulator module .
Using this information , it now knows that " her " can not be an object to the verb , " saw " . From here , the output regulator module confirms that " her " is a possessive noun and outputs it .
As you can see here , by regulating the output , the system is able to output the words once its role in the sentence has been confirmed . Today , I talked about the method that decides the output timing after the gradual syntax analysis step .
This method has been tested. We mounted it on a calculator and conducted an experiment . We confirmed that the method is capable of gradually outputting sentence structures .
That is all. Thank you .
Today I will talk about how the system decides when to output results during a gradual syntax analysis .
First , I will quickly explain what I mean by gradual syntax analysis .
Gradual syntax analysis captures the sentence structure before the input is complete . This is the framework of the system .
For example , look at this sentence here. " I saw her aunt in the park " . At " saw " , the gradual syntax analysis system recognizes that the " I " is the noun and " saw " is the verb in this sentence . So that 's the framework .
So , what is this used for ? Good examples are , real-time dialogue pressing system or a simultaneous interpretation system .
When thinning about gradual syntax analysis , the output timing is extremely important . This is because it is difficult to determine when the sentence structure is established .
For example , look at this input. Here , where it says " her " , this can be an object for the verb , " saw " , or it can be a possessive pronoun if a noun follows it. As you can see here , there are many possibilities , and at this point , it 's impossible to determine which way the sentence will go .
Next , let 's say that the word " aunt " was entered after " her " . This means that the " her " can not be an object of the verb , " saw " anymore. Instead , it is a possessive noun on the word , " aunt " . Now , we can determine the role " her " plays in the sentence .
So , we now know that deciding the appropriate output timing is important for a gradual syntax analysis. Now , let 's look at a practical method to make this possible .
Our method will decide the appropriate output timing for the machine .
So how does it work ? First , it delays the output timing until enough has been said to determine the sentence structure .
Then , within that time-frame , it outputs information like sentence structure as early as possible .
Now I will explain about the overview of this spec . The system consists of two modules , the gradual syntax analysis part , and the part that regulates its output .
The vocabularies are input into the gradual syntax analysis. Then , possible sentence structure relationships of these vocabularies are created here . Then , the analysis results are passed onto the module that regulates the output .
This module uses the sentence structure information and determines which parts have been confirmed and which parts are still undeterminable. Then , it outputs the confirmed parts .
Let 's look at an example .
This is the same example I gave earlier. Let 's assume the user has input everything up to " her " . First , the word " her " is input into the gradual syntax analysis system . Then , the gradual syntax analysis system outputs potential sentence structure relationships .
In this example , this includes " her " as an object to the verb , " saw " , or " her " as a possessive noun. It outputs these two possibilities .
The output regulator module receives these information. At this point , they ca n't determine which one is correct , so they hold onto it .
Then , the word " aunt " enters the system . Once " aunt " is input into the gradual syntax analysis system , the system will recognize " aunt " as a noun , and the information will be passed onto the output regulator module .
Using this information , it now knows that " her " can not be an object to the verb , " saw " . From here , the output regulator module confirms that " her " is a possessive noun and outputs it .
As you can see here , by regulating the output , the system is able to output the words once its role in the sentence has been confirmed .
Today , I talked about the method that decides the output timing after the gradual syntax analysis step . This method has been tested. We mounted it on a calculator and conducted an experiment . We confirmed that the method is capable of gradually outputting sentence structures .
That is all. Thank you .
Today I will talk about how the system decides when to output results during a gradual syntax analysis .
First , I will quickly explain what I mean by gradual syntax analysis . Gradual syntax analysis captures the sentence structure before the input is complete .
This is the framework of the system . For example , look at this sentence here. " I saw her aunt in the park " . At " saw " , the gradual syntax analysis system recognizes that the " I " is the noun and " saw " is the verb in this sentence .
So that 's the framework . So , what is this used for ? Good examples are , real-time dialogue pressing system or a simultaneous interpretation system .
When thinning about gradual syntax analysis , the output timing is extremely important . This is because it is difficult to determine when the sentence structure is established .
For example , look at this input. Here , where it says " her " , this can be an object for the verb , " saw " , or it can be a possessive pronoun if a noun follows it. As you can see here , there are many possibilities , and at this point , it 's impossible to determine which way the sentence will go .
Next , let 's say that the word " aunt " was entered after " her " . This means that the " her " can not be an object of the verb , " saw " anymore. Instead , it is a possessive noun on the word , " aunt " . Now , we can determine the role " her " plays in the sentence .
So , we now know that deciding the appropriate output timing is important for a gradual syntax analysis. Now , let 's look at a practical method to make this possible .
Our method will decide the appropriate output timing for the machine .
So how does it work ? First , it delays the output timing until enough has been said to determine the sentence structure .
Then , within that time-frame , it outputs information like sentence structure as early as possible . Now I will explain about the overview of this spec .
The system consists of two modules , the gradual syntax analysis part , and the part that regulates its output .
The vocabularies are input into the gradual syntax analysis. Then , possible sentence structure relationships of these vocabularies are created here . Then , the analysis results are passed onto the module that regulates the output .
This module uses the sentence structure information and determines which parts have been confirmed and which parts are still undeterminable. Then , it outputs the confirmed parts .
Let 's look at an example .
This is the same example I gave earlier. Let 's assume the user has input everything up to " her " . First , the word " her " is input into the gradual syntax analysis system .
Then , the gradual syntax analysis system outputs potential sentence structure relationships . In this example , this includes " her " as an object to the verb , " saw " , or " her " as a possessive noun. It outputs these two possibilities .
The output regulator module receives these information. At this point , they ca n't determine which one is correct , so they hold onto it .
Then , the word " aunt " enters the system . Once " aunt " is input into the gradual syntax analysis system , the system will recognize " aunt " as a noun , and the information will be passed onto the output regulator module .
Using this information , it now knows that " her " can not be an object to the verb , " saw " .
From here , the output regulator module confirms that " her " is a possessive noun and outputs it .
As you can see here , by regulating the output , the system is able to output the words once its role in the sentence has been confirmed . Today , I talked about the method that decides the output timing after the gradual syntax analysis step .
This method has been tested. We mounted it on a calculator and conducted an experiment . We confirmed that the method is capable of gradually outputting sentence structures .
That is all. Thank you .
Today I will talk about how the system decides when to output results during a gradual syntax analysis .
First , I will quickly explain what I mean by gradual syntax analysis .
Gradual syntax analysis captures the sentence structure before the input is complete . This is the framework of the system .
For example , look at this sentence here. " I saw her aunt in the park " . At " saw " , the gradual syntax analysis system recognizes that the " I " is the noun and " saw " is the verb in this sentence .
So that 's the framework .
So , what is this used for ? Good examples are , real-time dialogue pressing system or a simultaneous interpretation system .
When thinning about gradual syntax analysis , the output timing is extremely important . This is because it is difficult to determine when the sentence structure is established .
For example , look at this input. Here , where it says " her " , this can be an object for the verb , " saw " , or it can be a possessive pronoun if a noun follows it. As you can see here , there are many possibilities , and at this point , it 's impossible to determine which way the sentence will go .
Next , let 's say that the word " aunt " was entered after " her " . This means that the " her " can not be an object of the verb , " saw " anymore. Instead , it is a possessive noun on the word , " aunt " . Now , we can determine the role " her " plays in the sentence .
So , we now know that deciding the appropriate output timing is important for a gradual syntax analysis. Now , let 's look at a practical method to make this possible .
Our method will decide the appropriate output timing for the machine . So how does it work ? First , it delays the output timing until enough has been said to determine the sentence structure .
Then , within that time-frame , it outputs information like sentence structure as early as possible . Now I will explain about the overview of this spec .
The system consists of two modules , the gradual syntax analysis part , and the part that regulates its output . The vocabularies are input into the gradual syntax analysis. Then , possible sentence structure relationships of these vocabularies are created here .
Then , the analysis results are passed onto the module that regulates the output . This module uses the sentence structure information and determines which parts have been confirmed and which parts are still undeterminable. Then , it outputs the confirmed parts .
Let 's look at an example .
This is the same example I gave earlier. Let 's assume the user has input everything up to " her " .
First , the word " her " is input into the gradual syntax analysis system .
Then , the gradual syntax analysis system outputs potential sentence structure relationships . In this example , this includes " her " as an object to the verb , " saw " , or " her " as a possessive noun. It outputs these two possibilities .
The output regulator module receives these information. At this point , they ca n't determine which one is correct , so they hold onto it .
Then , the word " aunt " enters the system . Once " aunt " is input into the gradual syntax analysis system , the system will recognize " aunt " as a noun , and the information will be passed onto the output regulator module .
Using this information , it now knows that " her " can not be an object to the verb , " saw " . From here , the output regulator module confirms that " her " is a possessive noun and outputs it .
As you can see here , by regulating the output , the system is able to output the words once its role in the sentence has been confirmed . Today , I talked about the method that decides the output timing after the gradual syntax analysis step .
This method has been tested. We mounted it on a calculator and conducted an experiment . We confirmed that the method is capable of gradually outputting sentence structures . That is all. Thank you .
Ok , now I will give a presentation on correspondence estimation of sentences for translating expressions .
Today , more and more people are using the Internet and travelling around the world. Naturally , opportunities to communicate with people who speak a different language is on the rise .
Also , when travelling , many depend on a system that can translate spoken languages . When it comes to conversation , simultaneous interpretation is ideal .
So , let 's think about a simultaneous interpretation system . With a simultaneous interpretation system , it is too late if the system spits out results after the input has finished. It must be able to output the results before the speaker finishes the sentence .
In order to come up with the correct translation before the sentence is finished , the system must be able to perform complex tasks like syntax analysis in middle of the sentence .
So , we thought it would be possible to extract knowledge and tips used by simultaneous interpreters and incorporate that into the simultaneous interpretation system .
Recently , many researchers are studying parallel corpus , which is a set of texts containing original sentences and its translation. From here , we thought it may be possible to extract useful " translations for expressions " for simultaneous interpretation and install it into the system .
It takes three steps to extract a translation for an expression . First , we must map out the sentences in the parallel corpus .
For example , sentence number one , " Ohayo gozaimasu " correlates with " Good morning " , and sentence number two , " Tadaima goshokai itadakimasita Suzuki de gozaimasu " is splint into " Thank you for your kind introduction " , and number three , " My name is Suzuki " .
Next , we narrow it down even more and associate more specific phrases. For instance , the part , " Tadaima goshokai itadakimashita " correlates to number two , " Thank you for your kind introduction " .
Then from the matched phrases , we extract useful expression translations for simultaneous translation . Here , we have to think about the first step , mapping the sentences .
When doing so , there are two methods. One way is to take a statistical approach. The other way is using the translation dictionary .
One example of a statistical approach is measuring the length of the sentences. On the other hand , one example of the dictionary method is matching the vocabularies. I will explain both in detail .
First , let 's look at the method that measures the length of the sentences. In this method , we assume that the length ratio for the Japanese sentence and English sentence are fixed , more or less .
Look at this example. The first sentence is " Ohayo gozaimasu " . Where does this go in the English sentence ? The system will think it corresponds to " Good morning " or " Good morning thank you for your kind introduction " . Then , it analyzes the length ratio and assumes that the sentence corresponds to " Good morning " .
The next example is matching the vocabularies. This method uses the translation dictionary and converts the Japanese words into English .
Then , using the converted vocabularies , it uses correspondence relationship to predict the corresponding sentence . For example , let 's say that the system looked up " Ohayo gozaimasu " , it will be converted into " Good morning. If it looks up " Tadaima Goshokai Itadaki Suzuki " , it gets " Presently Introduction Suzuki " .
In order to translate these two sentences into English , the system then uses correspondence relationships for each vocabulary .
For " Ohayo gozaimasu " , there is a match , " Good morning " . In the English sentence , there is " Good morning " as seen in number one. So the system know nows that sentence number one and the English sentence number one matches .
In the second sentence , we see " Introduction " and " Suzuki " . In the English sentence , " Introduction " is in the second sentence and " Suzuki " is in the third sentence. So , it concludes that the second Japanese sentence corresponds to the second and third English sentence .
Ok , I will now summarize. Our research topic is , extracting a resourceful expression translation from the parallel corpus and install it in the simultaneous interpretation system , and to do so , we took two approaches. One , using statistics and measuring the length ratio of the sentences , and second , using the dictionary to find matching vocabularies .
That is all for today , thank you .
Ok , now I will give a presentation on correspondence estimation of sentences for translating expressions .
Today , more and more people are using the Internet and travelling around the world. Naturally , opportunities to communicate with people who speak a different language is on the rise . Also , when travelling , many depend on a system that can translate spoken languages .
When it comes to conversation , simultaneous interpretation is ideal . So , let 's think about a simultaneous interpretation system .
With a simultaneous interpretation system , it is too late if the system spits out results after the input has finished. It must be able to output the results before the speaker finishes the sentence . In order to come up with the correct translation before the sentence is finished , the system must be able to perform complex tasks like syntax analysis in middle of the sentence .
So , we thought it would be possible to extract knowledge and tips used by simultaneous interpreters and incorporate that into the simultaneous interpretation system .
Recently , many researchers are studying parallel corpus , which is a set of texts containing original sentences and its translation. From here , we thought it may be possible to extract useful " translations for expressions " for simultaneous interpretation and install it into the system .
It takes three steps to extract a translation for an expression . First , we must map out the sentences in the parallel corpus .
For example , sentence number one , " Ohayo gozaimasu " correlates with " Good morning " , and sentence number two , " Tadaima goshokai itadakimasita Suzuki de gozaimasu " is splint into " Thank you for your kind introduction " , and number three , " My name is Suzuki " .
Next , we narrow it down even more and associate more specific phrases. For instance , the part , " Tadaima goshokai itadakimashita " correlates to number two , " Thank you for your kind introduction " . Then from the matched phrases , we extract useful expression translations for simultaneous translation .
Here , we have to think about the first step , mapping the sentences .
When doing so , there are two methods. One way is to take a statistical approach. The other way is using the translation dictionary .
One example of a statistical approach is measuring the length of the sentences. On the other hand , one example of the dictionary method is matching the vocabularies. I will explain both in detail .
First , let 's look at the method that measures the length of the sentences. In this method , we assume that the length ratio for the Japanese sentence and English sentence are fixed , more or less .
Look at this example. The first sentence is " Ohayo gozaimasu " . Where does this go in the English sentence ? The system will think it corresponds to " Good morning " or " Good morning thank you for your kind introduction " . Then , it analyzes the length ratio and assumes that the sentence corresponds to " Good morning " .
The next example is matching the vocabularies. This method uses the translation dictionary and converts the Japanese words into English . Then , using the converted vocabularies , it uses correspondence relationship to predict the corresponding sentence .
For example , let 's say that the system looked up " Ohayo gozaimasu " , it will be converted into " Good morning. If it looks up " Tadaima Goshokai Itadaki Suzuki " , it gets " Presently Introduction Suzuki " . In order to translate these two sentences into English , the system then uses correspondence relationships for each vocabulary .
For " Ohayo gozaimasu " , there is a match , " Good morning " . In the English sentence , there is " Good morning " as seen in number one. So the system know nows that sentence number one and the English sentence number one matches .
In the second sentence , we see " Introduction " and " Suzuki " . In the English sentence , " Introduction " is in the second sentence and " Suzuki " is in the third sentence. So , it concludes that the second Japanese sentence corresponds to the second and third English sentence .
Ok , I will now summarize. Our research topic is , extracting a resourceful expression translation from the parallel corpus and install it in the simultaneous interpretation system , and to do so , we took two approaches. One , using statistics and measuring the length ratio of the sentences , and second , using the dictionary to find matching vocabularies . That is all for today , thank you .
Ok , now I will give a presentation on correspondence estimation of sentences for translating expressions .
Today , more and more people are using the Internet and travelling around the world. Naturally , opportunities to communicate with people who speak a different language is on the rise . Also , when travelling , many depend on a system that can translate spoken languages .
When it comes to conversation , simultaneous interpretation is ideal . So , let 's think about a simultaneous interpretation system .
With a simultaneous interpretation system , it is too late if the system spits out results after the input has finished. It must be able to output the results before the speaker finishes the sentence .
In order to come up with the correct translation before the sentence is finished , the system must be able to perform complex tasks like syntax analysis in middle of the sentence . So , we thought it would be possible to extract knowledge and tips used by simultaneous interpreters and incorporate that into the simultaneous interpretation system .
Recently , many researchers are studying parallel corpus , which is a set of texts containing original sentences and its translation. From here , we thought it may be possible to extract useful " translations for expressions " for simultaneous interpretation and install it into the system .
It takes three steps to extract a translation for an expression . First , we must map out the sentences in the parallel corpus .
For example , sentence number one , " Ohayo gozaimasu " correlates with " Good morning " , and sentence number two , " Tadaima goshokai itadakimasita Suzuki de gozaimasu " is splint into " Thank you for your kind introduction " , and number three , " My name is Suzuki " .
Next , we narrow it down even more and associate more specific phrases. For instance , the part , " Tadaima goshokai itadakimashita " correlates to number two , " Thank you for your kind introduction " .
Then from the matched phrases , we extract useful expression translations for simultaneous translation . Here , we have to think about the first step , mapping the sentences .
When doing so , there are two methods. One way is to take a statistical approach. The other way is using the translation dictionary .
One example of a statistical approach is measuring the length of the sentences. On the other hand , one example of the dictionary method is matching the vocabularies. I will explain both in detail .
First , let 's look at the method that measures the length of the sentences. In this method , we assume that the length ratio for the Japanese sentence and English sentence are fixed , more or less .
Look at this example. The first sentence is " Ohayo gozaimasu " . Where does this go in the English sentence ? The system will think it corresponds to " Good morning " or " Good morning thank you for your kind introduction " . Then , it analyzes the length ratio and assumes that the sentence corresponds to " Good morning " .
The next example is matching the vocabularies. This method uses the translation dictionary and converts the Japanese words into English . Then , using the converted vocabularies , it uses correspondence relationship to predict the corresponding sentence .
For example , let 's say that the system looked up " Ohayo gozaimasu " , it will be converted into " Good morning. If it looks up " Tadaima Goshokai Itadaki Suzuki " , it gets " Presently Introduction Suzuki " .
In order to translate these two sentences into English , the system then uses correspondence relationships for each vocabulary . For " Ohayo gozaimasu " , there is a match , " Good morning " . In the English sentence , there is " Good morning " as seen in number one. So the system know nows that sentence number one and the English sentence number one matches .
In the second sentence , we see " Introduction " and " Suzuki " . In the English sentence , " Introduction " is in the second sentence and " Suzuki " is in the third sentence. So , it concludes that the second Japanese sentence corresponds to the second and third English sentence .
Ok , I will now summarize. Our research topic is , extracting a resourceful expression translation from the parallel corpus and install it in the simultaneous interpretation system , and to do so , we took two approaches. One , using statistics and measuring the length ratio of the sentences , and second , using the dictionary to find matching vocabularies .
That is all for today , thank you .
Ok , now I will give a presentation on correspondence estimation of sentences for translating expressions .
Today , more and more people are using the Internet and travelling around the world. Naturally , opportunities to communicate with people who speak a different language is on the rise .
Also , when travelling , many depend on a system that can translate spoken languages . When it comes to conversation , simultaneous interpretation is ideal .
So , let 's think about a simultaneous interpretation system .
With a simultaneous interpretation system , it is too late if the system spits out results after the input has finished. It must be able to output the results before the speaker finishes the sentence .
In order to come up with the correct translation before the sentence is finished , the system must be able to perform complex tasks like syntax analysis in middle of the sentence . So , we thought it would be possible to extract knowledge and tips used by simultaneous interpreters and incorporate that into the simultaneous interpretation system .
Recently , many researchers are studying parallel corpus , which is a set of texts containing original sentences and its translation. From here , we thought it may be possible to extract useful " translations for expressions " for simultaneous interpretation and install it into the system .
It takes three steps to extract a translation for an expression . First , we must map out the sentences in the parallel corpus .
For example , sentence number one , " Ohayo gozaimasu " correlates with " Good morning " , and sentence number two , " Tadaima goshokai itadakimasita Suzuki de gozaimasu " is splint into " Thank you for your kind introduction " , and number three , " My name is Suzuki " .
Next , we narrow it down even more and associate more specific phrases. For instance , the part , " Tadaima goshokai itadakimashita " correlates to number two , " Thank you for your kind introduction " .
Then from the matched phrases , we extract useful expression translations for simultaneous translation . Here , we have to think about the first step , mapping the sentences .
When doing so , there are two methods. One way is to take a statistical approach. The other way is using the translation dictionary .
One example of a statistical approach is measuring the length of the sentences. On the other hand , one example of the dictionary method is matching the vocabularies. I will explain both in detail .
First , let 's look at the method that measures the length of the sentences. In this method , we assume that the length ratio for the Japanese sentence and English sentence are fixed , more or less .
Look at this example. The first sentence is " Ohayo gozaimasu " . Where does this go in the English sentence ? The system will think it corresponds to " Good morning " or " Good morning thank you for your kind introduction " . Then , it analyzes the length ratio and assumes that the sentence corresponds to " Good morning " .
The next example is matching the vocabularies. This method uses the translation dictionary and converts the Japanese words into English . Then , using the converted vocabularies , it uses correspondence relationship to predict the corresponding sentence .
For example , let 's say that the system looked up " Ohayo gozaimasu " , it will be converted into " Good morning. If it looks up " Tadaima Goshokai Itadaki Suzuki " , it gets " Presently Introduction Suzuki " .
In order to translate these two sentences into English , the system then uses correspondence relationships for each vocabulary .
For " Ohayo gozaimasu " , there is a match , " Good morning " . In the English sentence , there is " Good morning " as seen in number one. So the system know nows that sentence number one and the English sentence number one matches .
In the second sentence , we see " Introduction " and " Suzuki " . In the English sentence , " Introduction " is in the second sentence and " Suzuki " is in the third sentence. So , it concludes that the second Japanese sentence corresponds to the second and third English sentence .
Ok , I will now summarize. Our research topic is , extracting a resourceful expression translation from the parallel corpus and install it in the simultaneous interpretation system , and to do so , we took two approaches. One , using statistics and measuring the length ratio of the sentences , and second , using the dictionary to find matching vocabularies .
That is all for today , thank you .
Today 's presentation is entitled , Creating and evaluating a speech dialogue MP3 player .
In recent years , thanks to the Internet , a vast amount of MP3 files are available anywhere you go .
MP3 is a file format on the computer. Simply put , it allows us to use the audio files inside a CD on a computer . Today , you can obtain tens of thousands of MP3 files from the Internet .
Also , portable music players like these are getting smaller and smaller while its capacity keeps getting bigger and bigger .
With such devices , we can make use of a vast amount of MP3 files . So , this is where we are today. But when we want to search for a specific MP3 file or when using it in a portable music player , it is very difficult to control them using keyboards .
On the other hand , researchers are taking a closer look at dictation systems as an efficient method for searching for MP3 files . Furthermore , the voice input interface is very easy to use .
So , we created a speech dialogue MP3 player . With this system , you can operate the system using your voice and search for songs as well .
How did we do it ? First , let me explain about how spoken words are processed .
First , we set keywords based on the functions of an MP3 file and user speech . Using these keywords , we processed spoken words . This time , we registered eighty two keywords .
Some examples are " play " , " fast forward " , " a little bit " , and so on .
Based on these keywords , we sorted it by patterns of spoken sentences . As a result , we classified these words into one hundred and eight patterns .
For example , " I want to listen to this " is one pattern , and " Fast forward this a little bit " is another . Next , I will explain about how such keywords are processed .
First , I will talk about speech input . The sentence is , search for the Beatles. The machine will then look for keywords in that sentence .
In this example , the keyword is " Beatles " and " Search " . The system will now look for these keywords . Then , these keywords will be converted into a command .
Here , the word " Beatles " will turn into a command for artist keywords when conducting the search . Then these are put into order and rows of keywords are extracted .
This process applies to the previous example. Currently there are one hundred and eight patterns
After these are formulated , each command performs its registered action . In this case , the system will search for artists based on the rows of keywords , then respond to the search .
We created this system based on such analysis . We mounted it on a WINDOWS 98 OS .
We used the Visual C++ language .
As I mentioned earlier , there are eighty two words registered . Regarding the keyword rows , there are one hundred and eight types defined .
Next , we conducted an evaluation experiment .
Our goal is to evaluate the usability of this system . We had twenty college students test out the system .
We also compared the system with a conventional system . The objective was to perform as many tasks as possible within the given time .
Now I will explain about the tasks we assigned to the students .
For example , we would ask the students to play a certain song . This example is pretty simple , all you have to do is search for the song and play it .
In another task , we asked the students to find the title of the song that starts with , " all white " Here are the results .
First , we compared the task completion rate between our system and the conventional system .
The vertical axis represents the number of tasks completed . The blue indicates this system , and the green represents the conventional system .
We can see here that the task completion rate has increased with this system . This result indicates that the speech input increased the operation and search efficiency .
Next , we evaluated the usability of the system by conducting a survey asking about the new and old systems .
A five star rating system was used . Five being the highest rating .
This diagram indicates that six people gave the new system a five. Ten people gave a four , three people gave a three , one person gave a one .
With the conventional system , the results were one , eight , eight , three , respectively . When we compare these results , we can see that the new system got a higher rating overall .
On average , the new system scored one point higher than the old one at four point zero five. The new system scored three point three five . Ok , I will summarize .
Today I talked about a speech dialogue MP3 player system that processes tasks based on keyword analysis . We also conducted an experiment to evaluate the system and confirmed the effectiveness of operating and searching for tracks using dictation . That is all , thank you for your time .
Today 's presentation is entitled , Creating and evaluating a speech dialogue MP3 player .
In recent years , thanks to the Internet , a vast amount of MP3 files are available anywhere you go .
MP3 is a file format on the computer. Simply put , it allows us to use the audio files inside a CD on a computer .
Today , you can obtain tens of thousands of MP3 files from the Internet . Also , portable music players like these are getting smaller and smaller while its capacity keeps getting bigger and bigger .
With such devices , we can make use of a vast amount of MP3 files .
So , this is where we are today. But when we want to search for a specific MP3 file or when using it in a portable music player , it is very difficult to control them using keyboards . On the other hand , researchers are taking a closer look at dictation systems as an efficient method for searching for MP3 files . Furthermore , the voice input interface is very easy to use .
So , we created a speech dialogue MP3 player . With this system , you can operate the system using your voice and search for songs as well .
How did we do it ? First , let me explain about how spoken words are processed .
First , we set keywords based on the functions of an MP3 file and user speech . Using these keywords , we processed spoken words .
This time , we registered eighty two keywords . Some examples are " play " , " fast forward " , " a little bit " , and so on .
Based on these keywords , we sorted it by patterns of spoken sentences . As a result , we classified these words into one hundred and eight patterns .
For example , " I want to listen to this " is one pattern , and " Fast forward this a little bit " is another .
Next , I will explain about how such keywords are processed .
First , I will talk about speech input . The sentence is , search for the Beatles. The machine will then look for keywords in that sentence . In this example , the keyword is " Beatles " and " Search " . The system will now look for these keywords .
Then , these keywords will be converted into a command . Here , the word " Beatles " will turn into a command for artist keywords when conducting the search .
Then these are put into order and rows of keywords are extracted . This process applies to the previous example. Currently there are one hundred and eight patterns
After these are formulated , each command performs its registered action . In this case , the system will search for artists based on the rows of keywords , then respond to the search .
We created this system based on such analysis .
We mounted it on a WINDOWS 98 OS . We used the Visual C++ language .
As I mentioned earlier , there are eighty two words registered . Regarding the keyword rows , there are one hundred and eight types defined .
Next , we conducted an evaluation experiment . Our goal is to evaluate the usability of this system .
We had twenty college students test out the system . We also compared the system with a conventional system .
The objective was to perform as many tasks as possible within the given time . Now I will explain about the tasks we assigned to the students .
For example , we would ask the students to play a certain song . This example is pretty simple , all you have to do is search for the song and play it .
In another task , we asked the students to find the title of the song that starts with , " all white " Here are the results .
First , we compared the task completion rate between our system and the conventional system . The vertical axis represents the number of tasks completed .
The blue indicates this system , and the green represents the conventional system . We can see here that the task completion rate has increased with this system . This result indicates that the speech input increased the operation and search efficiency .
Next , we evaluated the usability of the system by conducting a survey asking about the new and old systems . A five star rating system was used .
Five being the highest rating . This diagram indicates that six people gave the new system a five. Ten people gave a four , three people gave a three , one person gave a one . With the conventional system , the results were one , eight , eight , three , respectively .
When we compare these results , we can see that the new system got a higher rating overall . On average , the new system scored one point higher than the old one at four point zero five. The new system scored three point three five . Ok , I will summarize .
Today I talked about a speech dialogue MP3 player system that processes tasks based on keyword analysis . We also conducted an experiment to evaluate the system and confirmed the effectiveness of operating and searching for tracks using dictation .
That is all , thank you for your time .
Today 's presentation is entitled , Creating and evaluating a speech dialogue MP3 player . In recent years , thanks to the Internet , a vast amount of MP3 files are available anywhere you go .
MP3 is a file format on the computer. Simply put , it allows us to use the audio files inside a CD on a computer . Today , you can obtain tens of thousands of MP3 files from the Internet .
Also , portable music players like these are getting smaller and smaller while its capacity keeps getting bigger and bigger . With such devices , we can make use of a vast amount of MP3 files .
So , this is where we are today. But when we want to search for a specific MP3 file or when using it in a portable music player , it is very difficult to control them using keyboards . On the other hand , researchers are taking a closer look at dictation systems as an efficient method for searching for MP3 files .
Furthermore , the voice input interface is very easy to use .
So , we created a speech dialogue MP3 player . With this system , you can operate the system using your voice and search for songs as well .
How did we do it ? First , let me explain about how spoken words are processed .
First , we set keywords based on the functions of an MP3 file and user speech . Using these keywords , we processed spoken words .
This time , we registered eighty two keywords . Some examples are " play " , " fast forward " , " a little bit " , and so on .
Based on these keywords , we sorted it by patterns of spoken sentences . As a result , we classified these words into one hundred and eight patterns .
For example , " I want to listen to this " is one pattern , and " Fast forward this a little bit " is another . Next , I will explain about how such keywords are processed .
First , I will talk about speech input . The sentence is , search for the Beatles. The machine will then look for keywords in that sentence .
In this example , the keyword is " Beatles " and " Search " . The system will now look for these keywords . Then , these keywords will be converted into a command .
Here , the word " Beatles " will turn into a command for artist keywords when conducting the search .
Then these are put into order and rows of keywords are extracted . This process applies to the previous example. Currently there are one hundred and eight patterns
After these are formulated , each command performs its registered action . In this case , the system will search for artists based on the rows of keywords , then respond to the search .
We created this system based on such analysis . We mounted it on a WINDOWS 98 OS .
We used the Visual C++ language .
As I mentioned earlier , there are eighty two words registered . Regarding the keyword rows , there are one hundred and eight types defined .
Next , we conducted an evaluation experiment . Our goal is to evaluate the usability of this system .
We had twenty college students test out the system .
We also compared the system with a conventional system .
The objective was to perform as many tasks as possible within the given time . Now I will explain about the tasks we assigned to the students .
For example , we would ask the students to play a certain song . This example is pretty simple , all you have to do is search for the song and play it .
In another task , we asked the students to find the title of the song that starts with , " all white " Here are the results .
First , we compared the task completion rate between our system and the conventional system . The vertical axis represents the number of tasks completed .
The blue indicates this system , and the green represents the conventional system . We can see here that the task completion rate has increased with this system .
This result indicates that the speech input increased the operation and search efficiency .
Next , we evaluated the usability of the system by conducting a survey asking about the new and old systems . A five star rating system was used .
Five being the highest rating . This diagram indicates that six people gave the new system a five. Ten people gave a four , three people gave a three , one person gave a one .
With the conventional system , the results were one , eight , eight , three , respectively . When we compare these results , we can see that the new system got a higher rating overall .
On average , the new system scored one point higher than the old one at four point zero five. The new system scored three point three five .
Ok , I will summarize . Today I talked about a speech dialogue MP3 player system that processes tasks based on keyword analysis .
We also conducted an experiment to evaluate the system and confirmed the effectiveness of operating and searching for tracks using dictation . That is all , thank you for your time .
Today 's presentation is entitled , Creating and evaluating a speech dialogue MP3 player .
In recent years , thanks to the Internet , a vast amount of MP3 files are available anywhere you go .
MP3 is a file format on the computer. Simply put , it allows us to use the audio files inside a CD on a computer . Today , you can obtain tens of thousands of MP3 files from the Internet .
Also , portable music players like these are getting smaller and smaller while its capacity keeps getting bigger and bigger . With such devices , we can make use of a vast amount of MP3 files .
So , this is where we are today. But when we want to search for a specific MP3 file or when using it in a portable music player , it is very difficult to control them using keyboards . On the other hand , researchers are taking a closer look at dictation systems as an efficient method for searching for MP3 files .
Furthermore , the voice input interface is very easy to use . So , we created a speech dialogue MP3 player .
With this system , you can operate the system using your voice and search for songs as well . How did we do it ? First , let me explain about how spoken words are processed .
First , we set keywords based on the functions of an MP3 file and user speech . Using these keywords , we processed spoken words .
This time , we registered eighty two keywords . Some examples are " play " , " fast forward " , " a little bit " , and so on .
Based on these keywords , we sorted it by patterns of spoken sentences . As a result , we classified these words into one hundred and eight patterns .
For example , " I want to listen to this " is one pattern , and " Fast forward this a little bit " is another .
Next , I will explain about how such keywords are processed .
First , I will talk about speech input . The sentence is , search for the Beatles. The machine will then look for keywords in that sentence .
In this example , the keyword is " Beatles " and " Search " . The system will now look for these keywords . Then , these keywords will be converted into a command .
Here , the word " Beatles " will turn into a command for artist keywords when conducting the search .
Then these are put into order and rows of keywords are extracted . This process applies to the previous example. Currently there are one hundred and eight patterns
After these are formulated , each command performs its registered action . In this case , the system will search for artists based on the rows of keywords , then respond to the search .
We created this system based on such analysis . We mounted it on a WINDOWS 98 OS . We used the Visual C++ language .
As I mentioned earlier , there are eighty two words registered .
Regarding the keyword rows , there are one hundred and eight types defined . Next , we conducted an evaluation experiment .
Our goal is to evaluate the usability of this system . We had twenty college students test out the system .
We also compared the system with a conventional system . The objective was to perform as many tasks as possible within the given time .
Now I will explain about the tasks we assigned to the students . For example , we would ask the students to play a certain song .
This example is pretty simple , all you have to do is search for the song and play it . In another task , we asked the students to find the title of the song that starts with , " all white "
Here are the results .
First , we compared the task completion rate between our system and the conventional system . The vertical axis represents the number of tasks completed .
The blue indicates this system , and the green represents the conventional system . We can see here that the task completion rate has increased with this system .
This result indicates that the speech input increased the operation and search efficiency .
Next , we evaluated the usability of the system by conducting a survey asking about the new and old systems . A five star rating system was used .
Five being the highest rating .
This diagram indicates that six people gave the new system a five. Ten people gave a four , three people gave a three , one person gave a one . With the conventional system , the results were one , eight , eight , three , respectively .
When we compare these results , we can see that the new system got a higher rating overall . On average , the new system scored one point higher than the old one at four point zero five. The new system scored three point three five .
Ok , I will summarize . Today I talked about a speech dialogue MP3 player system that processes tasks based on keyword analysis . We also conducted an experiment to evaluate the system and confirmed the effectiveness of operating and searching for tracks using dictation .
That is all , thank you for your time .
Today I 'll speak about honorific and polite Japanese expressions generated in the context of spoken English and how to translate them into Japanese . In the last few years there has been an increase in the opportunities available inside Japan to hear English .
Because of this there has developed a desire to be able to translate spoken expressions from English into Japanese . For example , it is believed that these spoken expressions will be very useful in lectures that are specifically geared towards Japanese or when someone who speaks English and someone who speaks Japanese are interacting .
We are speaking about translating spoken words , therefore it is thought that the Japanese produced should be very much like spoken expressions and that is the most desired.For example , if you say something once then the next time you bring it up you might use a pronoun or eliminate a redundant use of the subject of the sentence to shorten it. This often happens a lot in the Japanese language. Also , the person in question who you are talking about or the age and status of the person who is talking also plays a role in the kind of attitudinal Japanese which you might use. Polite , honorific , and humble forms of spoken word are often seen when using Japanese .
One of the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct translation from English to Japanese .
For example , if the phrase " I prepare this room for you " is translated directly into Japanese most people would think that it would sound something similar to " Watashi wa anata ni kono heya o yoi suru " , but when we use spoken expressions it might make a difference depending on if the speaker is in customer service. In that case the phrasing may become more polite , such as " Anata ni kono heya o Go-yoi itashimashita " .
In addition , there are many kinds of phrases when it comes to attitudinal expressions. This is just one example and there are many more .
Following further , since there are so many attitudinal expressions to choose from I contemplated on a system of translation rules to help in the selection process believing that they should be made and implemented. While these rules to help the selection process are necessary , in actuality they were already made with someone else 's previously existing research .
As an example , the phrases used for dialoging while on vacation have a set of words attributed to them and are a specifically defined system. If the speaker is in a scenario such as checking customers into a hotel while working at the front desk then there is already a rule for the words that need to be translated for that situation .
But , attitudinal expressions are much more complicated and the person using them needs to be able to analyse various factors regarding the situation to fine tune the expressions to each scenario correctly. Making a rule for selection which falls under these kinds of conditions is incredibly difficult to do .
For that reason , the goal of this research is to create a system that automatically selects attitudinal expressions . Regarding the method , examples using attitudinal expressions will be extrapolated from a parallel translation corpus .
This will be the basis for making regulations and the place for bringing out the parallel translation corpus. As an example , we are thinking that the original sentence of the conversation and the translated sentence will appear together .
For example if a person that speaks English says " I can prepare this room for you " , the interpreter would respond with the received interpreted result and the Japanese speaking person would utter " ... actual text missing ... " .
Based on that the interpreter would explain what kind of English phrasing to use when responding .
This is how one would use parallel translation corpus to learn the rules of selection for attitudinal expressions , but here I would like to explain a little bit more about the method of learning .
In this example , another attitudinal expression is being used , but what are the the characteristics of this sentence ?
This time I discussed 3 examples of who is speaking and what kinds of sentence patterns are being used . The result is that for the very first example an attitudinal expression is being used , so the appropriate accompanying expression can be selected by matching the following conditions , there is a verb in the sentence , the person speaking is a receptionist , and the sentence is a declarative statement. We will understand that at this particular time and under these conditions a particular phrase would be the correct expression .
Similarly , the same method of looking at the verb , the speaker , and the sentence structure can be used as a rule to select which attitudinal expression would be most appropriate and because of this we are able to achieve a system of rules for selection .
To summarize . Today we discussed how to create an automatic way of translating spoken English into Japanese attitudinal expressions. Specifically , I explained how a set of rules can be created from corpus .
This concludes my speech .
Today I 'll speak about honorific and polite Japanese expressions generated in the context of spoken English and how to translate them into Japanese . In the last few years there has been an increase in the opportunities available inside Japan to hear English .
Because of this there has developed a desire to be able to translate spoken expressions from English into Japanese . For example , it is believed that these spoken expressions will be very useful in lectures that are specifically geared towards Japanese or when someone who speaks English and someone who speaks Japanese are interacting .
We are speaking about translating spoken words , therefore it is thought that the Japanese produced should be very much like spoken expressions and that is the most desired.For example , if you say something once then the next time you bring it up you might use a pronoun or eliminate a redundant use of the subject of the sentence to shorten it. This often happens a lot in the Japanese language. Also , the person in question who you are talking about or the age and status of the person who is talking also plays a role in the kind of attitudinal Japanese which you might use. Polite , honorific , and humble forms of spoken word are often seen when using Japanese .
One of the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct translation from English to Japanese .
For example , if the phrase " I prepare this room for you " is translated directly into Japanese most people would think that it would sound something similar to " Watashi wa anata ni kono heya o yoi suru " , but when we use spoken expressions it might make a difference depending on if the speaker is in customer service. In that case the phrasing may become more polite , such as " Anata ni kono heya o Go-yoi itashimashita " .
In addition , there are many kinds of phrases when it comes to attitudinal expressions. This is just one example and there are many more . Following further , since there are so many attitudinal expressions to choose from I contemplated on a system of translation rules to help in the selection process believing that they should be made and implemented. While these rules to help the selection process are necessary , in actuality they were already made with someone else 's previously existing research .
As an example , the phrases used for dialoging while on vacation have a set of words attributed to them and are a specifically defined system. If the speaker is in a scenario such as checking customers into a hotel while working at the front desk then there is already a rule for the words that need to be translated for that situation .
But , attitudinal expressions are much more complicated and the person using them needs to be able to analyse various factors regarding the situation to fine tune the expressions to each scenario correctly. Making a rule for selection which falls under these kinds of conditions is incredibly difficult to do . For that reason , the goal of this research is to create a system that automatically selects attitudinal expressions .
Regarding the method , examples using attitudinal expressions will be extrapolated from a parallel translation corpus .
This will be the basis for making regulations and the place for bringing out the parallel translation corpus. As an example , we are thinking that the original sentence of the conversation and the translated sentence will appear together .
For example if a person that speaks English says " I can prepare this room for you " , the interpreter would respond with the received interpreted result and the Japanese speaking person would utter " ... actual text missing ... " . Based on that the interpreter would explain what kind of English phrasing to use when responding .
This is how one would use parallel translation corpus to learn the rules of selection for attitudinal expressions , but here I would like to explain a little bit more about the method of learning .
In this example , another attitudinal expression is being used , but what are the the characteristics of this sentence ? This time I discussed 3 examples of who is speaking and what kinds of sentence patterns are being used .
The result is that for the very first example an attitudinal expression is being used , so the appropriate accompanying expression can be selected by matching the following conditions , there is a verb in the sentence , the person speaking is a receptionist , and the sentence is a declarative statement. We will understand that at this particular time and under these conditions a particular phrase would be the correct expression .
Similarly , the same method of looking at the verb , the speaker , and the sentence structure can be used as a rule to select which attitudinal expression would be most appropriate and because of this we are able to achieve a system of rules for selection .
To summarize .
Today we discussed how to create an automatic way of translating spoken English into Japanese attitudinal expressions. Specifically , I explained how a set of rules can be created from corpus . This concludes my speech .
Today I 'll speak about honorific and polite Japanese expressions generated in the context of spoken English and how to translate them into Japanese . In the last few years there has been an increase in the opportunities available inside Japan to hear English .
Because of this there has developed a desire to be able to translate spoken expressions from English into Japanese .
For example , it is believed that these spoken expressions will be very useful in lectures that are specifically geared towards Japanese or when someone who speaks English and someone who speaks Japanese are interacting .
We are speaking about translating spoken words , therefore it is thought that the Japanese produced should be very much like spoken expressions and that is the most desired.For example , if you say something once then the next time you bring it up you might use a pronoun or eliminate a redundant use of the subject of the sentence to shorten it. This often happens a lot in the Japanese language. Also , the person in question who you are talking about or the age and status of the person who is talking also plays a role in the kind of attitudinal Japanese which you might use. Polite , honorific , and humble forms of spoken word are often seen when using Japanese .
One of the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct translation from English to Japanese .
For example , if the phrase " I prepare this room for you " is translated directly into Japanese most people would think that it would sound something similar to " Watashi wa anata ni kono heya o yoi suru " , but when we use spoken expressions it might make a difference depending on if the speaker is in customer service. In that case the phrasing may become more polite , such as " Anata ni kono heya o Go-yoi itashimashita " . In addition , there are many kinds of phrases when it comes to attitudinal expressions. This is just one example and there are many more .
Following further , since there are so many attitudinal expressions to choose from I contemplated on a system of translation rules to help in the selection process believing that they should be made and implemented. While these rules to help the selection process are necessary , in actuality they were already made with someone else 's previously existing research .
As an example , the phrases used for dialoging while on vacation have a set of words attributed to them and are a specifically defined system. If the speaker is in a scenario such as checking customers into a hotel while working at the front desk then there is already a rule for the words that need to be translated for that situation .
But , attitudinal expressions are much more complicated and the person using them needs to be able to analyse various factors regarding the situation to fine tune the expressions to each scenario correctly. Making a rule for selection which falls under these kinds of conditions is incredibly difficult to do .
For that reason , the goal of this research is to create a system that automatically selects attitudinal expressions .
Regarding the method , examples using attitudinal expressions will be extrapolated from a parallel translation corpus .
This will be the basis for making regulations and the place for bringing out the parallel translation corpus. As an example , we are thinking that the original sentence of the conversation and the translated sentence will appear together . For example if a person that speaks English says " I can prepare this room for you " , the interpreter would respond with the received interpreted result and the Japanese speaking person would utter " ... actual text missing ... " .
Based on that the interpreter would explain what kind of English phrasing to use when responding .
This is how one would use parallel translation corpus to learn the rules of selection for attitudinal expressions , but here I would like to explain a little bit more about the method of learning .
In this example , another attitudinal expression is being used , but what are the the characteristics of this sentence ?
This time I discussed 3 examples of who is speaking and what kinds of sentence patterns are being used .
The result is that for the very first example an attitudinal expression is being used , so the appropriate accompanying expression can be selected by matching the following conditions , there is a verb in the sentence , the person speaking is a receptionist , and the sentence is a declarative statement. We will understand that at this particular time and under these conditions a particular phrase would be the correct expression .
Similarly , the same method of looking at the verb , the speaker , and the sentence structure can be used as a rule to select which attitudinal expression would be most appropriate and because of this we are able to achieve a system of rules for selection .
To summarize . Today we discussed how to create an automatic way of translating spoken English into Japanese attitudinal expressions. Specifically , I explained how a set of rules can be created from corpus .
This concludes my speech .
Today I 'll speak about honorific and polite Japanese expressions generated in the context of spoken English and how to translate them into Japanese .
In the last few years there has been an increase in the opportunities available inside Japan to hear English . Because of this there has developed a desire to be able to translate spoken expressions from English into Japanese .
For example , it is believed that these spoken expressions will be very useful in lectures that are specifically geared towards Japanese or when someone who speaks English and someone who speaks Japanese are interacting .
We are speaking about translating spoken words , therefore it is thought that the Japanese produced should be very much like spoken expressions and that is the most desired.For example , if you say something once then the next time you bring it up you might use a pronoun or eliminate a redundant use of the subject of the sentence to shorten it. This often happens a lot in the Japanese language. Also , the person in question who you are talking about or the age and status of the person who is talking also plays a role in the kind of attitudinal Japanese which you might use. Polite , honorific , and humble forms of spoken word are often seen when using Japanese .
One of the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct translation from English to Japanese .
For example , if the phrase " I prepare this room for you " is translated directly into Japanese most people would think that it would sound something similar to " Watashi wa anata ni kono heya o yoi suru " , but when we use spoken expressions it might make a difference depending on if the speaker is in customer service. In that case the phrasing may become more polite , such as " Anata ni kono heya o Go-yoi itashimashita " .
In addition , there are many kinds of phrases when it comes to attitudinal expressions. This is just one example and there are many more .
Following further , since there are so many attitudinal expressions to choose from I contemplated on a system of translation rules to help in the selection process believing that they should be made and implemented. While these rules to help the selection process are necessary , in actuality they were already made with someone else 's previously existing research .
As an example , the phrases used for dialoging while on vacation have a set of words attributed to them and are a specifically defined system. If the speaker is in a scenario such as checking customers into a hotel while working at the front desk then there is already a rule for the words that need to be translated for that situation .
But , attitudinal expressions are much more complicated and the person using them needs to be able to analyse various factors regarding the situation to fine tune the expressions to each scenario correctly. Making a rule for selection which falls under these kinds of conditions is incredibly difficult to do .
For that reason , the goal of this research is to create a system that automatically selects attitudinal expressions .
Regarding the method , examples using attitudinal expressions will be extrapolated from a parallel translation corpus .
This will be the basis for making regulations and the place for bringing out the parallel translation corpus. As an example , we are thinking that the original sentence of the conversation and the translated sentence will appear together .
For example if a person that speaks English says " I can prepare this room for you " , the interpreter would respond with the received interpreted result and the Japanese speaking person would utter " ... actual text missing ... " .
Based on that the interpreter would explain what kind of English phrasing to use when responding .
This is how one would use parallel translation corpus to learn the rules of selection for attitudinal expressions , but here I would like to explain a little bit more about the method of learning .
In this example , another attitudinal expression is being used , but what are the the characteristics of this sentence ?
This time I discussed 3 examples of who is speaking and what kinds of sentence patterns are being used .
The result is that for the very first example an attitudinal expression is being used , so the appropriate accompanying expression can be selected by matching the following conditions , there is a verb in the sentence , the person speaking is a receptionist , and the sentence is a declarative statement. We will understand that at this particular time and under these conditions a particular phrase would be the correct expression .
Similarly , the same method of looking at the verb , the speaker , and the sentence structure can be used as a rule to select which attitudinal expression would be most appropriate and because of this we are able to achieve a system of rules for selection .
To summarize .
Today we discussed how to create an automatic way of translating spoken English into Japanese attitudinal expressions. Specifically , I explained how a set of rules can be created from corpus .
This concludes my speech .
Today 's presentation is entitled , An efficient gradual syntax analysis using statistics .
The background of this presentation , and the goal of our research , is to create a real-time dialogue processing system .
A real-time dialogue processing system is a system that can take what the user says , understand it , and respond to it immediately .
In the example below , the user says , " Ken studied English at school yesterday " . Then , by the time the user said " Ken studied " , the real-time dialogue processing system should already be responding to it , even though the user has n't finished the sentence .
To do so , the system must understand that " Ken " is the subject , and " studied " is the predicate of the sentence by the time the user finished saying these two words .
Even before the user finishes the sentence , the system must understand what has been said so far . This method is called gradual syntax analysis .
This is the same example. The input sentence is , " Ken studied English at school " . When the input sentence reaches its second to last word , " Ken studied English at " , there are two ways to perceive this sentence , even though the wording is exactly the same .
In this example , the top shows the pattern where the " at " will be followed by a name of a place. So the context is , Ken studied English at a certain location. On the other hand , the context of the bottom example is , Ken studied English that is located somewhere. So , depending on which word the modifier is on , the context becomes different .
To make the system respond in real-time , the system must choose one of the two .
This means that the system must determine which context is more accurate on its own .
And so , the goal of our research topic , making gradual syntax analysis more efficient , is to help the system filter out the correct context .
So this was our goal. Our approach was to install semantic information into the gradual syntax analysis mentioned earlier .
This is the same example. " Ken studied English at " is the input. The top example shows the " at " modifying the " studied " .
The bottom example shows the " at " modifying the " English " . So these are the two contexts. We thought that the system will have a better chance of doing so if it had semantic information beforehand. For example , if the system knew that some words do not modify certain words , in this case , the " at " and the " English " , the system can determine that the bottom example is wrong .
Now , the system can filter out the wrong context out of the two .
So , that is why we thought about utilizing semantic information to filter out context .
To install semantic information , we use a corpus , a large and structured set of texts containing multiple example sentences , and make the system learn the information before performing a syntax analysis .
So , I will summarize .
In order to create a real-time dialogue system , we came up with a way to make gradual syntax analysis more efficient . We also conducted an experiment to evaluate the effectiveness of our method . That is all for today , thank you .
Today 's presentation is entitled , An efficient gradual syntax analysis using statistics .
The background of this presentation , and the goal of our research , is to create a real-time dialogue processing system .
A real-time dialogue processing system is a system that can take what the user says , understand it , and respond to it immediately .
In the example below , the user says , " Ken studied English at school yesterday " . Then , by the time the user said " Ken studied " , the real-time dialogue processing system should already be responding to it , even though the user has n't finished the sentence .
To do so , the system must understand that " Ken " is the subject , and " studied " is the predicate of the sentence by the time the user finished saying these two words .
Even before the user finishes the sentence , the system must understand what has been said so far . This method is called gradual syntax analysis .
This is the same example. The input sentence is , " Ken studied English at school " . When the input sentence reaches its second to last word , " Ken studied English at " , there are two ways to perceive this sentence , even though the wording is exactly the same .
In this example , the top shows the pattern where the " at " will be followed by a name of a place. So the context is , Ken studied English at a certain location. On the other hand , the context of the bottom example is , Ken studied English that is located somewhere. So , depending on which word the modifier is on , the context becomes different .
To make the system respond in real-time , the system must choose one of the two . This means that the system must determine which context is more accurate on its own .
And so , the goal of our research topic , making gradual syntax analysis more efficient , is to help the system filter out the correct context .
So this was our goal. Our approach was to install semantic information into the gradual syntax analysis mentioned earlier .
This is the same example. " Ken studied English at " is the input. The top example shows the " at " modifying the " studied " .
The bottom example shows the " at " modifying the " English " . So these are the two contexts. We thought that the system will have a better chance of doing so if it had semantic information beforehand. For example , if the system knew that some words do not modify certain words , in this case , the " at " and the " English " , the system can determine that the bottom example is wrong .
Now , the system can filter out the wrong context out of the two . So , that is why we thought about utilizing semantic information to filter out context .
To install semantic information , we use a corpus , a large and structured set of texts containing multiple example sentences , and make the system learn the information before performing a syntax analysis .
So , I will summarize . In order to create a real-time dialogue system , we came up with a way to make gradual syntax analysis more efficient .
We also conducted an experiment to evaluate the effectiveness of our method . That is all for today , thank you .
Today 's presentation is entitled , An efficient gradual syntax analysis using statistics .
The background of this presentation , and the goal of our research , is to create a real-time dialogue processing system . A real-time dialogue processing system is a system that can take what the user says , understand it , and respond to it immediately .
In the example below , the user says , " Ken studied English at school yesterday " . Then , by the time the user said " Ken studied " , the real-time dialogue processing system should already be responding to it , even though the user has n't finished the sentence .
To do so , the system must understand that " Ken " is the subject , and " studied " is the predicate of the sentence by the time the user finished saying these two words .
Even before the user finishes the sentence , the system must understand what has been said so far .
This method is called gradual syntax analysis . This is the same example. The input sentence is , " Ken studied English at school " . When the input sentence reaches its second to last word , " Ken studied English at " , there are two ways to perceive this sentence , even though the wording is exactly the same .
In this example , the top shows the pattern where the " at " will be followed by a name of a place. So the context is , Ken studied English at a certain location. On the other hand , the context of the bottom example is , Ken studied English that is located somewhere. So , depending on which word the modifier is on , the context becomes different .
To make the system respond in real-time , the system must choose one of the two . This means that the system must determine which context is more accurate on its own .
And so , the goal of our research topic , making gradual syntax analysis more efficient , is to help the system filter out the correct context .
So this was our goal. Our approach was to install semantic information into the gradual syntax analysis mentioned earlier .
This is the same example. " Ken studied English at " is the input. The top example shows the " at " modifying the " studied " .
The bottom example shows the " at " modifying the " English " . So these are the two contexts. We thought that the system will have a better chance of doing so if it had semantic information beforehand. For example , if the system knew that some words do not modify certain words , in this case , the " at " and the " English " , the system can determine that the bottom example is wrong .
Now , the system can filter out the wrong context out of the two . So , that is why we thought about utilizing semantic information to filter out context .
To install semantic information , we use a corpus , a large and structured set of texts containing multiple example sentences , and make the system learn the information before performing a syntax analysis .
So , I will summarize . In order to create a real-time dialogue system , we came up with a way to make gradual syntax analysis more efficient .
We also conducted an experiment to evaluate the effectiveness of our method . That is all for today , thank you .
Today 's presentation is entitled , An efficient gradual syntax analysis using statistics .
The background of this presentation , and the goal of our research , is to create a real-time dialogue processing system .
A real-time dialogue processing system is a system that can take what the user says , understand it , and respond to it immediately .
In the example below , the user says , " Ken studied English at school yesterday " . Then , by the time the user said " Ken studied " , the real-time dialogue processing system should already be responding to it , even though the user has n't finished the sentence .
To do so , the system must understand that " Ken " is the subject , and " studied " is the predicate of the sentence by the time the user finished saying these two words .
Even before the user finishes the sentence , the system must understand what has been said so far . This method is called gradual syntax analysis .
This is the same example. The input sentence is , " Ken studied English at school " . When the input sentence reaches its second to last word , " Ken studied English at " , there are two ways to perceive this sentence , even though the wording is exactly the same .
In this example , the top shows the pattern where the " at " will be followed by a name of a place. So the context is , Ken studied English at a certain location. On the other hand , the context of the bottom example is , Ken studied English that is located somewhere. So , depending on which word the modifier is on , the context becomes different .
To make the system respond in real-time , the system must choose one of the two .
This means that the system must determine which context is more accurate on its own .
And so , the goal of our research topic , making gradual syntax analysis more efficient , is to help the system filter out the correct context .
So this was our goal. Our approach was to install semantic information into the gradual syntax analysis mentioned earlier .
This is the same example. " Ken studied English at " is the input. The top example shows the " at " modifying the " studied " .
The bottom example shows the " at " modifying the " English " . So these are the two contexts. We thought that the system will have a better chance of doing so if it had semantic information beforehand. For example , if the system knew that some words do not modify certain words , in this case , the " at " and the " English " , the system can determine that the bottom example is wrong .
Now , the system can filter out the wrong context out of the two . So , that is why we thought about utilizing semantic information to filter out context .
To install semantic information , we use a corpus , a large and structured set of texts containing multiple example sentences , and make the system learn the information before performing a syntax analysis .
So , I will summarize . In order to create a real-time dialogue system , we came up with a way to make gradual syntax analysis more efficient .
We also conducted an experiment to evaluate the effectiveness of our method . That is all for today , thank you .
Today I will talk to you about a mobile agent that supports ad hoc communication . First , let me talk about the background of this research. Internet has developed significantly over the years and almost everyone uses it today .
Recently , wireless infrastructure like wireless LAN and cellphones has made significant developments as well . Today , we can communicate almost anywhere .
And so , most of us today carry these tiny " calculators " , also known as mobile computing . These " calculators " we carry around are filled with our personal information .
And we use these devices to manage information anywhere and anytime with ease . However , when it came to the use of such telecommunication devices on-the-go , we used to think about migration transparency .
What I mean by that is , normally , when you are on the move , sometimes you do n't have power or access to a network. There are certain restrictions . But migration transparency refers to the idea of making it seem like it 's not moving at all , when it actually is. For instance , if I want to use my computer in front of my desk while I 'm on the move , that 's migration transparency . So that is why we have been using wireless cellphones. But we thought we would think about our dependency on mobility .
What I mean is , the thing holds meaning because it moved. I moved myself into this meeting room and that means something. The thing has meaning because it 's in front of the TV , or it 's in front of the fridge , and so on . That 's how I want to think about information and telecommunication . So when does communication dependent on movement occur ? There is the periodical meeting at a specific location for one. Or you can run into someone by coincidence .
We defined this kind of communication , or telecommunication , based on coincidence as ad hoc communication. And we would like to support it .
Here is a simple example of an ad hoc communication. So these people are walking around , and a friend sits here . You meet your friend by coincidence and start talking normally , but on top of that , you both have this tiny mobile device . Then , the two devices exchange information directly .
So , aside from what you say verbally , you can exchange other information stored in the device , such as schedule or pictures . By making such an interaction possible , we want to make a normal conversation more fruitful. In other words , we want to make it possible to exchange more information .
In that sense , we want to develop a mobile information system that supports ad hoc communication . So what exactly are we going to help with ? Like in the previous picture , we would like to provide support when people are talking .
For instance , creating opportunities for people to meet . Let 's say you meet someone for the first time , but you both had a mutual friend . In such cases , if you match personal information stored on the device , you can find out things like which high school you went to .
You may find out that you also went to that school , or that you had a mutual friend . With a normal conversation , you wo n't be able to realize such information. But with the help of our devices , we can .
So this is an example of what I was talking about . Now , aside from such person-to-person interactions , we would also like to support interactions between a human and information systems .
What do I mean by interactions between a human and information ? For instance , at a station , there is an information kiosk that you can use . You press buttons like this .
But what if the Kiosk device could read the information on your mobile device before you even touch the machine ? This will make your Kiosk experience much easier. You can use these kinds of devices around the city freely .
This is n't limited to Kiosk devices. There are multiple devices we use everywhere from the office or at home. We want to make it so that you can use these devices freely . So now , let us think about what ad hoc communication means .
Think of ad hoc communication as the interaction a person has when he or she meets another person . Our idea is to make this communication more fruitful , and generally , the number of participants vary .
There are no rules as to how many people we interact with. It can be two , three , four , or even hundreds of people in seminars. There are also panel discussions or business meetings . Sometimes , we prepare for our interactions , but most of the time , we are not prepared . When you run into someone by coincidence , you do n't have a speech prepared or know what kind of information you will exchange beforehand .
You do it on the spot . Our focus is on sharing information on a technical basis. So , how do we share information about " sharing information " ?
If you are in a meeting , you might pass out papers to everyone and they will take notes on it . That is one way to share information , but we want to think about how to help do the same thing with mobile devices .
We must also remember to choose the appropriate communication methods. The goal of ad hoc communication will vary depending on the scene . For instance , if you 're in a meeting room , you want to hold a business meeting. That is why you want to distribute hand-outs .
But when you run into someone in the city , your goal is not to hold a meeting. You want to exchange personal information . So that 's what I mean . So , how can mobile devices help ?
It must be able to adapt to the situation . It must understand the surrounding situation and share the appropriate information .
Let 's take a look one by one. First , for the network with lots of movements , an ad hoc network might work well. Ad hoc network technology has been a hot research topic these days .
Ad hoc network refers to a network that is established on demand. It appears only when necessary. Most of the time , it is created wirelessly .
The significance of this is , the fact that it does n't have a server that manages the network . There is no preparation to establish the network , so it ca n't be a server to begin with .
So there is no server to manage the network . Regarding this topic , there are already some mounting ideas . We have been researching infrared ad hoc network .
Next comes the question , how do we share applications ? For instance , if you run into a friend , the friend may not have the same application you have to share information . And you ca n't exactly talk to one another beforehand and decide to install the same app .
What I mean is , it 's impossible to prepare all telecommunication software beforehand. So we need a system where the necessary app is distributed on demand .
To do so , we will be using something called a mobile agent . Mobile agent is a program that lets you move around the network .
Lastly I want to talk about how to adapt to the environment. Because ultimately , we want to make it so that the device figures out what we want to do , so we do n't have to control it every time . To do so , we need a system that recognizes the surrounding environment. We plan to use an intelligent agent to make this possible . Intelligent agent is a program that can read the surrounding environment on its own and figure out the current situation the user is in .
In such cases , the intelligent agent uses the mobile app and the ad hoc agent network to establish an ad hoc communication system we want to create .
Let 's look at an example. Let 's say there is a meeting coming up in the office. Here is the intelligent agent. When you say that you 're going to the meeting , the intelligent agent moves the necessary information onto your mobile device .
When you get to the meeting room , the intelligent agent distributes the necessary information to the participants . And let 's say there was a presentation board here. The agent will send the appropriate information there as well .
So , this user does n't have to tell the device he or she is in a meeting. The agent recognizes the situation and does what 's necessary , like hand out information for the meeting .
Ok , I will summarize . Our study on ad hoc communication has been focused on supporting communication with people and with machines .
To support this , we nave created an ad hoc communication system based on a mobile agent system . By using this , we believe we can create a system that allows us to enjoy ad hoc communication with ease at anytime , anywhere .
That is all . Thank you for listening .
Today I will talk to you about a mobile agent that supports ad hoc communication . First , let me talk about the background of this research. Internet has developed significantly over the years and almost everyone uses it today .
Recently , wireless infrastructure like wireless LAN and cellphones has made significant developments as well . Today , we can communicate almost anywhere .
And so , most of us today carry these tiny " calculators " , also known as mobile computing . These " calculators " we carry around are filled with our personal information .
And we use these devices to manage information anywhere and anytime with ease .
However , when it came to the use of such telecommunication devices on-the-go , we used to think about migration transparency . What I mean by that is , normally , when you are on the move , sometimes you do n't have power or access to a network. There are certain restrictions .
But migration transparency refers to the idea of making it seem like it 's not moving at all , when it actually is. For instance , if I want to use my computer in front of my desk while I 'm on the move , that 's migration transparency .
So that is why we have been using wireless cellphones. But we thought we would think about our dependency on mobility . What I mean is , the thing holds meaning because it moved. I moved myself into this meeting room and that means something. The thing has meaning because it 's in front of the TV , or it 's in front of the fridge , and so on .
That 's how I want to think about information and telecommunication . So when does communication dependent on movement occur ? There is the periodical meeting at a specific location for one. Or you can run into someone by coincidence . We defined this kind of communication , or telecommunication , based on coincidence as ad hoc communication. And we would like to support it .
Here is a simple example of an ad hoc communication. So these people are walking around , and a friend sits here .
You meet your friend by coincidence and start talking normally , but on top of that , you both have this tiny mobile device . Then , the two devices exchange information directly .
So , aside from what you say verbally , you can exchange other information stored in the device , such as schedule or pictures . By making such an interaction possible , we want to make a normal conversation more fruitful. In other words , we want to make it possible to exchange more information .
In that sense , we want to develop a mobile information system that supports ad hoc communication . So what exactly are we going to help with ? Like in the previous picture , we would like to provide support when people are talking .
For instance , creating opportunities for people to meet . Let 's say you meet someone for the first time , but you both had a mutual friend . In such cases , if you match personal information stored on the device , you can find out things like which high school you went to .
You may find out that you also went to that school , or that you had a mutual friend . With a normal conversation , you wo n't be able to realize such information. But with the help of our devices , we can .
So this is an example of what I was talking about . Now , aside from such person-to-person interactions , we would also like to support interactions between a human and information systems .
What do I mean by interactions between a human and information ? For instance , at a station , there is an information kiosk that you can use . You press buttons like this .
But what if the Kiosk device could read the information on your mobile device before you even touch the machine ? This will make your Kiosk experience much easier. You can use these kinds of devices around the city freely . This is n't limited to Kiosk devices. There are multiple devices we use everywhere from the office or at home. We want to make it so that you can use these devices freely .
So now , let us think about what ad hoc communication means . Think of ad hoc communication as the interaction a person has when he or she meets another person .
Our idea is to make this communication more fruitful , and generally , the number of participants vary . There are no rules as to how many people we interact with. It can be two , three , four , or even hundreds of people in seminars. There are also panel discussions or business meetings .
Sometimes , we prepare for our interactions , but most of the time , we are not prepared . When you run into someone by coincidence , you do n't have a speech prepared or know what kind of information you will exchange beforehand .
You do it on the spot . Our focus is on sharing information on a technical basis. So , how do we share information about " sharing information " ? If you are in a meeting , you might pass out papers to everyone and they will take notes on it .
That is one way to share information , but we want to think about how to help do the same thing with mobile devices . We must also remember to choose the appropriate communication methods. The goal of ad hoc communication will vary depending on the scene .
For instance , if you 're in a meeting room , you want to hold a business meeting. That is why you want to distribute hand-outs . But when you run into someone in the city , your goal is not to hold a meeting. You want to exchange personal information .
So that 's what I mean . So , how can mobile devices help ? It must be able to adapt to the situation .
It must understand the surrounding situation and share the appropriate information .
Let 's take a look one by one. First , for the network with lots of movements , an ad hoc network might work well. Ad hoc network technology has been a hot research topic these days . Ad hoc network refers to a network that is established on demand. It appears only when necessary. Most of the time , it is created wirelessly .
The significance of this is , the fact that it does n't have a server that manages the network . There is no preparation to establish the network , so it ca n't be a server to begin with . So there is no server to manage the network .
Regarding this topic , there are already some mounting ideas . We have been researching infrared ad hoc network .
Next comes the question , how do we share applications ? For instance , if you run into a friend , the friend may not have the same application you have to share information .
And you ca n't exactly talk to one another beforehand and decide to install the same app . What I mean is , it 's impossible to prepare all telecommunication software beforehand. So we need a system where the necessary app is distributed on demand .
To do so , we will be using something called a mobile agent . Mobile agent is a program that lets you move around the network .
Lastly I want to talk about how to adapt to the environment. Because ultimately , we want to make it so that the device figures out what we want to do , so we do n't have to control it every time .
To do so , we need a system that recognizes the surrounding environment. We plan to use an intelligent agent to make this possible .
Intelligent agent is a program that can read the surrounding environment on its own and figure out the current situation the user is in . In such cases , the intelligent agent uses the mobile app and the ad hoc agent network to establish an ad hoc communication system we want to create .
Let 's look at an example. Let 's say there is a meeting coming up in the office. Here is the intelligent agent. When you say that you 're going to the meeting , the intelligent agent moves the necessary information onto your mobile device .
When you get to the meeting room , the intelligent agent distributes the necessary information to the participants . And let 's say there was a presentation board here. The agent will send the appropriate information there as well .
So , this user does n't have to tell the device he or she is in a meeting. The agent recognizes the situation and does what 's necessary , like hand out information for the meeting .
Ok , I will summarize . Our study on ad hoc communication has been focused on supporting communication with people and with machines . To support this , we nave created an ad hoc communication system based on a mobile agent system .
By using this , we believe we can create a system that allows us to enjoy ad hoc communication with ease at anytime , anywhere . That is all . Thank you for listening .
Today I will talk to you about a mobile agent that supports ad hoc communication . First , let me talk about the background of this research. Internet has developed significantly over the years and almost everyone uses it today .
Recently , wireless infrastructure like wireless LAN and cellphones has made significant developments as well . Today , we can communicate almost anywhere .
And so , most of us today carry these tiny " calculators " , also known as mobile computing . These " calculators " we carry around are filled with our personal information .
And we use these devices to manage information anywhere and anytime with ease . However , when it came to the use of such telecommunication devices on-the-go , we used to think about migration transparency .
What I mean by that is , normally , when you are on the move , sometimes you do n't have power or access to a network. There are certain restrictions . But migration transparency refers to the idea of making it seem like it 's not moving at all , when it actually is. For instance , if I want to use my computer in front of my desk while I 'm on the move , that 's migration transparency .
So that is why we have been using wireless cellphones. But we thought we would think about our dependency on mobility .
What I mean is , the thing holds meaning because it moved. I moved myself into this meeting room and that means something. The thing has meaning because it 's in front of the TV , or it 's in front of the fridge , and so on . That 's how I want to think about information and telecommunication .
So when does communication dependent on movement occur ? There is the periodical meeting at a specific location for one. Or you can run into someone by coincidence .
We defined this kind of communication , or telecommunication , based on coincidence as ad hoc communication. And we would like to support it . Here is a simple example of an ad hoc communication. So these people are walking around , and a friend sits here .
You meet your friend by coincidence and start talking normally , but on top of that , you both have this tiny mobile device . Then , the two devices exchange information directly . So , aside from what you say verbally , you can exchange other information stored in the device , such as schedule or pictures .
By making such an interaction possible , we want to make a normal conversation more fruitful. In other words , we want to make it possible to exchange more information .
In that sense , we want to develop a mobile information system that supports ad hoc communication . So what exactly are we going to help with ? Like in the previous picture , we would like to provide support when people are talking .
For instance , creating opportunities for people to meet . Let 's say you meet someone for the first time , but you both had a mutual friend .
In such cases , if you match personal information stored on the device , you can find out things like which high school you went to . You may find out that you also went to that school , or that you had a mutual friend .
With a normal conversation , you wo n't be able to realize such information. But with the help of our devices , we can . So this is an example of what I was talking about . Now , aside from such person-to-person interactions , we would also like to support interactions between a human and information systems .
What do I mean by interactions between a human and information ? For instance , at a station , there is an information kiosk that you can use . You press buttons like this . But what if the Kiosk device could read the information on your mobile device before you even touch the machine ? This will make your Kiosk experience much easier. You can use these kinds of devices around the city freely .
This is n't limited to Kiosk devices. There are multiple devices we use everywhere from the office or at home. We want to make it so that you can use these devices freely .
So now , let us think about what ad hoc communication means .
Think of ad hoc communication as the interaction a person has when he or she meets another person . Our idea is to make this communication more fruitful , and generally , the number of participants vary .
There are no rules as to how many people we interact with. It can be two , three , four , or even hundreds of people in seminars. There are also panel discussions or business meetings .
Sometimes , we prepare for our interactions , but most of the time , we are not prepared .
When you run into someone by coincidence , you do n't have a speech prepared or know what kind of information you will exchange beforehand .
You do it on the spot . Our focus is on sharing information on a technical basis. So , how do we share information about " sharing information " ?
If you are in a meeting , you might pass out papers to everyone and they will take notes on it . That is one way to share information , but we want to think about how to help do the same thing with mobile devices .
We must also remember to choose the appropriate communication methods. The goal of ad hoc communication will vary depending on the scene .
For instance , if you 're in a meeting room , you want to hold a business meeting. That is why you want to distribute hand-outs . But when you run into someone in the city , your goal is not to hold a meeting. You want to exchange personal information .
So that 's what I mean . So , how can mobile devices help ? It must be able to adapt to the situation .
It must understand the surrounding situation and share the appropriate information . Let 's take a look one by one. First , for the network with lots of movements , an ad hoc network might work well. Ad hoc network technology has been a hot research topic these days .
Ad hoc network refers to a network that is established on demand. It appears only when necessary. Most of the time , it is created wirelessly .
The significance of this is , the fact that it does n't have a server that manages the network . There is no preparation to establish the network , so it ca n't be a server to begin with . So there is no server to manage the network .
Regarding this topic , there are already some mounting ideas . We have been researching infrared ad hoc network .
Next comes the question , how do we share applications ? For instance , if you run into a friend , the friend may not have the same application you have to share information . And you ca n't exactly talk to one another beforehand and decide to install the same app .
What I mean is , it 's impossible to prepare all telecommunication software beforehand. So we need a system where the necessary app is distributed on demand .
To do so , we will be using something called a mobile agent . Mobile agent is a program that lets you move around the network .
Lastly I want to talk about how to adapt to the environment. Because ultimately , we want to make it so that the device figures out what we want to do , so we do n't have to control it every time .
To do so , we need a system that recognizes the surrounding environment. We plan to use an intelligent agent to make this possible . Intelligent agent is a program that can read the surrounding environment on its own and figure out the current situation the user is in .
In such cases , the intelligent agent uses the mobile app and the ad hoc agent network to establish an ad hoc communication system we want to create . Let 's look at an example. Let 's say there is a meeting coming up in the office. Here is the intelligent agent. When you say that you 're going to the meeting , the intelligent agent moves the necessary information onto your mobile device .
When you get to the meeting room , the intelligent agent distributes the necessary information to the participants .
And let 's say there was a presentation board here. The agent will send the appropriate information there as well . So , this user does n't have to tell the device he or she is in a meeting. The agent recognizes the situation and does what 's necessary , like hand out information for the meeting .
Ok , I will summarize .
Our study on ad hoc communication has been focused on supporting communication with people and with machines . To support this , we nave created an ad hoc communication system based on a mobile agent system .
By using this , we believe we can create a system that allows us to enjoy ad hoc communication with ease at anytime , anywhere . That is all . Thank you for listening .
Today I will talk to you about a mobile agent that supports ad hoc communication .
First , let me talk about the background of this research. Internet has developed significantly over the years and almost everyone uses it today .
Recently , wireless infrastructure like wireless LAN and cellphones has made significant developments as well . Today , we can communicate almost anywhere .
And so , most of us today carry these tiny " calculators " , also known as mobile computing .
These " calculators " we carry around are filled with our personal information .
And we use these devices to manage information anywhere and anytime with ease . However , when it came to the use of such telecommunication devices on-the-go , we used to think about migration transparency .
What I mean by that is , normally , when you are on the move , sometimes you do n't have power or access to a network. There are certain restrictions .
But migration transparency refers to the idea of making it seem like it 's not moving at all , when it actually is. For instance , if I want to use my computer in front of my desk while I 'm on the move , that 's migration transparency . So that is why we have been using wireless cellphones. But we thought we would think about our dependency on mobility .
What I mean is , the thing holds meaning because it moved. I moved myself into this meeting room and that means something. The thing has meaning because it 's in front of the TV , or it 's in front of the fridge , and so on . That 's how I want to think about information and telecommunication .
So when does communication dependent on movement occur ? There is the periodical meeting at a specific location for one. Or you can run into someone by coincidence . We defined this kind of communication , or telecommunication , based on coincidence as ad hoc communication. And we would like to support it .
Here is a simple example of an ad hoc communication. So these people are walking around , and a friend sits here .
You meet your friend by coincidence and start talking normally , but on top of that , you both have this tiny mobile device . Then , the two devices exchange information directly .
So , aside from what you say verbally , you can exchange other information stored in the device , such as schedule or pictures . By making such an interaction possible , we want to make a normal conversation more fruitful. In other words , we want to make it possible to exchange more information .
In that sense , we want to develop a mobile information system that supports ad hoc communication . So what exactly are we going to help with ? Like in the previous picture , we would like to provide support when people are talking .
For instance , creating opportunities for people to meet .
Let 's say you meet someone for the first time , but you both had a mutual friend . In such cases , if you match personal information stored on the device , you can find out things like which high school you went to . You may find out that you also went to that school , or that you had a mutual friend .
With a normal conversation , you wo n't be able to realize such information. But with the help of our devices , we can . So this is an example of what I was talking about . Now , aside from such person-to-person interactions , we would also like to support interactions between a human and information systems .
What do I mean by interactions between a human and information ? For instance , at a station , there is an information kiosk that you can use . You press buttons like this .
But what if the Kiosk device could read the information on your mobile device before you even touch the machine ? This will make your Kiosk experience much easier. You can use these kinds of devices around the city freely . This is n't limited to Kiosk devices. There are multiple devices we use everywhere from the office or at home. We want to make it so that you can use these devices freely .
So now , let us think about what ad hoc communication means . Think of ad hoc communication as the interaction a person has when he or she meets another person .
Our idea is to make this communication more fruitful , and generally , the number of participants vary .
There are no rules as to how many people we interact with. It can be two , three , four , or even hundreds of people in seminars. There are also panel discussions or business meetings . Sometimes , we prepare for our interactions , but most of the time , we are not prepared .
When you run into someone by coincidence , you do n't have a speech prepared or know what kind of information you will exchange beforehand . You do it on the spot .
Our focus is on sharing information on a technical basis. So , how do we share information about " sharing information " ?
If you are in a meeting , you might pass out papers to everyone and they will take notes on it .
That is one way to share information , but we want to think about how to help do the same thing with mobile devices . We must also remember to choose the appropriate communication methods. The goal of ad hoc communication will vary depending on the scene .
For instance , if you 're in a meeting room , you want to hold a business meeting. That is why you want to distribute hand-outs . But when you run into someone in the city , your goal is not to hold a meeting. You want to exchange personal information . So that 's what I mean .
So , how can mobile devices help ? It must be able to adapt to the situation .
It must understand the surrounding situation and share the appropriate information .
Let 's take a look one by one. First , for the network with lots of movements , an ad hoc network might work well. Ad hoc network technology has been a hot research topic these days .
Ad hoc network refers to a network that is established on demand. It appears only when necessary. Most of the time , it is created wirelessly .
The significance of this is , the fact that it does n't have a server that manages the network . There is no preparation to establish the network , so it ca n't be a server to begin with .
So there is no server to manage the network .
Regarding this topic , there are already some mounting ideas . We have been researching infrared ad hoc network .
Next comes the question , how do we share applications ? For instance , if you run into a friend , the friend may not have the same application you have to share information . And you ca n't exactly talk to one another beforehand and decide to install the same app .
What I mean is , it 's impossible to prepare all telecommunication software beforehand. So we need a system where the necessary app is distributed on demand .
To do so , we will be using something called a mobile agent . Mobile agent is a program that lets you move around the network .
Lastly I want to talk about how to adapt to the environment. Because ultimately , we want to make it so that the device figures out what we want to do , so we do n't have to control it every time . To do so , we need a system that recognizes the surrounding environment. We plan to use an intelligent agent to make this possible .
Intelligent agent is a program that can read the surrounding environment on its own and figure out the current situation the user is in . In such cases , the intelligent agent uses the mobile app and the ad hoc agent network to establish an ad hoc communication system we want to create .
Let 's look at an example. Let 's say there is a meeting coming up in the office. Here is the intelligent agent. When you say that you 're going to the meeting , the intelligent agent moves the necessary information onto your mobile device .
When you get to the meeting room , the intelligent agent distributes the necessary information to the participants . And let 's say there was a presentation board here. The agent will send the appropriate information there as well .
So , this user does n't have to tell the device he or she is in a meeting. The agent recognizes the situation and does what 's necessary , like hand out information for the meeting . Ok , I will summarize .
Our study on ad hoc communication has been focused on supporting communication with people and with machines . To support this , we nave created an ad hoc communication system based on a mobile agent system .
By using this , we believe we can create a system that allows us to enjoy ad hoc communication with ease at anytime , anywhere . That is all . Thank you for listening .
